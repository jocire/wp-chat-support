/*
 * ATTENTION: The "eval" devtool has been used (maybe by default in mode: "development").
 * This devtool is neither made for production nor for readable output files.
 * It uses "eval()" calls to create a separate source file in the browser devtools.
 * If you are trying to read the output file, select a different devtool (https://webpack.js.org/configuration/devtool/)
 * or disable the default devtool with "devtool: false".
 * If you are looking for production-ready output files, see mode: "production" (https://webpack.js.org/configuration/mode/).
 */
/******/ (() => { // webpackBootstrap
/******/ 	"use strict";
/******/ 	var __webpack_modules__ = ({

/***/ "./src/index.js":
/*!**********************!*\
  !*** ./src/index.js ***!
  \**********************/
/***/ ((__unused_webpack_module, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony import */ var openai__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! openai */ \"./node_modules/openai/index.mjs\");\nfunction _typeof(o) { \"@babel/helpers - typeof\"; return _typeof = \"function\" == typeof Symbol && \"symbol\" == typeof Symbol.iterator ? function (o) { return typeof o; } : function (o) { return o && \"function\" == typeof Symbol && o.constructor === Symbol && o !== Symbol.prototype ? \"symbol\" : typeof o; }, _typeof(o); }\nfunction _regeneratorRuntime() { \"use strict\"; /*! regenerator-runtime -- Copyright (c) 2014-present, Facebook, Inc. -- license (MIT): https://github.com/facebook/regenerator/blob/main/LICENSE */ _regeneratorRuntime = function _regeneratorRuntime() { return e; }; var t, e = {}, r = Object.prototype, n = r.hasOwnProperty, o = Object.defineProperty || function (t, e, r) { t[e] = r.value; }, i = \"function\" == typeof Symbol ? Symbol : {}, a = i.iterator || \"@@iterator\", c = i.asyncIterator || \"@@asyncIterator\", u = i.toStringTag || \"@@toStringTag\"; function define(t, e, r) { return Object.defineProperty(t, e, { value: r, enumerable: !0, configurable: !0, writable: !0 }), t[e]; } try { define({}, \"\"); } catch (t) { define = function define(t, e, r) { return t[e] = r; }; } function wrap(t, e, r, n) { var i = e && e.prototype instanceof Generator ? e : Generator, a = Object.create(i.prototype), c = new Context(n || []); return o(a, \"_invoke\", { value: makeInvokeMethod(t, r, c) }), a; } function tryCatch(t, e, r) { try { return { type: \"normal\", arg: t.call(e, r) }; } catch (t) { return { type: \"throw\", arg: t }; } } e.wrap = wrap; var h = \"suspendedStart\", l = \"suspendedYield\", f = \"executing\", s = \"completed\", y = {}; function Generator() {} function GeneratorFunction() {} function GeneratorFunctionPrototype() {} var p = {}; define(p, a, function () { return this; }); var d = Object.getPrototypeOf, v = d && d(d(values([]))); v && v !== r && n.call(v, a) && (p = v); var g = GeneratorFunctionPrototype.prototype = Generator.prototype = Object.create(p); function defineIteratorMethods(t) { [\"next\", \"throw\", \"return\"].forEach(function (e) { define(t, e, function (t) { return this._invoke(e, t); }); }); } function AsyncIterator(t, e) { function invoke(r, o, i, a) { var c = tryCatch(t[r], t, o); if (\"throw\" !== c.type) { var u = c.arg, h = u.value; return h && \"object\" == _typeof(h) && n.call(h, \"__await\") ? e.resolve(h.__await).then(function (t) { invoke(\"next\", t, i, a); }, function (t) { invoke(\"throw\", t, i, a); }) : e.resolve(h).then(function (t) { u.value = t, i(u); }, function (t) { return invoke(\"throw\", t, i, a); }); } a(c.arg); } var r; o(this, \"_invoke\", { value: function value(t, n) { function callInvokeWithMethodAndArg() { return new e(function (e, r) { invoke(t, n, e, r); }); } return r = r ? r.then(callInvokeWithMethodAndArg, callInvokeWithMethodAndArg) : callInvokeWithMethodAndArg(); } }); } function makeInvokeMethod(e, r, n) { var o = h; return function (i, a) { if (o === f) throw Error(\"Generator is already running\"); if (o === s) { if (\"throw\" === i) throw a; return { value: t, done: !0 }; } for (n.method = i, n.arg = a;;) { var c = n.delegate; if (c) { var u = maybeInvokeDelegate(c, n); if (u) { if (u === y) continue; return u; } } if (\"next\" === n.method) n.sent = n._sent = n.arg;else if (\"throw\" === n.method) { if (o === h) throw o = s, n.arg; n.dispatchException(n.arg); } else \"return\" === n.method && n.abrupt(\"return\", n.arg); o = f; var p = tryCatch(e, r, n); if (\"normal\" === p.type) { if (o = n.done ? s : l, p.arg === y) continue; return { value: p.arg, done: n.done }; } \"throw\" === p.type && (o = s, n.method = \"throw\", n.arg = p.arg); } }; } function maybeInvokeDelegate(e, r) { var n = r.method, o = e.iterator[n]; if (o === t) return r.delegate = null, \"throw\" === n && e.iterator[\"return\"] && (r.method = \"return\", r.arg = t, maybeInvokeDelegate(e, r), \"throw\" === r.method) || \"return\" !== n && (r.method = \"throw\", r.arg = new TypeError(\"The iterator does not provide a '\" + n + \"' method\")), y; var i = tryCatch(o, e.iterator, r.arg); if (\"throw\" === i.type) return r.method = \"throw\", r.arg = i.arg, r.delegate = null, y; var a = i.arg; return a ? a.done ? (r[e.resultName] = a.value, r.next = e.nextLoc, \"return\" !== r.method && (r.method = \"next\", r.arg = t), r.delegate = null, y) : a : (r.method = \"throw\", r.arg = new TypeError(\"iterator result is not an object\"), r.delegate = null, y); } function pushTryEntry(t) { var e = { tryLoc: t[0] }; 1 in t && (e.catchLoc = t[1]), 2 in t && (e.finallyLoc = t[2], e.afterLoc = t[3]), this.tryEntries.push(e); } function resetTryEntry(t) { var e = t.completion || {}; e.type = \"normal\", delete e.arg, t.completion = e; } function Context(t) { this.tryEntries = [{ tryLoc: \"root\" }], t.forEach(pushTryEntry, this), this.reset(!0); } function values(e) { if (e || \"\" === e) { var r = e[a]; if (r) return r.call(e); if (\"function\" == typeof e.next) return e; if (!isNaN(e.length)) { var o = -1, i = function next() { for (; ++o < e.length;) if (n.call(e, o)) return next.value = e[o], next.done = !1, next; return next.value = t, next.done = !0, next; }; return i.next = i; } } throw new TypeError(_typeof(e) + \" is not iterable\"); } return GeneratorFunction.prototype = GeneratorFunctionPrototype, o(g, \"constructor\", { value: GeneratorFunctionPrototype, configurable: !0 }), o(GeneratorFunctionPrototype, \"constructor\", { value: GeneratorFunction, configurable: !0 }), GeneratorFunction.displayName = define(GeneratorFunctionPrototype, u, \"GeneratorFunction\"), e.isGeneratorFunction = function (t) { var e = \"function\" == typeof t && t.constructor; return !!e && (e === GeneratorFunction || \"GeneratorFunction\" === (e.displayName || e.name)); }, e.mark = function (t) { return Object.setPrototypeOf ? Object.setPrototypeOf(t, GeneratorFunctionPrototype) : (t.__proto__ = GeneratorFunctionPrototype, define(t, u, \"GeneratorFunction\")), t.prototype = Object.create(g), t; }, e.awrap = function (t) { return { __await: t }; }, defineIteratorMethods(AsyncIterator.prototype), define(AsyncIterator.prototype, c, function () { return this; }), e.AsyncIterator = AsyncIterator, e.async = function (t, r, n, o, i) { void 0 === i && (i = Promise); var a = new AsyncIterator(wrap(t, r, n, o), i); return e.isGeneratorFunction(r) ? a : a.next().then(function (t) { return t.done ? t.value : a.next(); }); }, defineIteratorMethods(g), define(g, u, \"Generator\"), define(g, a, function () { return this; }), define(g, \"toString\", function () { return \"[object Generator]\"; }), e.keys = function (t) { var e = Object(t), r = []; for (var n in e) r.push(n); return r.reverse(), function next() { for (; r.length;) { var t = r.pop(); if (t in e) return next.value = t, next.done = !1, next; } return next.done = !0, next; }; }, e.values = values, Context.prototype = { constructor: Context, reset: function reset(e) { if (this.prev = 0, this.next = 0, this.sent = this._sent = t, this.done = !1, this.delegate = null, this.method = \"next\", this.arg = t, this.tryEntries.forEach(resetTryEntry), !e) for (var r in this) \"t\" === r.charAt(0) && n.call(this, r) && !isNaN(+r.slice(1)) && (this[r] = t); }, stop: function stop() { this.done = !0; var t = this.tryEntries[0].completion; if (\"throw\" === t.type) throw t.arg; return this.rval; }, dispatchException: function dispatchException(e) { if (this.done) throw e; var r = this; function handle(n, o) { return a.type = \"throw\", a.arg = e, r.next = n, o && (r.method = \"next\", r.arg = t), !!o; } for (var o = this.tryEntries.length - 1; o >= 0; --o) { var i = this.tryEntries[o], a = i.completion; if (\"root\" === i.tryLoc) return handle(\"end\"); if (i.tryLoc <= this.prev) { var c = n.call(i, \"catchLoc\"), u = n.call(i, \"finallyLoc\"); if (c && u) { if (this.prev < i.catchLoc) return handle(i.catchLoc, !0); if (this.prev < i.finallyLoc) return handle(i.finallyLoc); } else if (c) { if (this.prev < i.catchLoc) return handle(i.catchLoc, !0); } else { if (!u) throw Error(\"try statement without catch or finally\"); if (this.prev < i.finallyLoc) return handle(i.finallyLoc); } } } }, abrupt: function abrupt(t, e) { for (var r = this.tryEntries.length - 1; r >= 0; --r) { var o = this.tryEntries[r]; if (o.tryLoc <= this.prev && n.call(o, \"finallyLoc\") && this.prev < o.finallyLoc) { var i = o; break; } } i && (\"break\" === t || \"continue\" === t) && i.tryLoc <= e && e <= i.finallyLoc && (i = null); var a = i ? i.completion : {}; return a.type = t, a.arg = e, i ? (this.method = \"next\", this.next = i.finallyLoc, y) : this.complete(a); }, complete: function complete(t, e) { if (\"throw\" === t.type) throw t.arg; return \"break\" === t.type || \"continue\" === t.type ? this.next = t.arg : \"return\" === t.type ? (this.rval = this.arg = t.arg, this.method = \"return\", this.next = \"end\") : \"normal\" === t.type && e && (this.next = e), y; }, finish: function finish(t) { for (var e = this.tryEntries.length - 1; e >= 0; --e) { var r = this.tryEntries[e]; if (r.finallyLoc === t) return this.complete(r.completion, r.afterLoc), resetTryEntry(r), y; } }, \"catch\": function _catch(t) { for (var e = this.tryEntries.length - 1; e >= 0; --e) { var r = this.tryEntries[e]; if (r.tryLoc === t) { var n = r.completion; if (\"throw\" === n.type) { var o = n.arg; resetTryEntry(r); } return o; } } throw Error(\"illegal catch attempt\"); }, delegateYield: function delegateYield(e, r, n) { return this.delegate = { iterator: values(e), resultName: r, nextLoc: n }, \"next\" === this.method && (this.arg = t), y; } }, e; }\nfunction asyncGeneratorStep(n, t, e, r, o, a, c) { try { var i = n[a](c), u = i.value; } catch (n) { return void e(n); } i.done ? t(u) : Promise.resolve(u).then(r, o); }\nfunction _asyncToGenerator(n) { return function () { var t = this, e = arguments; return new Promise(function (r, o) { var a = n.apply(t, e); function _next(n) { asyncGeneratorStep(a, r, o, _next, _throw, \"next\", n); } function _throw(n) { asyncGeneratorStep(a, r, o, _next, _throw, \"throw\", n); } _next(void 0); }); }; }\n// Import OpenAI\n\n\n// Instantiate the OpenAI client using the localized API key\nvar openai = new openai__WEBPACK_IMPORTED_MODULE_0__[\"default\"]({\n  apiKey: wpmlChat.api_key // Use the API key from wp_localize_script\n});\nconsole.log(\"Hello From Index\");\nfunction main() {\n  return _main.apply(this, arguments);\n}\nfunction _main() {\n  _main = _asyncToGenerator( /*#__PURE__*/_regeneratorRuntime().mark(function _callee() {\n    var assistant, thread, message, run;\n    return _regeneratorRuntime().wrap(function _callee$(_context) {\n      while (1) switch (_context.prev = _context.next) {\n        case 0:\n          _context.next = 2;\n          return openai.beta.assistants.create({\n            name: \"WPML AI Agent\",\n            instructions: \"You are a helpful agent helping with questions using content found on https://wpml.org. You can answer questions about using the plugin WPML, meeting minimal requirements, informing about known issues and giving advice for developers about how to use WordPress, WPML, and its addons. You do not reply to questions about anything that is not related to those topics.\",\n            tools: [{\n              type: \"code_interpreter\"\n            }],\n            model: \"gpt-3.5-turbo\"\n          });\n        case 2:\n          assistant = _context.sent;\n          _context.next = 5;\n          return openai.beta.threads.create();\n        case 5:\n          thread = _context.sent;\n          _context.next = 8;\n          return openai.beta.threads.messages.create(thread.id, {\n            role: \"user\",\n            content: \"I need help with translating my website\"\n          });\n        case 8:\n          message = _context.sent;\n          run = openai.beta.threads.runs.stream(thread.id, {\n            assistant_id: assistant.id\n          }).on('textCreated', function (text) {\n            return console.log('\\nassistant > ');\n          }).on('textDelta', function (textDelta, snapshot) {\n            return console.log(textDelta.value);\n          }).on('toolCallCreated', function (toolCall) {\n            return console.log(\"\\nassistant > \".concat(toolCall.type, \"\\n\\n\"));\n          }).on('toolCallDelta', function (toolCallDelta, snapshot) {\n            if (toolCallDelta.type === 'code_interpreter') {\n              if (toolCallDelta.code_interpreter.input) {\n                console.log(toolCallDelta.code_interpreter.input);\n              }\n              if (toolCallDelta.code_interpreter.outputs) {\n                console.log(\"\\noutput >\\n\");\n                toolCallDelta.code_interpreter.outputs.forEach(function (output) {\n                  if (output.type === \"logs\") {\n                    console.log(\"\\n\".concat(output.logs, \"\\n\"));\n                  }\n                });\n              }\n            }\n          });\n        case 10:\n        case \"end\":\n          return _context.stop();\n      }\n    }, _callee);\n  }));\n  return _main.apply(this, arguments);\n}\nmain();\n\n//# sourceURL=webpack://wpml-chat-support/./src/index.js?");

/***/ }),

/***/ "./node_modules/openai/_shims/MultipartBody.mjs":
/*!******************************************************!*\
  !*** ./node_modules/openai/_shims/MultipartBody.mjs ***!
  \******************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   MultipartBody: () => (/* binding */ MultipartBody)\n/* harmony export */ });\n/**\n * Disclaimer: modules in _shims aren't intended to be imported by SDK users.\n */\nclass MultipartBody {\n    constructor(body) {\n        this.body = body;\n    }\n    get [Symbol.toStringTag]() {\n        return 'MultipartBody';\n    }\n}\n//# sourceMappingURL=MultipartBody.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/_shims/MultipartBody.mjs?");

/***/ }),

/***/ "./node_modules/openai/_shims/index.mjs":
/*!**********************************************!*\
  !*** ./node_modules/openai/_shims/index.mjs ***!
  \**********************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Blob: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.Blob),\n/* harmony export */   File: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.File),\n/* harmony export */   FormData: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.FormData),\n/* harmony export */   Headers: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.Headers),\n/* harmony export */   ReadableStream: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.ReadableStream),\n/* harmony export */   Request: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.Request),\n/* harmony export */   Response: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.Response),\n/* harmony export */   auto: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.auto),\n/* harmony export */   fetch: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.fetch),\n/* harmony export */   fileFromPath: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.fileFromPath),\n/* harmony export */   getDefaultAgent: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.getDefaultAgent),\n/* harmony export */   getMultipartRequestOptions: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.getMultipartRequestOptions),\n/* harmony export */   isFsReadStream: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.isFsReadStream),\n/* harmony export */   kind: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.kind),\n/* harmony export */   setShims: () => (/* reexport safe */ _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.setShims)\n/* harmony export */ });\n/* harmony import */ var _registry_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./registry.mjs */ \"./node_modules/openai/_shims/registry.mjs\");\n/* harmony import */ var openai_shims_auto_runtime__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! openai/_shims/auto/runtime */ \"./node_modules/openai/_shims/web-runtime.mjs\");\n/**\n * Disclaimer: modules in _shims aren't intended to be imported by SDK users.\n */\n\n\nif (!_registry_mjs__WEBPACK_IMPORTED_MODULE_0__.kind) _registry_mjs__WEBPACK_IMPORTED_MODULE_0__.setShims(openai_shims_auto_runtime__WEBPACK_IMPORTED_MODULE_1__.getRuntime(), { auto: true });\n\n\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/_shims/index.mjs?");

/***/ }),

/***/ "./node_modules/openai/_shims/registry.mjs":
/*!*************************************************!*\
  !*** ./node_modules/openai/_shims/registry.mjs ***!
  \*************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Blob: () => (/* binding */ Blob),\n/* harmony export */   File: () => (/* binding */ File),\n/* harmony export */   FormData: () => (/* binding */ FormData),\n/* harmony export */   Headers: () => (/* binding */ Headers),\n/* harmony export */   ReadableStream: () => (/* binding */ ReadableStream),\n/* harmony export */   Request: () => (/* binding */ Request),\n/* harmony export */   Response: () => (/* binding */ Response),\n/* harmony export */   auto: () => (/* binding */ auto),\n/* harmony export */   fetch: () => (/* binding */ fetch),\n/* harmony export */   fileFromPath: () => (/* binding */ fileFromPath),\n/* harmony export */   getDefaultAgent: () => (/* binding */ getDefaultAgent),\n/* harmony export */   getMultipartRequestOptions: () => (/* binding */ getMultipartRequestOptions),\n/* harmony export */   isFsReadStream: () => (/* binding */ isFsReadStream),\n/* harmony export */   kind: () => (/* binding */ kind),\n/* harmony export */   setShims: () => (/* binding */ setShims)\n/* harmony export */ });\nlet auto = false;\nlet kind = undefined;\nlet fetch = undefined;\nlet Request = undefined;\nlet Response = undefined;\nlet Headers = undefined;\nlet FormData = undefined;\nlet Blob = undefined;\nlet File = undefined;\nlet ReadableStream = undefined;\nlet getMultipartRequestOptions = undefined;\nlet getDefaultAgent = undefined;\nlet fileFromPath = undefined;\nlet isFsReadStream = undefined;\nfunction setShims(shims, options = { auto: false }) {\n    if (auto) {\n        throw new Error(`you must \\`import 'openai/shims/${shims.kind}'\\` before importing anything else from openai`);\n    }\n    if (kind) {\n        throw new Error(`can't \\`import 'openai/shims/${shims.kind}'\\` after \\`import 'openai/shims/${kind}'\\``);\n    }\n    auto = options.auto;\n    kind = shims.kind;\n    fetch = shims.fetch;\n    Request = shims.Request;\n    Response = shims.Response;\n    Headers = shims.Headers;\n    FormData = shims.FormData;\n    Blob = shims.Blob;\n    File = shims.File;\n    ReadableStream = shims.ReadableStream;\n    getMultipartRequestOptions = shims.getMultipartRequestOptions;\n    getDefaultAgent = shims.getDefaultAgent;\n    fileFromPath = shims.fileFromPath;\n    isFsReadStream = shims.isFsReadStream;\n}\n//# sourceMappingURL=registry.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/_shims/registry.mjs?");

/***/ }),

/***/ "./node_modules/openai/_shims/web-runtime.mjs":
/*!****************************************************!*\
  !*** ./node_modules/openai/_shims/web-runtime.mjs ***!
  \****************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   getRuntime: () => (/* binding */ getRuntime)\n/* harmony export */ });\n/* harmony import */ var _MultipartBody_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./MultipartBody.mjs */ \"./node_modules/openai/_shims/MultipartBody.mjs\");\n\nfunction getRuntime({ manuallyImported } = {}) {\n    const recommendation = manuallyImported ?\n        `You may need to use polyfills`\n        : `Add one of these imports before your first \\`import … from 'openai'\\`:\n- \\`import 'openai/shims/node'\\` (if you're running on Node)\n- \\`import 'openai/shims/web'\\` (otherwise)\n`;\n    let _fetch, _Request, _Response, _Headers;\n    try {\n        // @ts-ignore\n        _fetch = fetch;\n        // @ts-ignore\n        _Request = Request;\n        // @ts-ignore\n        _Response = Response;\n        // @ts-ignore\n        _Headers = Headers;\n    }\n    catch (error) {\n        throw new Error(`this environment is missing the following Web Fetch API type: ${error.message}. ${recommendation}`);\n    }\n    return {\n        kind: 'web',\n        fetch: _fetch,\n        Request: _Request,\n        Response: _Response,\n        Headers: _Headers,\n        FormData: \n        // @ts-ignore\n        typeof FormData !== 'undefined' ? FormData : (class FormData {\n            // @ts-ignore\n            constructor() {\n                throw new Error(`file uploads aren't supported in this environment yet as 'FormData' is undefined. ${recommendation}`);\n            }\n        }),\n        Blob: typeof Blob !== 'undefined' ? Blob : (class Blob {\n            constructor() {\n                throw new Error(`file uploads aren't supported in this environment yet as 'Blob' is undefined. ${recommendation}`);\n            }\n        }),\n        File: \n        // @ts-ignore\n        typeof File !== 'undefined' ? File : (class File {\n            // @ts-ignore\n            constructor() {\n                throw new Error(`file uploads aren't supported in this environment yet as 'File' is undefined. ${recommendation}`);\n            }\n        }),\n        ReadableStream: \n        // @ts-ignore\n        typeof ReadableStream !== 'undefined' ? ReadableStream : (class ReadableStream {\n            // @ts-ignore\n            constructor() {\n                throw new Error(`streaming isn't supported in this environment yet as 'ReadableStream' is undefined. ${recommendation}`);\n            }\n        }),\n        getMultipartRequestOptions: async (\n        // @ts-ignore\n        form, opts) => ({\n            ...opts,\n            body: new _MultipartBody_mjs__WEBPACK_IMPORTED_MODULE_0__.MultipartBody(form),\n        }),\n        getDefaultAgent: (url) => undefined,\n        fileFromPath: () => {\n            throw new Error('The `fileFromPath` function is only supported in Node. See the README for more details: https://www.github.com/openai/openai-node#file-uploads');\n        },\n        isFsReadStream: (value) => false,\n    };\n}\n//# sourceMappingURL=web-runtime.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/_shims/web-runtime.mjs?");

/***/ }),

/***/ "./node_modules/openai/core.mjs":
/*!**************************************!*\
  !*** ./node_modules/openai/core.mjs ***!
  \**************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   APIClient: () => (/* binding */ APIClient),\n/* harmony export */   APIPromise: () => (/* binding */ APIPromise),\n/* harmony export */   AbstractPage: () => (/* binding */ AbstractPage),\n/* harmony export */   PagePromise: () => (/* binding */ PagePromise),\n/* harmony export */   castToError: () => (/* binding */ castToError),\n/* harmony export */   coerceBoolean: () => (/* binding */ coerceBoolean),\n/* harmony export */   coerceFloat: () => (/* binding */ coerceFloat),\n/* harmony export */   coerceInteger: () => (/* binding */ coerceInteger),\n/* harmony export */   createForm: () => (/* reexport safe */ _uploads_mjs__WEBPACK_IMPORTED_MODULE_1__.createForm),\n/* harmony export */   createResponseHeaders: () => (/* binding */ createResponseHeaders),\n/* harmony export */   debug: () => (/* binding */ debug),\n/* harmony export */   ensurePresent: () => (/* binding */ ensurePresent),\n/* harmony export */   getRequiredHeader: () => (/* binding */ getRequiredHeader),\n/* harmony export */   hasOwn: () => (/* binding */ hasOwn),\n/* harmony export */   isEmptyObj: () => (/* binding */ isEmptyObj),\n/* harmony export */   isHeadersProtocol: () => (/* binding */ isHeadersProtocol),\n/* harmony export */   isObj: () => (/* binding */ isObj),\n/* harmony export */   isRequestOptions: () => (/* binding */ isRequestOptions),\n/* harmony export */   isRunningInBrowser: () => (/* binding */ isRunningInBrowser),\n/* harmony export */   maybeCoerceBoolean: () => (/* binding */ maybeCoerceBoolean),\n/* harmony export */   maybeCoerceFloat: () => (/* binding */ maybeCoerceFloat),\n/* harmony export */   maybeCoerceInteger: () => (/* binding */ maybeCoerceInteger),\n/* harmony export */   maybeMultipartFormRequestOptions: () => (/* reexport safe */ _uploads_mjs__WEBPACK_IMPORTED_MODULE_1__.maybeMultipartFormRequestOptions),\n/* harmony export */   multipartFormRequestOptions: () => (/* reexport safe */ _uploads_mjs__WEBPACK_IMPORTED_MODULE_1__.multipartFormRequestOptions),\n/* harmony export */   readEnv: () => (/* binding */ readEnv),\n/* harmony export */   safeJSON: () => (/* binding */ safeJSON),\n/* harmony export */   sleep: () => (/* binding */ sleep),\n/* harmony export */   toBase64: () => (/* binding */ toBase64)\n/* harmony export */ });\n/* harmony import */ var _version_mjs__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! ./version.mjs */ \"./node_modules/openai/version.mjs\");\n/* harmony import */ var _streaming_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ./streaming.mjs */ \"./node_modules/openai/streaming.mjs\");\n/* harmony import */ var _error_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ./error.mjs */ \"./node_modules/openai/error.mjs\");\n/* harmony import */ var _shims_index_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./_shims/index.mjs */ \"./node_modules/openai/_shims/index.mjs\");\n/* harmony import */ var _uploads_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./uploads.mjs */ \"./node_modules/openai/uploads.mjs\");\nvar __classPrivateFieldSet = (undefined && undefined.__classPrivateFieldSet) || function (receiver, state, value, kind, f) {\n    if (kind === \"m\") throw new TypeError(\"Private method is not writable\");\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a setter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\n    return (kind === \"a\" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value)), value;\n};\nvar __classPrivateFieldGet = (undefined && undefined.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar _AbstractPage_client;\n\n\n\n\n\n\nasync function defaultParseResponse(props) {\n    const { response } = props;\n    if (props.options.stream) {\n        debug('response', response.status, response.url, response.headers, response.body);\n        // Note: there is an invariant here that isn't represented in the type system\n        // that if you set `stream: true` the response type must also be `Stream<T>`\n        if (props.options.__streamClass) {\n            return props.options.__streamClass.fromSSEResponse(response, props.controller);\n        }\n        return _streaming_mjs__WEBPACK_IMPORTED_MODULE_2__.Stream.fromSSEResponse(response, props.controller);\n    }\n    // fetch refuses to read the body when the status code is 204.\n    if (response.status === 204) {\n        return null;\n    }\n    if (props.options.__binaryResponse) {\n        return response;\n    }\n    const contentType = response.headers.get('content-type');\n    const isJSON = contentType?.includes('application/json') || contentType?.includes('application/vnd.api+json');\n    if (isJSON) {\n        const json = await response.json();\n        debug('response', response.status, response.url, response.headers, json);\n        return json;\n    }\n    const text = await response.text();\n    debug('response', response.status, response.url, response.headers, text);\n    // TODO handle blob, arraybuffer, other content types, etc.\n    return text;\n}\n/**\n * A subclass of `Promise` providing additional helper methods\n * for interacting with the SDK.\n */\nclass APIPromise extends Promise {\n    constructor(responsePromise, parseResponse = defaultParseResponse) {\n        super((resolve) => {\n            // this is maybe a bit weird but this has to be a no-op to not implicitly\n            // parse the response body; instead .then, .catch, .finally are overridden\n            // to parse the response\n            resolve(null);\n        });\n        this.responsePromise = responsePromise;\n        this.parseResponse = parseResponse;\n    }\n    _thenUnwrap(transform) {\n        return new APIPromise(this.responsePromise, async (props) => transform(await this.parseResponse(props)));\n    }\n    /**\n     * Gets the raw `Response` instance instead of parsing the response\n     * data.\n     *\n     * If you want to parse the response body but still get the `Response`\n     * instance, you can use {@link withResponse()}.\n     *\n     * 👋 Getting the wrong TypeScript type for `Response`?\n     * Try setting `\"moduleResolution\": \"NodeNext\"` if you can,\n     * or add one of these imports before your first `import … from 'openai'`:\n     * - `import 'openai/shims/node'` (if you're running on Node)\n     * - `import 'openai/shims/web'` (otherwise)\n     */\n    asResponse() {\n        return this.responsePromise.then((p) => p.response);\n    }\n    /**\n     * Gets the parsed response data and the raw `Response` instance.\n     *\n     * If you just want to get the raw `Response` instance without parsing it,\n     * you can use {@link asResponse()}.\n     *\n     *\n     * 👋 Getting the wrong TypeScript type for `Response`?\n     * Try setting `\"moduleResolution\": \"NodeNext\"` if you can,\n     * or add one of these imports before your first `import … from 'openai'`:\n     * - `import 'openai/shims/node'` (if you're running on Node)\n     * - `import 'openai/shims/web'` (otherwise)\n     */\n    async withResponse() {\n        const [data, response] = await Promise.all([this.parse(), this.asResponse()]);\n        return { data, response };\n    }\n    parse() {\n        if (!this.parsedPromise) {\n            this.parsedPromise = this.responsePromise.then(this.parseResponse);\n        }\n        return this.parsedPromise;\n    }\n    then(onfulfilled, onrejected) {\n        return this.parse().then(onfulfilled, onrejected);\n    }\n    catch(onrejected) {\n        return this.parse().catch(onrejected);\n    }\n    finally(onfinally) {\n        return this.parse().finally(onfinally);\n    }\n}\nclass APIClient {\n    constructor({ baseURL, maxRetries = 2, timeout = 600000, // 10 minutes\n    httpAgent, fetch: overridenFetch, }) {\n        this.baseURL = baseURL;\n        this.maxRetries = validatePositiveInteger('maxRetries', maxRetries);\n        this.timeout = validatePositiveInteger('timeout', timeout);\n        this.httpAgent = httpAgent;\n        this.fetch = overridenFetch ?? _shims_index_mjs__WEBPACK_IMPORTED_MODULE_0__.fetch;\n    }\n    authHeaders(opts) {\n        return {};\n    }\n    /**\n     * Override this to add your own default headers, for example:\n     *\n     *  {\n     *    ...super.defaultHeaders(),\n     *    Authorization: 'Bearer 123',\n     *  }\n     */\n    defaultHeaders(opts) {\n        return {\n            Accept: 'application/json',\n            'Content-Type': 'application/json',\n            'User-Agent': this.getUserAgent(),\n            ...getPlatformHeaders(),\n            ...this.authHeaders(opts),\n        };\n    }\n    /**\n     * Override this to add your own headers validation:\n     */\n    validateHeaders(headers, customHeaders) { }\n    defaultIdempotencyKey() {\n        return `stainless-node-retry-${uuid4()}`;\n    }\n    get(path, opts) {\n        return this.methodRequest('get', path, opts);\n    }\n    post(path, opts) {\n        return this.methodRequest('post', path, opts);\n    }\n    patch(path, opts) {\n        return this.methodRequest('patch', path, opts);\n    }\n    put(path, opts) {\n        return this.methodRequest('put', path, opts);\n    }\n    delete(path, opts) {\n        return this.methodRequest('delete', path, opts);\n    }\n    methodRequest(method, path, opts) {\n        return this.request(Promise.resolve(opts).then(async (opts) => {\n            const body = opts && (0,_uploads_mjs__WEBPACK_IMPORTED_MODULE_1__.isBlobLike)(opts?.body) ? new DataView(await opts.body.arrayBuffer())\n                : opts?.body instanceof DataView ? opts.body\n                    : opts?.body instanceof ArrayBuffer ? new DataView(opts.body)\n                        : opts && ArrayBuffer.isView(opts?.body) ? new DataView(opts.body.buffer)\n                            : opts?.body;\n            return { method, path, ...opts, body };\n        }));\n    }\n    getAPIList(path, Page, opts) {\n        return this.requestAPIList(Page, { method: 'get', path, ...opts });\n    }\n    calculateContentLength(body) {\n        if (typeof body === 'string') {\n            if (typeof Buffer !== 'undefined') {\n                return Buffer.byteLength(body, 'utf8').toString();\n            }\n            if (typeof TextEncoder !== 'undefined') {\n                const encoder = new TextEncoder();\n                const encoded = encoder.encode(body);\n                return encoded.length.toString();\n            }\n        }\n        else if (ArrayBuffer.isView(body)) {\n            return body.byteLength.toString();\n        }\n        return null;\n    }\n    buildRequest(options) {\n        const { method, path, query, headers: headers = {} } = options;\n        const body = ArrayBuffer.isView(options.body) || (options.__binaryRequest && typeof options.body === 'string') ?\n            options.body\n            : (0,_uploads_mjs__WEBPACK_IMPORTED_MODULE_1__.isMultipartBody)(options.body) ? options.body.body\n                : options.body ? JSON.stringify(options.body, null, 2)\n                    : null;\n        const contentLength = this.calculateContentLength(body);\n        const url = this.buildURL(path, query);\n        if ('timeout' in options)\n            validatePositiveInteger('timeout', options.timeout);\n        const timeout = options.timeout ?? this.timeout;\n        const httpAgent = options.httpAgent ?? this.httpAgent ?? (0,_shims_index_mjs__WEBPACK_IMPORTED_MODULE_0__.getDefaultAgent)(url);\n        const minAgentTimeout = timeout + 1000;\n        if (typeof httpAgent?.options?.timeout === 'number' &&\n            minAgentTimeout > (httpAgent.options.timeout ?? 0)) {\n            // Allow any given request to bump our agent active socket timeout.\n            // This may seem strange, but leaking active sockets should be rare and not particularly problematic,\n            // and without mutating agent we would need to create more of them.\n            // This tradeoff optimizes for performance.\n            httpAgent.options.timeout = minAgentTimeout;\n        }\n        if (this.idempotencyHeader && method !== 'get') {\n            if (!options.idempotencyKey)\n                options.idempotencyKey = this.defaultIdempotencyKey();\n            headers[this.idempotencyHeader] = options.idempotencyKey;\n        }\n        const reqHeaders = this.buildHeaders({ options, headers, contentLength });\n        const req = {\n            method,\n            ...(body && { body: body }),\n            headers: reqHeaders,\n            ...(httpAgent && { agent: httpAgent }),\n            // @ts-ignore node-fetch uses a custom AbortSignal type that is\n            // not compatible with standard web types\n            signal: options.signal ?? null,\n        };\n        return { req, url, timeout };\n    }\n    buildHeaders({ options, headers, contentLength, }) {\n        const reqHeaders = {};\n        if (contentLength) {\n            reqHeaders['content-length'] = contentLength;\n        }\n        const defaultHeaders = this.defaultHeaders(options);\n        applyHeadersMut(reqHeaders, defaultHeaders);\n        applyHeadersMut(reqHeaders, headers);\n        // let builtin fetch set the Content-Type for multipart bodies\n        if ((0,_uploads_mjs__WEBPACK_IMPORTED_MODULE_1__.isMultipartBody)(options.body) && _shims_index_mjs__WEBPACK_IMPORTED_MODULE_0__.kind !== 'node') {\n            delete reqHeaders['content-type'];\n        }\n        this.validateHeaders(reqHeaders, headers);\n        return reqHeaders;\n    }\n    /**\n     * Used as a callback for mutating the given `FinalRequestOptions` object.\n     */\n    async prepareOptions(options) { }\n    /**\n     * Used as a callback for mutating the given `RequestInit` object.\n     *\n     * This is useful for cases where you want to add certain headers based off of\n     * the request properties, e.g. `method` or `url`.\n     */\n    async prepareRequest(request, { url, options }) { }\n    parseHeaders(headers) {\n        return (!headers ? {}\n            : Symbol.iterator in headers ?\n                Object.fromEntries(Array.from(headers).map((header) => [...header]))\n                : { ...headers });\n    }\n    makeStatusError(status, error, message, headers) {\n        return _error_mjs__WEBPACK_IMPORTED_MODULE_3__.APIError.generate(status, error, message, headers);\n    }\n    request(options, remainingRetries = null) {\n        return new APIPromise(this.makeRequest(options, remainingRetries));\n    }\n    async makeRequest(optionsInput, retriesRemaining) {\n        const options = await optionsInput;\n        if (retriesRemaining == null) {\n            retriesRemaining = options.maxRetries ?? this.maxRetries;\n        }\n        await this.prepareOptions(options);\n        const { req, url, timeout } = this.buildRequest(options);\n        await this.prepareRequest(req, { url, options });\n        debug('request', url, options, req.headers);\n        if (options.signal?.aborted) {\n            throw new _error_mjs__WEBPACK_IMPORTED_MODULE_3__.APIUserAbortError();\n        }\n        const controller = new AbortController();\n        const response = await this.fetchWithTimeout(url, req, timeout, controller).catch(castToError);\n        if (response instanceof Error) {\n            if (options.signal?.aborted) {\n                throw new _error_mjs__WEBPACK_IMPORTED_MODULE_3__.APIUserAbortError();\n            }\n            if (retriesRemaining) {\n                return this.retryRequest(options, retriesRemaining);\n            }\n            if (response.name === 'AbortError') {\n                throw new _error_mjs__WEBPACK_IMPORTED_MODULE_3__.APIConnectionTimeoutError();\n            }\n            throw new _error_mjs__WEBPACK_IMPORTED_MODULE_3__.APIConnectionError({ cause: response });\n        }\n        const responseHeaders = createResponseHeaders(response.headers);\n        if (!response.ok) {\n            if (retriesRemaining && this.shouldRetry(response)) {\n                const retryMessage = `retrying, ${retriesRemaining} attempts remaining`;\n                debug(`response (error; ${retryMessage})`, response.status, url, responseHeaders);\n                return this.retryRequest(options, retriesRemaining, responseHeaders);\n            }\n            const errText = await response.text().catch((e) => castToError(e).message);\n            const errJSON = safeJSON(errText);\n            const errMessage = errJSON ? undefined : errText;\n            const retryMessage = retriesRemaining ? `(error; no more retries left)` : `(error; not retryable)`;\n            debug(`response (error; ${retryMessage})`, response.status, url, responseHeaders, errMessage);\n            const err = this.makeStatusError(response.status, errJSON, errMessage, responseHeaders);\n            throw err;\n        }\n        return { response, options, controller };\n    }\n    requestAPIList(Page, options) {\n        const request = this.makeRequest(options, null);\n        return new PagePromise(this, request, Page);\n    }\n    buildURL(path, query) {\n        const url = isAbsoluteURL(path) ?\n            new URL(path)\n            : new URL(this.baseURL + (this.baseURL.endsWith('/') && path.startsWith('/') ? path.slice(1) : path));\n        const defaultQuery = this.defaultQuery();\n        if (!isEmptyObj(defaultQuery)) {\n            query = { ...defaultQuery, ...query };\n        }\n        if (typeof query === 'object' && query && !Array.isArray(query)) {\n            url.search = this.stringifyQuery(query);\n        }\n        return url.toString();\n    }\n    stringifyQuery(query) {\n        return Object.entries(query)\n            .filter(([_, value]) => typeof value !== 'undefined')\n            .map(([key, value]) => {\n            if (typeof value === 'string' || typeof value === 'number' || typeof value === 'boolean') {\n                return `${encodeURIComponent(key)}=${encodeURIComponent(value)}`;\n            }\n            if (value === null) {\n                return `${encodeURIComponent(key)}=`;\n            }\n            throw new _error_mjs__WEBPACK_IMPORTED_MODULE_3__.OpenAIError(`Cannot stringify type ${typeof value}; Expected string, number, boolean, or null. If you need to pass nested query parameters, you can manually encode them, e.g. { query: { 'foo[key1]': value1, 'foo[key2]': value2 } }, and please open a GitHub issue requesting better support for your use case.`);\n        })\n            .join('&');\n    }\n    async fetchWithTimeout(url, init, ms, controller) {\n        const { signal, ...options } = init || {};\n        if (signal)\n            signal.addEventListener('abort', () => controller.abort());\n        const timeout = setTimeout(() => controller.abort(), ms);\n        return (this.getRequestClient()\n            // use undefined this binding; fetch errors if bound to something else in browser/cloudflare\n            .fetch.call(undefined, url, { signal: controller.signal, ...options })\n            .finally(() => {\n            clearTimeout(timeout);\n        }));\n    }\n    getRequestClient() {\n        return { fetch: this.fetch };\n    }\n    shouldRetry(response) {\n        // Note this is not a standard header.\n        const shouldRetryHeader = response.headers.get('x-should-retry');\n        // If the server explicitly says whether or not to retry, obey.\n        if (shouldRetryHeader === 'true')\n            return true;\n        if (shouldRetryHeader === 'false')\n            return false;\n        // Retry on request timeouts.\n        if (response.status === 408)\n            return true;\n        // Retry on lock timeouts.\n        if (response.status === 409)\n            return true;\n        // Retry on rate limits.\n        if (response.status === 429)\n            return true;\n        // Retry internal errors.\n        if (response.status >= 500)\n            return true;\n        return false;\n    }\n    async retryRequest(options, retriesRemaining, responseHeaders) {\n        let timeoutMillis;\n        // Note the `retry-after-ms` header may not be standard, but is a good idea and we'd like proactive support for it.\n        const retryAfterMillisHeader = responseHeaders?.['retry-after-ms'];\n        if (retryAfterMillisHeader) {\n            const timeoutMs = parseFloat(retryAfterMillisHeader);\n            if (!Number.isNaN(timeoutMs)) {\n                timeoutMillis = timeoutMs;\n            }\n        }\n        // About the Retry-After header: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Retry-After\n        const retryAfterHeader = responseHeaders?.['retry-after'];\n        if (retryAfterHeader && !timeoutMillis) {\n            const timeoutSeconds = parseFloat(retryAfterHeader);\n            if (!Number.isNaN(timeoutSeconds)) {\n                timeoutMillis = timeoutSeconds * 1000;\n            }\n            else {\n                timeoutMillis = Date.parse(retryAfterHeader) - Date.now();\n            }\n        }\n        // If the API asks us to wait a certain amount of time (and it's a reasonable amount),\n        // just do what it says, but otherwise calculate a default\n        if (!(timeoutMillis && 0 <= timeoutMillis && timeoutMillis < 60 * 1000)) {\n            const maxRetries = options.maxRetries ?? this.maxRetries;\n            timeoutMillis = this.calculateDefaultRetryTimeoutMillis(retriesRemaining, maxRetries);\n        }\n        await sleep(timeoutMillis);\n        return this.makeRequest(options, retriesRemaining - 1);\n    }\n    calculateDefaultRetryTimeoutMillis(retriesRemaining, maxRetries) {\n        const initialRetryDelay = 0.5;\n        const maxRetryDelay = 8.0;\n        const numRetries = maxRetries - retriesRemaining;\n        // Apply exponential backoff, but not more than the max.\n        const sleepSeconds = Math.min(initialRetryDelay * Math.pow(2, numRetries), maxRetryDelay);\n        // Apply some jitter, take up to at most 25 percent of the retry time.\n        const jitter = 1 - Math.random() * 0.25;\n        return sleepSeconds * jitter * 1000;\n    }\n    getUserAgent() {\n        return `${this.constructor.name}/JS ${_version_mjs__WEBPACK_IMPORTED_MODULE_4__.VERSION}`;\n    }\n}\nclass AbstractPage {\n    constructor(client, response, body, options) {\n        _AbstractPage_client.set(this, void 0);\n        __classPrivateFieldSet(this, _AbstractPage_client, client, \"f\");\n        this.options = options;\n        this.response = response;\n        this.body = body;\n    }\n    hasNextPage() {\n        const items = this.getPaginatedItems();\n        if (!items.length)\n            return false;\n        return this.nextPageInfo() != null;\n    }\n    async getNextPage() {\n        const nextInfo = this.nextPageInfo();\n        if (!nextInfo) {\n            throw new _error_mjs__WEBPACK_IMPORTED_MODULE_3__.OpenAIError('No next page expected; please check `.hasNextPage()` before calling `.getNextPage()`.');\n        }\n        const nextOptions = { ...this.options };\n        if ('params' in nextInfo && typeof nextOptions.query === 'object') {\n            nextOptions.query = { ...nextOptions.query, ...nextInfo.params };\n        }\n        else if ('url' in nextInfo) {\n            const params = [...Object.entries(nextOptions.query || {}), ...nextInfo.url.searchParams.entries()];\n            for (const [key, value] of params) {\n                nextInfo.url.searchParams.set(key, value);\n            }\n            nextOptions.query = undefined;\n            nextOptions.path = nextInfo.url.toString();\n        }\n        return await __classPrivateFieldGet(this, _AbstractPage_client, \"f\").requestAPIList(this.constructor, nextOptions);\n    }\n    async *iterPages() {\n        // eslint-disable-next-line @typescript-eslint/no-this-alias\n        let page = this;\n        yield page;\n        while (page.hasNextPage()) {\n            page = await page.getNextPage();\n            yield page;\n        }\n    }\n    async *[(_AbstractPage_client = new WeakMap(), Symbol.asyncIterator)]() {\n        for await (const page of this.iterPages()) {\n            for (const item of page.getPaginatedItems()) {\n                yield item;\n            }\n        }\n    }\n}\n/**\n * This subclass of Promise will resolve to an instantiated Page once the request completes.\n *\n * It also implements AsyncIterable to allow auto-paginating iteration on an unawaited list call, eg:\n *\n *    for await (const item of client.items.list()) {\n *      console.log(item)\n *    }\n */\nclass PagePromise extends APIPromise {\n    constructor(client, request, Page) {\n        super(request, async (props) => new Page(client, props.response, await defaultParseResponse(props), props.options));\n    }\n    /**\n     * Allow auto-paginating iteration on an unawaited list call, eg:\n     *\n     *    for await (const item of client.items.list()) {\n     *      console.log(item)\n     *    }\n     */\n    async *[Symbol.asyncIterator]() {\n        const page = await this;\n        for await (const item of page) {\n            yield item;\n        }\n    }\n}\nconst createResponseHeaders = (headers) => {\n    return new Proxy(Object.fromEntries(\n    // @ts-ignore\n    headers.entries()), {\n        get(target, name) {\n            const key = name.toString();\n            return target[key.toLowerCase()] || target[key];\n        },\n    });\n};\n// This is required so that we can determine if a given object matches the RequestOptions\n// type at runtime. While this requires duplication, it is enforced by the TypeScript\n// compiler such that any missing / extraneous keys will cause an error.\nconst requestOptionsKeys = {\n    method: true,\n    path: true,\n    query: true,\n    body: true,\n    headers: true,\n    maxRetries: true,\n    stream: true,\n    timeout: true,\n    httpAgent: true,\n    signal: true,\n    idempotencyKey: true,\n    __binaryRequest: true,\n    __binaryResponse: true,\n    __streamClass: true,\n};\nconst isRequestOptions = (obj) => {\n    return (typeof obj === 'object' &&\n        obj !== null &&\n        !isEmptyObj(obj) &&\n        Object.keys(obj).every((k) => hasOwn(requestOptionsKeys, k)));\n};\nconst getPlatformProperties = () => {\n    if (typeof Deno !== 'undefined' && Deno.build != null) {\n        return {\n            'X-Stainless-Lang': 'js',\n            'X-Stainless-Package-Version': _version_mjs__WEBPACK_IMPORTED_MODULE_4__.VERSION,\n            'X-Stainless-OS': normalizePlatform(Deno.build.os),\n            'X-Stainless-Arch': normalizeArch(Deno.build.arch),\n            'X-Stainless-Runtime': 'deno',\n            'X-Stainless-Runtime-Version': typeof Deno.version === 'string' ? Deno.version : Deno.version?.deno ?? 'unknown',\n        };\n    }\n    if (typeof EdgeRuntime !== 'undefined') {\n        return {\n            'X-Stainless-Lang': 'js',\n            'X-Stainless-Package-Version': _version_mjs__WEBPACK_IMPORTED_MODULE_4__.VERSION,\n            'X-Stainless-OS': 'Unknown',\n            'X-Stainless-Arch': `other:${EdgeRuntime}`,\n            'X-Stainless-Runtime': 'edge',\n            'X-Stainless-Runtime-Version': process.version,\n        };\n    }\n    // Check if Node.js\n    if (Object.prototype.toString.call(typeof process !== 'undefined' ? process : 0) === '[object process]') {\n        return {\n            'X-Stainless-Lang': 'js',\n            'X-Stainless-Package-Version': _version_mjs__WEBPACK_IMPORTED_MODULE_4__.VERSION,\n            'X-Stainless-OS': normalizePlatform(process.platform),\n            'X-Stainless-Arch': normalizeArch(process.arch),\n            'X-Stainless-Runtime': 'node',\n            'X-Stainless-Runtime-Version': process.version,\n        };\n    }\n    const browserInfo = getBrowserInfo();\n    if (browserInfo) {\n        return {\n            'X-Stainless-Lang': 'js',\n            'X-Stainless-Package-Version': _version_mjs__WEBPACK_IMPORTED_MODULE_4__.VERSION,\n            'X-Stainless-OS': 'Unknown',\n            'X-Stainless-Arch': 'unknown',\n            'X-Stainless-Runtime': `browser:${browserInfo.browser}`,\n            'X-Stainless-Runtime-Version': browserInfo.version,\n        };\n    }\n    // TODO add support for Cloudflare workers, etc.\n    return {\n        'X-Stainless-Lang': 'js',\n        'X-Stainless-Package-Version': _version_mjs__WEBPACK_IMPORTED_MODULE_4__.VERSION,\n        'X-Stainless-OS': 'Unknown',\n        'X-Stainless-Arch': 'unknown',\n        'X-Stainless-Runtime': 'unknown',\n        'X-Stainless-Runtime-Version': 'unknown',\n    };\n};\n// Note: modified from https://github.com/JS-DevTools/host-environment/blob/b1ab79ecde37db5d6e163c050e54fe7d287d7c92/src/isomorphic.browser.ts\nfunction getBrowserInfo() {\n    if (typeof navigator === 'undefined' || !navigator) {\n        return null;\n    }\n    // NOTE: The order matters here!\n    const browserPatterns = [\n        { key: 'edge', pattern: /Edge(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n        { key: 'ie', pattern: /MSIE(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n        { key: 'ie', pattern: /Trident(?:.*rv\\:(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n        { key: 'chrome', pattern: /Chrome(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n        { key: 'firefox', pattern: /Firefox(?:\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?/ },\n        { key: 'safari', pattern: /(?:Version\\W+(\\d+)\\.(\\d+)(?:\\.(\\d+))?)?(?:\\W+Mobile\\S*)?\\W+Safari/ },\n    ];\n    // Find the FIRST matching browser\n    for (const { key, pattern } of browserPatterns) {\n        const match = pattern.exec(navigator.userAgent);\n        if (match) {\n            const major = match[1] || 0;\n            const minor = match[2] || 0;\n            const patch = match[3] || 0;\n            return { browser: key, version: `${major}.${minor}.${patch}` };\n        }\n    }\n    return null;\n}\nconst normalizeArch = (arch) => {\n    // Node docs:\n    // - https://nodejs.org/api/process.html#processarch\n    // Deno docs:\n    // - https://doc.deno.land/deno/stable/~/Deno.build\n    if (arch === 'x32')\n        return 'x32';\n    if (arch === 'x86_64' || arch === 'x64')\n        return 'x64';\n    if (arch === 'arm')\n        return 'arm';\n    if (arch === 'aarch64' || arch === 'arm64')\n        return 'arm64';\n    if (arch)\n        return `other:${arch}`;\n    return 'unknown';\n};\nconst normalizePlatform = (platform) => {\n    // Node platforms:\n    // - https://nodejs.org/api/process.html#processplatform\n    // Deno platforms:\n    // - https://doc.deno.land/deno/stable/~/Deno.build\n    // - https://github.com/denoland/deno/issues/14799\n    platform = platform.toLowerCase();\n    // NOTE: this iOS check is untested and may not work\n    // Node does not work natively on IOS, there is a fork at\n    // https://github.com/nodejs-mobile/nodejs-mobile\n    // however it is unknown at the time of writing how to detect if it is running\n    if (platform.includes('ios'))\n        return 'iOS';\n    if (platform === 'android')\n        return 'Android';\n    if (platform === 'darwin')\n        return 'MacOS';\n    if (platform === 'win32')\n        return 'Windows';\n    if (platform === 'freebsd')\n        return 'FreeBSD';\n    if (platform === 'openbsd')\n        return 'OpenBSD';\n    if (platform === 'linux')\n        return 'Linux';\n    if (platform)\n        return `Other:${platform}`;\n    return 'Unknown';\n};\nlet _platformHeaders;\nconst getPlatformHeaders = () => {\n    return (_platformHeaders ?? (_platformHeaders = getPlatformProperties()));\n};\nconst safeJSON = (text) => {\n    try {\n        return JSON.parse(text);\n    }\n    catch (err) {\n        return undefined;\n    }\n};\n// https://stackoverflow.com/a/19709846\nconst startsWithSchemeRegexp = new RegExp('^(?:[a-z]+:)?//', 'i');\nconst isAbsoluteURL = (url) => {\n    return startsWithSchemeRegexp.test(url);\n};\nconst sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));\nconst validatePositiveInteger = (name, n) => {\n    if (typeof n !== 'number' || !Number.isInteger(n)) {\n        throw new _error_mjs__WEBPACK_IMPORTED_MODULE_3__.OpenAIError(`${name} must be an integer`);\n    }\n    if (n < 0) {\n        throw new _error_mjs__WEBPACK_IMPORTED_MODULE_3__.OpenAIError(`${name} must be a positive integer`);\n    }\n    return n;\n};\nconst castToError = (err) => {\n    if (err instanceof Error)\n        return err;\n    return new Error(err);\n};\nconst ensurePresent = (value) => {\n    if (value == null)\n        throw new _error_mjs__WEBPACK_IMPORTED_MODULE_3__.OpenAIError(`Expected a value to be given but received ${value} instead.`);\n    return value;\n};\n/**\n * Read an environment variable.\n *\n * Trims beginning and trailing whitespace.\n *\n * Will return undefined if the environment variable doesn't exist or cannot be accessed.\n */\nconst readEnv = (env) => {\n    if (typeof process !== 'undefined') {\n        return process.env?.[env]?.trim() ?? undefined;\n    }\n    if (typeof Deno !== 'undefined') {\n        return Deno.env?.get?.(env)?.trim();\n    }\n    return undefined;\n};\nconst coerceInteger = (value) => {\n    if (typeof value === 'number')\n        return Math.round(value);\n    if (typeof value === 'string')\n        return parseInt(value, 10);\n    throw new _error_mjs__WEBPACK_IMPORTED_MODULE_3__.OpenAIError(`Could not coerce ${value} (type: ${typeof value}) into a number`);\n};\nconst coerceFloat = (value) => {\n    if (typeof value === 'number')\n        return value;\n    if (typeof value === 'string')\n        return parseFloat(value);\n    throw new _error_mjs__WEBPACK_IMPORTED_MODULE_3__.OpenAIError(`Could not coerce ${value} (type: ${typeof value}) into a number`);\n};\nconst coerceBoolean = (value) => {\n    if (typeof value === 'boolean')\n        return value;\n    if (typeof value === 'string')\n        return value === 'true';\n    return Boolean(value);\n};\nconst maybeCoerceInteger = (value) => {\n    if (value === undefined) {\n        return undefined;\n    }\n    return coerceInteger(value);\n};\nconst maybeCoerceFloat = (value) => {\n    if (value === undefined) {\n        return undefined;\n    }\n    return coerceFloat(value);\n};\nconst maybeCoerceBoolean = (value) => {\n    if (value === undefined) {\n        return undefined;\n    }\n    return coerceBoolean(value);\n};\n// https://stackoverflow.com/a/34491287\nfunction isEmptyObj(obj) {\n    if (!obj)\n        return true;\n    for (const _k in obj)\n        return false;\n    return true;\n}\n// https://eslint.org/docs/latest/rules/no-prototype-builtins\nfunction hasOwn(obj, key) {\n    return Object.prototype.hasOwnProperty.call(obj, key);\n}\n/**\n * Copies headers from \"newHeaders\" onto \"targetHeaders\",\n * using lower-case for all properties,\n * ignoring any keys with undefined values,\n * and deleting any keys with null values.\n */\nfunction applyHeadersMut(targetHeaders, newHeaders) {\n    for (const k in newHeaders) {\n        if (!hasOwn(newHeaders, k))\n            continue;\n        const lowerKey = k.toLowerCase();\n        if (!lowerKey)\n            continue;\n        const val = newHeaders[k];\n        if (val === null) {\n            delete targetHeaders[lowerKey];\n        }\n        else if (val !== undefined) {\n            targetHeaders[lowerKey] = val;\n        }\n    }\n}\nfunction debug(action, ...args) {\n    if (typeof process !== 'undefined' && process?.env?.['DEBUG'] === 'true') {\n        console.log(`OpenAI:DEBUG:${action}`, ...args);\n    }\n}\n/**\n * https://stackoverflow.com/a/2117523\n */\nconst uuid4 = () => {\n    return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, (c) => {\n        const r = (Math.random() * 16) | 0;\n        const v = c === 'x' ? r : (r & 0x3) | 0x8;\n        return v.toString(16);\n    });\n};\nconst isRunningInBrowser = () => {\n    return (\n    // @ts-ignore\n    typeof window !== 'undefined' &&\n        // @ts-ignore\n        typeof window.document !== 'undefined' &&\n        // @ts-ignore\n        typeof navigator !== 'undefined');\n};\nconst isHeadersProtocol = (headers) => {\n    return typeof headers?.get === 'function';\n};\nconst getRequiredHeader = (headers, header) => {\n    const lowerCasedHeader = header.toLowerCase();\n    if (isHeadersProtocol(headers)) {\n        // to deal with the case where the header looks like Stainless-Event-Id\n        const intercapsHeader = header[0]?.toUpperCase() +\n            header.substring(1).replace(/([^\\w])(\\w)/g, (_m, g1, g2) => g1 + g2.toUpperCase());\n        for (const key of [header, lowerCasedHeader, header.toUpperCase(), intercapsHeader]) {\n            const value = headers.get(key);\n            if (value) {\n                return value;\n            }\n        }\n    }\n    for (const [key, value] of Object.entries(headers)) {\n        if (key.toLowerCase() === lowerCasedHeader) {\n            if (Array.isArray(value)) {\n                if (value.length <= 1)\n                    return value[0];\n                console.warn(`Received ${value.length} entries for the ${header} header, using the first entry.`);\n                return value[0];\n            }\n            return value;\n        }\n    }\n    throw new Error(`Could not find ${header} header`);\n};\n/**\n * Encodes a string to Base64 format.\n */\nconst toBase64 = (str) => {\n    if (!str)\n        return '';\n    if (typeof Buffer !== 'undefined') {\n        return Buffer.from(str).toString('base64');\n    }\n    if (typeof btoa !== 'undefined') {\n        return btoa(str);\n    }\n    throw new _error_mjs__WEBPACK_IMPORTED_MODULE_3__.OpenAIError('Cannot generate b64 string; Expected `Buffer` or `btoa` to be defined');\n};\nfunction isObj(obj) {\n    return obj != null && typeof obj === 'object' && !Array.isArray(obj);\n}\n//# sourceMappingURL=core.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/core.mjs?");

/***/ }),

/***/ "./node_modules/openai/error.mjs":
/*!***************************************!*\
  !*** ./node_modules/openai/error.mjs ***!
  \***************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   APIConnectionError: () => (/* binding */ APIConnectionError),\n/* harmony export */   APIConnectionTimeoutError: () => (/* binding */ APIConnectionTimeoutError),\n/* harmony export */   APIError: () => (/* binding */ APIError),\n/* harmony export */   APIUserAbortError: () => (/* binding */ APIUserAbortError),\n/* harmony export */   AuthenticationError: () => (/* binding */ AuthenticationError),\n/* harmony export */   BadRequestError: () => (/* binding */ BadRequestError),\n/* harmony export */   ConflictError: () => (/* binding */ ConflictError),\n/* harmony export */   InternalServerError: () => (/* binding */ InternalServerError),\n/* harmony export */   NotFoundError: () => (/* binding */ NotFoundError),\n/* harmony export */   OpenAIError: () => (/* binding */ OpenAIError),\n/* harmony export */   PermissionDeniedError: () => (/* binding */ PermissionDeniedError),\n/* harmony export */   RateLimitError: () => (/* binding */ RateLimitError),\n/* harmony export */   UnprocessableEntityError: () => (/* binding */ UnprocessableEntityError)\n/* harmony export */ });\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./core.mjs */ \"./node_modules/openai/core.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nclass OpenAIError extends Error {\n}\nclass APIError extends OpenAIError {\n    constructor(status, error, message, headers) {\n        super(`${APIError.makeMessage(status, error, message)}`);\n        this.status = status;\n        this.headers = headers;\n        this.request_id = headers?.['x-request-id'];\n        const data = error;\n        this.error = data;\n        this.code = data?.['code'];\n        this.param = data?.['param'];\n        this.type = data?.['type'];\n    }\n    static makeMessage(status, error, message) {\n        const msg = error?.message ?\n            typeof error.message === 'string' ?\n                error.message\n                : JSON.stringify(error.message)\n            : error ? JSON.stringify(error)\n                : message;\n        if (status && msg) {\n            return `${status} ${msg}`;\n        }\n        if (status) {\n            return `${status} status code (no body)`;\n        }\n        if (msg) {\n            return msg;\n        }\n        return '(no status code or body)';\n    }\n    static generate(status, errorResponse, message, headers) {\n        if (!status) {\n            return new APIConnectionError({ cause: (0,_core_mjs__WEBPACK_IMPORTED_MODULE_0__.castToError)(errorResponse) });\n        }\n        const error = errorResponse?.['error'];\n        if (status === 400) {\n            return new BadRequestError(status, error, message, headers);\n        }\n        if (status === 401) {\n            return new AuthenticationError(status, error, message, headers);\n        }\n        if (status === 403) {\n            return new PermissionDeniedError(status, error, message, headers);\n        }\n        if (status === 404) {\n            return new NotFoundError(status, error, message, headers);\n        }\n        if (status === 409) {\n            return new ConflictError(status, error, message, headers);\n        }\n        if (status === 422) {\n            return new UnprocessableEntityError(status, error, message, headers);\n        }\n        if (status === 429) {\n            return new RateLimitError(status, error, message, headers);\n        }\n        if (status >= 500) {\n            return new InternalServerError(status, error, message, headers);\n        }\n        return new APIError(status, error, message, headers);\n    }\n}\nclass APIUserAbortError extends APIError {\n    constructor({ message } = {}) {\n        super(undefined, undefined, message || 'Request was aborted.', undefined);\n        this.status = undefined;\n    }\n}\nclass APIConnectionError extends APIError {\n    constructor({ message, cause }) {\n        super(undefined, undefined, message || 'Connection error.', undefined);\n        this.status = undefined;\n        // in some environments the 'cause' property is already declared\n        // @ts-ignore\n        if (cause)\n            this.cause = cause;\n    }\n}\nclass APIConnectionTimeoutError extends APIConnectionError {\n    constructor({ message } = {}) {\n        super({ message: message ?? 'Request timed out.' });\n    }\n}\nclass BadRequestError extends APIError {\n    constructor() {\n        super(...arguments);\n        this.status = 400;\n    }\n}\nclass AuthenticationError extends APIError {\n    constructor() {\n        super(...arguments);\n        this.status = 401;\n    }\n}\nclass PermissionDeniedError extends APIError {\n    constructor() {\n        super(...arguments);\n        this.status = 403;\n    }\n}\nclass NotFoundError extends APIError {\n    constructor() {\n        super(...arguments);\n        this.status = 404;\n    }\n}\nclass ConflictError extends APIError {\n    constructor() {\n        super(...arguments);\n        this.status = 409;\n    }\n}\nclass UnprocessableEntityError extends APIError {\n    constructor() {\n        super(...arguments);\n        this.status = 422;\n    }\n}\nclass RateLimitError extends APIError {\n    constructor() {\n        super(...arguments);\n        this.status = 429;\n    }\n}\nclass InternalServerError extends APIError {\n}\n//# sourceMappingURL=error.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/error.mjs?");

/***/ }),

/***/ "./node_modules/openai/index.mjs":
/*!***************************************!*\
  !*** ./node_modules/openai/index.mjs ***!
  \***************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   APIConnectionError: () => (/* binding */ APIConnectionError),\n/* harmony export */   APIConnectionTimeoutError: () => (/* binding */ APIConnectionTimeoutError),\n/* harmony export */   APIError: () => (/* binding */ APIError),\n/* harmony export */   APIUserAbortError: () => (/* binding */ APIUserAbortError),\n/* harmony export */   AuthenticationError: () => (/* binding */ AuthenticationError),\n/* harmony export */   AzureOpenAI: () => (/* binding */ AzureOpenAI),\n/* harmony export */   BadRequestError: () => (/* binding */ BadRequestError),\n/* harmony export */   ConflictError: () => (/* binding */ ConflictError),\n/* harmony export */   InternalServerError: () => (/* binding */ InternalServerError),\n/* harmony export */   NotFoundError: () => (/* binding */ NotFoundError),\n/* harmony export */   OpenAI: () => (/* binding */ OpenAI),\n/* harmony export */   OpenAIError: () => (/* binding */ OpenAIError),\n/* harmony export */   PermissionDeniedError: () => (/* binding */ PermissionDeniedError),\n/* harmony export */   RateLimitError: () => (/* binding */ RateLimitError),\n/* harmony export */   UnprocessableEntityError: () => (/* binding */ UnprocessableEntityError),\n/* harmony export */   \"default\": () => (__WEBPACK_DEFAULT_EXPORT__),\n/* harmony export */   fileFromPath: () => (/* binding */ fileFromPath),\n/* harmony export */   toFile: () => (/* binding */ toFile)\n/* harmony export */ });\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./core.mjs */ \"./node_modules/openai/core.mjs\");\n/* harmony import */ var _error_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./error.mjs */ \"./node_modules/openai/error.mjs\");\n/* harmony import */ var _uploads_mjs__WEBPACK_IMPORTED_MODULE_13__ = __webpack_require__(/*! ./uploads.mjs */ \"./node_modules/openai/uploads.mjs\");\n/* harmony import */ var _uploads_mjs__WEBPACK_IMPORTED_MODULE_14__ = __webpack_require__(/*! ./uploads.mjs */ \"./node_modules/openai/_shims/index.mjs\");\n/* harmony import */ var _pagination_mjs__WEBPACK_IMPORTED_MODULE_15__ = __webpack_require__(/*! ./pagination.mjs */ \"./node_modules/openai/pagination.mjs\");\n/* harmony import */ var _resources_index_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ./resources/index.mjs */ \"./node_modules/openai/resources/completions.mjs\");\n/* harmony import */ var _resources_index_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ./resources/index.mjs */ \"./node_modules/openai/resources/chat/chat.mjs\");\n/* harmony import */ var _resources_index_mjs__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! ./resources/index.mjs */ \"./node_modules/openai/resources/embeddings.mjs\");\n/* harmony import */ var _resources_index_mjs__WEBPACK_IMPORTED_MODULE_5__ = __webpack_require__(/*! ./resources/index.mjs */ \"./node_modules/openai/resources/files.mjs\");\n/* harmony import */ var _resources_index_mjs__WEBPACK_IMPORTED_MODULE_6__ = __webpack_require__(/*! ./resources/index.mjs */ \"./node_modules/openai/resources/images.mjs\");\n/* harmony import */ var _resources_index_mjs__WEBPACK_IMPORTED_MODULE_7__ = __webpack_require__(/*! ./resources/index.mjs */ \"./node_modules/openai/resources/audio/audio.mjs\");\n/* harmony import */ var _resources_index_mjs__WEBPACK_IMPORTED_MODULE_8__ = __webpack_require__(/*! ./resources/index.mjs */ \"./node_modules/openai/resources/moderations.mjs\");\n/* harmony import */ var _resources_index_mjs__WEBPACK_IMPORTED_MODULE_9__ = __webpack_require__(/*! ./resources/index.mjs */ \"./node_modules/openai/resources/models.mjs\");\n/* harmony import */ var _resources_index_mjs__WEBPACK_IMPORTED_MODULE_10__ = __webpack_require__(/*! ./resources/index.mjs */ \"./node_modules/openai/resources/fine-tuning/fine-tuning.mjs\");\n/* harmony import */ var _resources_index_mjs__WEBPACK_IMPORTED_MODULE_11__ = __webpack_require__(/*! ./resources/index.mjs */ \"./node_modules/openai/resources/beta/beta.mjs\");\n/* harmony import */ var _resources_index_mjs__WEBPACK_IMPORTED_MODULE_12__ = __webpack_require__(/*! ./resources/index.mjs */ \"./node_modules/openai/resources/batches.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nvar _a;\n\n\n\n\n\n/** API Client for interfacing with the OpenAI API. */\nclass OpenAI extends _core_mjs__WEBPACK_IMPORTED_MODULE_0__.APIClient {\n    /**\n     * API Client for interfacing with the OpenAI API.\n     *\n     * @param {string | undefined} [opts.apiKey=process.env['OPENAI_API_KEY'] ?? undefined]\n     * @param {string | null | undefined} [opts.organization=process.env['OPENAI_ORG_ID'] ?? null]\n     * @param {string | null | undefined} [opts.project=process.env['OPENAI_PROJECT_ID'] ?? null]\n     * @param {string} [opts.baseURL=process.env['OPENAI_BASE_URL'] ?? https://api.openai.com/v1] - Override the default base URL for the API.\n     * @param {number} [opts.timeout=10 minutes] - The maximum amount of time (in milliseconds) the client will wait for a response before timing out.\n     * @param {number} [opts.httpAgent] - An HTTP agent used to manage HTTP(s) connections.\n     * @param {Core.Fetch} [opts.fetch] - Specify a custom `fetch` function implementation.\n     * @param {number} [opts.maxRetries=2] - The maximum number of times the client will retry a request.\n     * @param {Core.Headers} opts.defaultHeaders - Default headers to include with every request to the API.\n     * @param {Core.DefaultQuery} opts.defaultQuery - Default query parameters to include with every request to the API.\n     * @param {boolean} [opts.dangerouslyAllowBrowser=false] - By default, client-side use of this library is not allowed, as it risks exposing your secret API credentials to attackers.\n     */\n    constructor({ baseURL = _core_mjs__WEBPACK_IMPORTED_MODULE_0__.readEnv('OPENAI_BASE_URL'), apiKey = _core_mjs__WEBPACK_IMPORTED_MODULE_0__.readEnv('OPENAI_API_KEY'), organization = _core_mjs__WEBPACK_IMPORTED_MODULE_0__.readEnv('OPENAI_ORG_ID') ?? null, project = _core_mjs__WEBPACK_IMPORTED_MODULE_0__.readEnv('OPENAI_PROJECT_ID') ?? null, ...opts } = {}) {\n        if (apiKey === undefined) {\n            throw new _error_mjs__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(\"The OPENAI_API_KEY environment variable is missing or empty; either provide it, or instantiate the OpenAI client with an apiKey option, like new OpenAI({ apiKey: 'My API Key' }).\");\n        }\n        const options = {\n            apiKey,\n            organization,\n            project,\n            ...opts,\n            baseURL: baseURL || `https://api.openai.com/v1`,\n        };\n        if (!options.dangerouslyAllowBrowser && _core_mjs__WEBPACK_IMPORTED_MODULE_0__.isRunningInBrowser()) {\n            throw new _error_mjs__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(\"It looks like you're running in a browser-like environment.\\n\\nThis is disabled by default, as it risks exposing your secret API credentials to attackers.\\nIf you understand the risks and have appropriate mitigations in place,\\nyou can set the `dangerouslyAllowBrowser` option to `true`, e.g.,\\n\\nnew OpenAI({ apiKey, dangerouslyAllowBrowser: true });\\n\\nhttps://help.openai.com/en/articles/5112595-best-practices-for-api-key-safety\\n\");\n        }\n        super({\n            baseURL: options.baseURL,\n            timeout: options.timeout ?? 600000 /* 10 minutes */,\n            httpAgent: options.httpAgent,\n            maxRetries: options.maxRetries,\n            fetch: options.fetch,\n        });\n        this.completions = new _resources_index_mjs__WEBPACK_IMPORTED_MODULE_2__.Completions(this);\n        this.chat = new _resources_index_mjs__WEBPACK_IMPORTED_MODULE_3__.Chat(this);\n        this.embeddings = new _resources_index_mjs__WEBPACK_IMPORTED_MODULE_4__.Embeddings(this);\n        this.files = new _resources_index_mjs__WEBPACK_IMPORTED_MODULE_5__.Files(this);\n        this.images = new _resources_index_mjs__WEBPACK_IMPORTED_MODULE_6__.Images(this);\n        this.audio = new _resources_index_mjs__WEBPACK_IMPORTED_MODULE_7__.Audio(this);\n        this.moderations = new _resources_index_mjs__WEBPACK_IMPORTED_MODULE_8__.Moderations(this);\n        this.models = new _resources_index_mjs__WEBPACK_IMPORTED_MODULE_9__.Models(this);\n        this.fineTuning = new _resources_index_mjs__WEBPACK_IMPORTED_MODULE_10__.FineTuning(this);\n        this.beta = new _resources_index_mjs__WEBPACK_IMPORTED_MODULE_11__.Beta(this);\n        this.batches = new _resources_index_mjs__WEBPACK_IMPORTED_MODULE_12__.Batches(this);\n        this._options = options;\n        this.apiKey = apiKey;\n        this.organization = organization;\n        this.project = project;\n    }\n    defaultQuery() {\n        return this._options.defaultQuery;\n    }\n    defaultHeaders(opts) {\n        return {\n            ...super.defaultHeaders(opts),\n            'OpenAI-Organization': this.organization,\n            'OpenAI-Project': this.project,\n            ...this._options.defaultHeaders,\n        };\n    }\n    authHeaders(opts) {\n        return { Authorization: `Bearer ${this.apiKey}` };\n    }\n}\n_a = OpenAI;\nOpenAI.OpenAI = _a;\nOpenAI.OpenAIError = _error_mjs__WEBPACK_IMPORTED_MODULE_1__.OpenAIError;\nOpenAI.APIError = _error_mjs__WEBPACK_IMPORTED_MODULE_1__.APIError;\nOpenAI.APIConnectionError = _error_mjs__WEBPACK_IMPORTED_MODULE_1__.APIConnectionError;\nOpenAI.APIConnectionTimeoutError = _error_mjs__WEBPACK_IMPORTED_MODULE_1__.APIConnectionTimeoutError;\nOpenAI.APIUserAbortError = _error_mjs__WEBPACK_IMPORTED_MODULE_1__.APIUserAbortError;\nOpenAI.NotFoundError = _error_mjs__WEBPACK_IMPORTED_MODULE_1__.NotFoundError;\nOpenAI.ConflictError = _error_mjs__WEBPACK_IMPORTED_MODULE_1__.ConflictError;\nOpenAI.RateLimitError = _error_mjs__WEBPACK_IMPORTED_MODULE_1__.RateLimitError;\nOpenAI.BadRequestError = _error_mjs__WEBPACK_IMPORTED_MODULE_1__.BadRequestError;\nOpenAI.AuthenticationError = _error_mjs__WEBPACK_IMPORTED_MODULE_1__.AuthenticationError;\nOpenAI.InternalServerError = _error_mjs__WEBPACK_IMPORTED_MODULE_1__.InternalServerError;\nOpenAI.PermissionDeniedError = _error_mjs__WEBPACK_IMPORTED_MODULE_1__.PermissionDeniedError;\nOpenAI.UnprocessableEntityError = _error_mjs__WEBPACK_IMPORTED_MODULE_1__.UnprocessableEntityError;\nOpenAI.toFile = _uploads_mjs__WEBPACK_IMPORTED_MODULE_13__.toFile;\nOpenAI.fileFromPath = _uploads_mjs__WEBPACK_IMPORTED_MODULE_14__.fileFromPath;\nconst { OpenAIError, APIError, APIConnectionError, APIConnectionTimeoutError, APIUserAbortError, NotFoundError, ConflictError, RateLimitError, BadRequestError, AuthenticationError, InternalServerError, PermissionDeniedError, UnprocessableEntityError, } = _error_mjs__WEBPACK_IMPORTED_MODULE_1__;\nvar toFile = _uploads_mjs__WEBPACK_IMPORTED_MODULE_13__.toFile;\nvar fileFromPath = _uploads_mjs__WEBPACK_IMPORTED_MODULE_14__.fileFromPath;\n(function (OpenAI) {\n    OpenAI.Page = _pagination_mjs__WEBPACK_IMPORTED_MODULE_15__.Page;\n    OpenAI.CursorPage = _pagination_mjs__WEBPACK_IMPORTED_MODULE_15__.CursorPage;\n    OpenAI.Completions = _resources_index_mjs__WEBPACK_IMPORTED_MODULE_2__.Completions;\n    OpenAI.Chat = _resources_index_mjs__WEBPACK_IMPORTED_MODULE_3__.Chat;\n    OpenAI.Embeddings = _resources_index_mjs__WEBPACK_IMPORTED_MODULE_4__.Embeddings;\n    OpenAI.Files = _resources_index_mjs__WEBPACK_IMPORTED_MODULE_5__.Files;\n    OpenAI.FileObjectsPage = _resources_index_mjs__WEBPACK_IMPORTED_MODULE_5__.FileObjectsPage;\n    OpenAI.Images = _resources_index_mjs__WEBPACK_IMPORTED_MODULE_6__.Images;\n    OpenAI.Audio = _resources_index_mjs__WEBPACK_IMPORTED_MODULE_7__.Audio;\n    OpenAI.Moderations = _resources_index_mjs__WEBPACK_IMPORTED_MODULE_8__.Moderations;\n    OpenAI.Models = _resources_index_mjs__WEBPACK_IMPORTED_MODULE_9__.Models;\n    OpenAI.ModelsPage = _resources_index_mjs__WEBPACK_IMPORTED_MODULE_9__.ModelsPage;\n    OpenAI.FineTuning = _resources_index_mjs__WEBPACK_IMPORTED_MODULE_10__.FineTuning;\n    OpenAI.Beta = _resources_index_mjs__WEBPACK_IMPORTED_MODULE_11__.Beta;\n    OpenAI.Batches = _resources_index_mjs__WEBPACK_IMPORTED_MODULE_12__.Batches;\n    OpenAI.BatchesPage = _resources_index_mjs__WEBPACK_IMPORTED_MODULE_12__.BatchesPage;\n})(OpenAI || (OpenAI = {}));\n/** API Client for interfacing with the Azure OpenAI API. */\nclass AzureOpenAI extends OpenAI {\n    /**\n     * API Client for interfacing with the Azure OpenAI API.\n     *\n     * @param {string | undefined} [opts.apiVersion=process.env['OPENAI_API_VERSION'] ?? undefined]\n     * @param {string | undefined} [opts.endpoint=process.env['AZURE_OPENAI_ENDPOINT'] ?? undefined] - Your Azure endpoint, including the resource, e.g. `https://example-resource.azure.openai.com/`\n     * @param {string | undefined} [opts.apiKey=process.env['AZURE_OPENAI_API_KEY'] ?? undefined]\n     * @param {string | undefined} opts.deployment - A model deployment, if given, sets the base client URL to include `/deployments/{deployment}`.\n     * @param {string | null | undefined} [opts.organization=process.env['OPENAI_ORG_ID'] ?? null]\n     * @param {string} [opts.baseURL=process.env['OPENAI_BASE_URL']] - Sets the base URL for the API.\n     * @param {number} [opts.timeout=10 minutes] - The maximum amount of time (in milliseconds) the client will wait for a response before timing out.\n     * @param {number} [opts.httpAgent] - An HTTP agent used to manage HTTP(s) connections.\n     * @param {Core.Fetch} [opts.fetch] - Specify a custom `fetch` function implementation.\n     * @param {number} [opts.maxRetries=2] - The maximum number of times the client will retry a request.\n     * @param {Core.Headers} opts.defaultHeaders - Default headers to include with every request to the API.\n     * @param {Core.DefaultQuery} opts.defaultQuery - Default query parameters to include with every request to the API.\n     * @param {boolean} [opts.dangerouslyAllowBrowser=false] - By default, client-side use of this library is not allowed, as it risks exposing your secret API credentials to attackers.\n     */\n    constructor({ baseURL = _core_mjs__WEBPACK_IMPORTED_MODULE_0__.readEnv('OPENAI_BASE_URL'), apiKey = _core_mjs__WEBPACK_IMPORTED_MODULE_0__.readEnv('AZURE_OPENAI_API_KEY'), apiVersion = _core_mjs__WEBPACK_IMPORTED_MODULE_0__.readEnv('OPENAI_API_VERSION'), endpoint, deployment, azureADTokenProvider, dangerouslyAllowBrowser, ...opts } = {}) {\n        if (!apiVersion) {\n            throw new _error_mjs__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(\"The OPENAI_API_VERSION environment variable is missing or empty; either provide it, or instantiate the AzureOpenAI client with an apiVersion option, like new AzureOpenAI({ apiVersion: 'My API Version' }).\");\n        }\n        if (typeof azureADTokenProvider === 'function') {\n            dangerouslyAllowBrowser = true;\n        }\n        if (!azureADTokenProvider && !apiKey) {\n            throw new _error_mjs__WEBPACK_IMPORTED_MODULE_1__.OpenAIError('Missing credentials. Please pass one of `apiKey` and `azureADTokenProvider`, or set the `AZURE_OPENAI_API_KEY` environment variable.');\n        }\n        if (azureADTokenProvider && apiKey) {\n            throw new _error_mjs__WEBPACK_IMPORTED_MODULE_1__.OpenAIError('The `apiKey` and `azureADTokenProvider` arguments are mutually exclusive; only one can be passed at a time.');\n        }\n        // define a sentinel value to avoid any typing issues\n        apiKey ?? (apiKey = API_KEY_SENTINEL);\n        opts.defaultQuery = { ...opts.defaultQuery, 'api-version': apiVersion };\n        if (!baseURL) {\n            if (!endpoint) {\n                endpoint = process.env['AZURE_OPENAI_ENDPOINT'];\n            }\n            if (!endpoint) {\n                throw new _error_mjs__WEBPACK_IMPORTED_MODULE_1__.OpenAIError('Must provide one of the `baseURL` or `endpoint` arguments, or the `AZURE_OPENAI_ENDPOINT` environment variable');\n            }\n            baseURL = `${endpoint}/openai`;\n        }\n        else {\n            if (endpoint) {\n                throw new _error_mjs__WEBPACK_IMPORTED_MODULE_1__.OpenAIError('baseURL and endpoint are mutually exclusive');\n            }\n        }\n        super({\n            apiKey,\n            baseURL,\n            ...opts,\n            ...(dangerouslyAllowBrowser !== undefined ? { dangerouslyAllowBrowser } : {}),\n        });\n        this.apiVersion = '';\n        this._azureADTokenProvider = azureADTokenProvider;\n        this.apiVersion = apiVersion;\n        this._deployment = deployment;\n    }\n    buildRequest(options) {\n        if (_deployments_endpoints.has(options.path) && options.method === 'post' && options.body !== undefined) {\n            if (!_core_mjs__WEBPACK_IMPORTED_MODULE_0__.isObj(options.body)) {\n                throw new Error('Expected request body to be an object');\n            }\n            const model = this._deployment || options.body['model'];\n            delete options.body['model'];\n            if (model !== undefined && !this.baseURL.includes('/deployments')) {\n                options.path = `/deployments/${model}${options.path}`;\n            }\n        }\n        return super.buildRequest(options);\n    }\n    async _getAzureADToken() {\n        if (typeof this._azureADTokenProvider === 'function') {\n            const token = await this._azureADTokenProvider();\n            if (!token || typeof token !== 'string') {\n                throw new _error_mjs__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`Expected 'azureADTokenProvider' argument to return a string but it returned ${token}`);\n            }\n            return token;\n        }\n        return undefined;\n    }\n    authHeaders(opts) {\n        return {};\n    }\n    async prepareOptions(opts) {\n        if (opts.headers?.['Authorization'] || opts.headers?.['api-key']) {\n            return super.prepareOptions(opts);\n        }\n        const token = await this._getAzureADToken();\n        opts.headers ?? (opts.headers = {});\n        if (token) {\n            opts.headers['Authorization'] = `Bearer ${token}`;\n        }\n        else if (this.apiKey !== API_KEY_SENTINEL) {\n            opts.headers['api-key'] = this.apiKey;\n        }\n        else {\n            throw new _error_mjs__WEBPACK_IMPORTED_MODULE_1__.OpenAIError('Unable to handle auth');\n        }\n        return super.prepareOptions(opts);\n    }\n}\nconst _deployments_endpoints = new Set([\n    '/completions',\n    '/chat/completions',\n    '/embeddings',\n    '/audio/transcriptions',\n    '/audio/translations',\n    '/audio/speech',\n    '/images/generations',\n]);\nconst API_KEY_SENTINEL = '<Missing Key>';\n// ---------------------- End Azure ----------------------\n/* harmony default export */ const __WEBPACK_DEFAULT_EXPORT__ = (OpenAI);\n//# sourceMappingURL=index.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/index.mjs?");

/***/ }),

/***/ "./node_modules/openai/lib/AbstractAssistantStreamRunner.mjs":
/*!*******************************************************************!*\
  !*** ./node_modules/openai/lib/AbstractAssistantStreamRunner.mjs ***!
  \*******************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   AbstractAssistantStreamRunner: () => (/* binding */ AbstractAssistantStreamRunner)\n/* harmony export */ });\n/* harmony import */ var openai_error__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! openai/error */ \"./node_modules/openai/error.mjs\");\nvar __classPrivateFieldSet = (undefined && undefined.__classPrivateFieldSet) || function (receiver, state, value, kind, f) {\n    if (kind === \"m\") throw new TypeError(\"Private method is not writable\");\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a setter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\n    return (kind === \"a\" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value)), value;\n};\nvar __classPrivateFieldGet = (undefined && undefined.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar _AbstractAssistantStreamRunner_connectedPromise, _AbstractAssistantStreamRunner_resolveConnectedPromise, _AbstractAssistantStreamRunner_rejectConnectedPromise, _AbstractAssistantStreamRunner_endPromise, _AbstractAssistantStreamRunner_resolveEndPromise, _AbstractAssistantStreamRunner_rejectEndPromise, _AbstractAssistantStreamRunner_listeners, _AbstractAssistantStreamRunner_ended, _AbstractAssistantStreamRunner_errored, _AbstractAssistantStreamRunner_aborted, _AbstractAssistantStreamRunner_catchingPromiseCreated, _AbstractAssistantStreamRunner_handleError;\n\nclass AbstractAssistantStreamRunner {\n    constructor() {\n        this.controller = new AbortController();\n        _AbstractAssistantStreamRunner_connectedPromise.set(this, void 0);\n        _AbstractAssistantStreamRunner_resolveConnectedPromise.set(this, () => { });\n        _AbstractAssistantStreamRunner_rejectConnectedPromise.set(this, () => { });\n        _AbstractAssistantStreamRunner_endPromise.set(this, void 0);\n        _AbstractAssistantStreamRunner_resolveEndPromise.set(this, () => { });\n        _AbstractAssistantStreamRunner_rejectEndPromise.set(this, () => { });\n        _AbstractAssistantStreamRunner_listeners.set(this, {});\n        _AbstractAssistantStreamRunner_ended.set(this, false);\n        _AbstractAssistantStreamRunner_errored.set(this, false);\n        _AbstractAssistantStreamRunner_aborted.set(this, false);\n        _AbstractAssistantStreamRunner_catchingPromiseCreated.set(this, false);\n        _AbstractAssistantStreamRunner_handleError.set(this, (error) => {\n            __classPrivateFieldSet(this, _AbstractAssistantStreamRunner_errored, true, \"f\");\n            if (error instanceof Error && error.name === 'AbortError') {\n                error = new openai_error__WEBPACK_IMPORTED_MODULE_0__.APIUserAbortError();\n            }\n            if (error instanceof openai_error__WEBPACK_IMPORTED_MODULE_0__.APIUserAbortError) {\n                __classPrivateFieldSet(this, _AbstractAssistantStreamRunner_aborted, true, \"f\");\n                return this._emit('abort', error);\n            }\n            if (error instanceof openai_error__WEBPACK_IMPORTED_MODULE_0__.OpenAIError) {\n                return this._emit('error', error);\n            }\n            if (error instanceof Error) {\n                const openAIError = new openai_error__WEBPACK_IMPORTED_MODULE_0__.OpenAIError(error.message);\n                // @ts-ignore\n                openAIError.cause = error;\n                return this._emit('error', openAIError);\n            }\n            return this._emit('error', new openai_error__WEBPACK_IMPORTED_MODULE_0__.OpenAIError(String(error)));\n        });\n        __classPrivateFieldSet(this, _AbstractAssistantStreamRunner_connectedPromise, new Promise((resolve, reject) => {\n            __classPrivateFieldSet(this, _AbstractAssistantStreamRunner_resolveConnectedPromise, resolve, \"f\");\n            __classPrivateFieldSet(this, _AbstractAssistantStreamRunner_rejectConnectedPromise, reject, \"f\");\n        }), \"f\");\n        __classPrivateFieldSet(this, _AbstractAssistantStreamRunner_endPromise, new Promise((resolve, reject) => {\n            __classPrivateFieldSet(this, _AbstractAssistantStreamRunner_resolveEndPromise, resolve, \"f\");\n            __classPrivateFieldSet(this, _AbstractAssistantStreamRunner_rejectEndPromise, reject, \"f\");\n        }), \"f\");\n        // Don't let these promises cause unhandled rejection errors.\n        // we will manually cause an unhandled rejection error later\n        // if the user hasn't registered any error listener or called\n        // any promise-returning method.\n        __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_connectedPromise, \"f\").catch(() => { });\n        __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_endPromise, \"f\").catch(() => { });\n    }\n    _run(executor) {\n        // Unfortunately if we call `executor()` immediately we get runtime errors about\n        // references to `this` before the `super()` constructor call returns.\n        setTimeout(() => {\n            executor().then(() => {\n                // this._emitFinal();\n                this._emit('end');\n            }, __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_handleError, \"f\"));\n        }, 0);\n    }\n    _addRun(run) {\n        return run;\n    }\n    _connected() {\n        if (this.ended)\n            return;\n        __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_resolveConnectedPromise, \"f\").call(this);\n        this._emit('connect');\n    }\n    get ended() {\n        return __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_ended, \"f\");\n    }\n    get errored() {\n        return __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_errored, \"f\");\n    }\n    get aborted() {\n        return __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_aborted, \"f\");\n    }\n    abort() {\n        this.controller.abort();\n    }\n    /**\n     * Adds the listener function to the end of the listeners array for the event.\n     * No checks are made to see if the listener has already been added. Multiple calls passing\n     * the same combination of event and listener will result in the listener being added, and\n     * called, multiple times.\n     * @returns this ChatCompletionStream, so that calls can be chained\n     */\n    on(event, listener) {\n        const listeners = __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_listeners, \"f\")[event] || (__classPrivateFieldGet(this, _AbstractAssistantStreamRunner_listeners, \"f\")[event] = []);\n        listeners.push({ listener });\n        return this;\n    }\n    /**\n     * Removes the specified listener from the listener array for the event.\n     * off() will remove, at most, one instance of a listener from the listener array. If any single\n     * listener has been added multiple times to the listener array for the specified event, then\n     * off() must be called multiple times to remove each instance.\n     * @returns this ChatCompletionStream, so that calls can be chained\n     */\n    off(event, listener) {\n        const listeners = __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_listeners, \"f\")[event];\n        if (!listeners)\n            return this;\n        const index = listeners.findIndex((l) => l.listener === listener);\n        if (index >= 0)\n            listeners.splice(index, 1);\n        return this;\n    }\n    /**\n     * Adds a one-time listener function for the event. The next time the event is triggered,\n     * this listener is removed and then invoked.\n     * @returns this ChatCompletionStream, so that calls can be chained\n     */\n    once(event, listener) {\n        const listeners = __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_listeners, \"f\")[event] || (__classPrivateFieldGet(this, _AbstractAssistantStreamRunner_listeners, \"f\")[event] = []);\n        listeners.push({ listener, once: true });\n        return this;\n    }\n    /**\n     * This is similar to `.once()`, but returns a Promise that resolves the next time\n     * the event is triggered, instead of calling a listener callback.\n     * @returns a Promise that resolves the next time given event is triggered,\n     * or rejects if an error is emitted.  (If you request the 'error' event,\n     * returns a promise that resolves with the error).\n     *\n     * Example:\n     *\n     *   const message = await stream.emitted('message') // rejects if the stream errors\n     */\n    emitted(event) {\n        return new Promise((resolve, reject) => {\n            __classPrivateFieldSet(this, _AbstractAssistantStreamRunner_catchingPromiseCreated, true, \"f\");\n            if (event !== 'error')\n                this.once('error', reject);\n            this.once(event, resolve);\n        });\n    }\n    async done() {\n        __classPrivateFieldSet(this, _AbstractAssistantStreamRunner_catchingPromiseCreated, true, \"f\");\n        await __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_endPromise, \"f\");\n    }\n    _emit(event, ...args) {\n        // make sure we don't emit any events after end\n        if (__classPrivateFieldGet(this, _AbstractAssistantStreamRunner_ended, \"f\")) {\n            return;\n        }\n        if (event === 'end') {\n            __classPrivateFieldSet(this, _AbstractAssistantStreamRunner_ended, true, \"f\");\n            __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_resolveEndPromise, \"f\").call(this);\n        }\n        const listeners = __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_listeners, \"f\")[event];\n        if (listeners) {\n            __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_listeners, \"f\")[event] = listeners.filter((l) => !l.once);\n            listeners.forEach(({ listener }) => listener(...args));\n        }\n        if (event === 'abort') {\n            const error = args[0];\n            if (!__classPrivateFieldGet(this, _AbstractAssistantStreamRunner_catchingPromiseCreated, \"f\") && !listeners?.length) {\n                Promise.reject(error);\n            }\n            __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_rejectConnectedPromise, \"f\").call(this, error);\n            __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_rejectEndPromise, \"f\").call(this, error);\n            this._emit('end');\n            return;\n        }\n        if (event === 'error') {\n            // NOTE: _emit('error', error) should only be called from #handleError().\n            const error = args[0];\n            if (!__classPrivateFieldGet(this, _AbstractAssistantStreamRunner_catchingPromiseCreated, \"f\") && !listeners?.length) {\n                // Trigger an unhandled rejection if the user hasn't registered any error handlers.\n                // If you are seeing stack traces here, make sure to handle errors via either:\n                // - runner.on('error', () => ...)\n                // - await runner.done()\n                // - await runner.finalChatCompletion()\n                // - etc.\n                Promise.reject(error);\n            }\n            __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_rejectConnectedPromise, \"f\").call(this, error);\n            __classPrivateFieldGet(this, _AbstractAssistantStreamRunner_rejectEndPromise, \"f\").call(this, error);\n            this._emit('end');\n        }\n    }\n    async _threadAssistantStream(body, thread, options) {\n        return await this._createThreadAssistantStream(thread, body, options);\n    }\n    async _runAssistantStream(threadId, runs, params, options) {\n        return await this._createAssistantStream(runs, threadId, params, options);\n    }\n    async _runToolAssistantStream(threadId, runId, runs, params, options) {\n        return await this._createToolAssistantStream(runs, threadId, runId, params, options);\n    }\n    async _createThreadAssistantStream(thread, body, options) {\n        const signal = options?.signal;\n        if (signal) {\n            if (signal.aborted)\n                this.controller.abort();\n            signal.addEventListener('abort', () => this.controller.abort());\n        }\n        // this.#validateParams(params);\n        const runResult = await thread.createAndRun({ ...body, stream: false }, { ...options, signal: this.controller.signal });\n        this._connected();\n        return this._addRun(runResult);\n    }\n    async _createToolAssistantStream(run, threadId, runId, params, options) {\n        const signal = options?.signal;\n        if (signal) {\n            if (signal.aborted)\n                this.controller.abort();\n            signal.addEventListener('abort', () => this.controller.abort());\n        }\n        const runResult = await run.submitToolOutputs(threadId, runId, { ...params, stream: false }, { ...options, signal: this.controller.signal });\n        this._connected();\n        return this._addRun(runResult);\n    }\n    async _createAssistantStream(run, threadId, params, options) {\n        const signal = options?.signal;\n        if (signal) {\n            if (signal.aborted)\n                this.controller.abort();\n            signal.addEventListener('abort', () => this.controller.abort());\n        }\n        // this.#validateParams(params);\n        const runResult = await run.create(threadId, { ...params, stream: false }, { ...options, signal: this.controller.signal });\n        this._connected();\n        return this._addRun(runResult);\n    }\n}\n_AbstractAssistantStreamRunner_connectedPromise = new WeakMap(), _AbstractAssistantStreamRunner_resolveConnectedPromise = new WeakMap(), _AbstractAssistantStreamRunner_rejectConnectedPromise = new WeakMap(), _AbstractAssistantStreamRunner_endPromise = new WeakMap(), _AbstractAssistantStreamRunner_resolveEndPromise = new WeakMap(), _AbstractAssistantStreamRunner_rejectEndPromise = new WeakMap(), _AbstractAssistantStreamRunner_listeners = new WeakMap(), _AbstractAssistantStreamRunner_ended = new WeakMap(), _AbstractAssistantStreamRunner_errored = new WeakMap(), _AbstractAssistantStreamRunner_aborted = new WeakMap(), _AbstractAssistantStreamRunner_catchingPromiseCreated = new WeakMap(), _AbstractAssistantStreamRunner_handleError = new WeakMap();\n//# sourceMappingURL=AbstractAssistantStreamRunner.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/lib/AbstractAssistantStreamRunner.mjs?");

/***/ }),

/***/ "./node_modules/openai/lib/AbstractChatCompletionRunner.mjs":
/*!******************************************************************!*\
  !*** ./node_modules/openai/lib/AbstractChatCompletionRunner.mjs ***!
  \******************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   AbstractChatCompletionRunner: () => (/* binding */ AbstractChatCompletionRunner)\n/* harmony export */ });\n/* harmony import */ var openai_error__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! openai/error */ \"./node_modules/openai/error.mjs\");\n/* harmony import */ var _RunnableFunction_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ./RunnableFunction.mjs */ \"./node_modules/openai/lib/RunnableFunction.mjs\");\n/* harmony import */ var _chatCompletionUtils_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./chatCompletionUtils.mjs */ \"./node_modules/openai/lib/chatCompletionUtils.mjs\");\nvar __classPrivateFieldSet = (undefined && undefined.__classPrivateFieldSet) || function (receiver, state, value, kind, f) {\n    if (kind === \"m\") throw new TypeError(\"Private method is not writable\");\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a setter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\n    return (kind === \"a\" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value)), value;\n};\nvar __classPrivateFieldGet = (undefined && undefined.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar _AbstractChatCompletionRunner_instances, _AbstractChatCompletionRunner_connectedPromise, _AbstractChatCompletionRunner_resolveConnectedPromise, _AbstractChatCompletionRunner_rejectConnectedPromise, _AbstractChatCompletionRunner_endPromise, _AbstractChatCompletionRunner_resolveEndPromise, _AbstractChatCompletionRunner_rejectEndPromise, _AbstractChatCompletionRunner_listeners, _AbstractChatCompletionRunner_ended, _AbstractChatCompletionRunner_errored, _AbstractChatCompletionRunner_aborted, _AbstractChatCompletionRunner_catchingPromiseCreated, _AbstractChatCompletionRunner_getFinalContent, _AbstractChatCompletionRunner_getFinalMessage, _AbstractChatCompletionRunner_getFinalFunctionCall, _AbstractChatCompletionRunner_getFinalFunctionCallResult, _AbstractChatCompletionRunner_calculateTotalUsage, _AbstractChatCompletionRunner_handleError, _AbstractChatCompletionRunner_validateParams, _AbstractChatCompletionRunner_stringifyFunctionCallResult;\n\n\n\nconst DEFAULT_MAX_CHAT_COMPLETIONS = 10;\nclass AbstractChatCompletionRunner {\n    constructor() {\n        _AbstractChatCompletionRunner_instances.add(this);\n        this.controller = new AbortController();\n        _AbstractChatCompletionRunner_connectedPromise.set(this, void 0);\n        _AbstractChatCompletionRunner_resolveConnectedPromise.set(this, () => { });\n        _AbstractChatCompletionRunner_rejectConnectedPromise.set(this, () => { });\n        _AbstractChatCompletionRunner_endPromise.set(this, void 0);\n        _AbstractChatCompletionRunner_resolveEndPromise.set(this, () => { });\n        _AbstractChatCompletionRunner_rejectEndPromise.set(this, () => { });\n        _AbstractChatCompletionRunner_listeners.set(this, {});\n        this._chatCompletions = [];\n        this.messages = [];\n        _AbstractChatCompletionRunner_ended.set(this, false);\n        _AbstractChatCompletionRunner_errored.set(this, false);\n        _AbstractChatCompletionRunner_aborted.set(this, false);\n        _AbstractChatCompletionRunner_catchingPromiseCreated.set(this, false);\n        _AbstractChatCompletionRunner_handleError.set(this, (error) => {\n            __classPrivateFieldSet(this, _AbstractChatCompletionRunner_errored, true, \"f\");\n            if (error instanceof Error && error.name === 'AbortError') {\n                error = new openai_error__WEBPACK_IMPORTED_MODULE_0__.APIUserAbortError();\n            }\n            if (error instanceof openai_error__WEBPACK_IMPORTED_MODULE_0__.APIUserAbortError) {\n                __classPrivateFieldSet(this, _AbstractChatCompletionRunner_aborted, true, \"f\");\n                return this._emit('abort', error);\n            }\n            if (error instanceof openai_error__WEBPACK_IMPORTED_MODULE_0__.OpenAIError) {\n                return this._emit('error', error);\n            }\n            if (error instanceof Error) {\n                const openAIError = new openai_error__WEBPACK_IMPORTED_MODULE_0__.OpenAIError(error.message);\n                // @ts-ignore\n                openAIError.cause = error;\n                return this._emit('error', openAIError);\n            }\n            return this._emit('error', new openai_error__WEBPACK_IMPORTED_MODULE_0__.OpenAIError(String(error)));\n        });\n        __classPrivateFieldSet(this, _AbstractChatCompletionRunner_connectedPromise, new Promise((resolve, reject) => {\n            __classPrivateFieldSet(this, _AbstractChatCompletionRunner_resolveConnectedPromise, resolve, \"f\");\n            __classPrivateFieldSet(this, _AbstractChatCompletionRunner_rejectConnectedPromise, reject, \"f\");\n        }), \"f\");\n        __classPrivateFieldSet(this, _AbstractChatCompletionRunner_endPromise, new Promise((resolve, reject) => {\n            __classPrivateFieldSet(this, _AbstractChatCompletionRunner_resolveEndPromise, resolve, \"f\");\n            __classPrivateFieldSet(this, _AbstractChatCompletionRunner_rejectEndPromise, reject, \"f\");\n        }), \"f\");\n        // Don't let these promises cause unhandled rejection errors.\n        // we will manually cause an unhandled rejection error later\n        // if the user hasn't registered any error listener or called\n        // any promise-returning method.\n        __classPrivateFieldGet(this, _AbstractChatCompletionRunner_connectedPromise, \"f\").catch(() => { });\n        __classPrivateFieldGet(this, _AbstractChatCompletionRunner_endPromise, \"f\").catch(() => { });\n    }\n    _run(executor) {\n        // Unfortunately if we call `executor()` immediately we get runtime errors about\n        // references to `this` before the `super()` constructor call returns.\n        setTimeout(() => {\n            executor().then(() => {\n                this._emitFinal();\n                this._emit('end');\n            }, __classPrivateFieldGet(this, _AbstractChatCompletionRunner_handleError, \"f\"));\n        }, 0);\n    }\n    _addChatCompletion(chatCompletion) {\n        this._chatCompletions.push(chatCompletion);\n        this._emit('chatCompletion', chatCompletion);\n        const message = chatCompletion.choices[0]?.message;\n        if (message)\n            this._addMessage(message);\n        return chatCompletion;\n    }\n    _addMessage(message, emit = true) {\n        if (!('content' in message))\n            message.content = null;\n        this.messages.push(message);\n        if (emit) {\n            this._emit('message', message);\n            if (((0,_chatCompletionUtils_mjs__WEBPACK_IMPORTED_MODULE_1__.isFunctionMessage)(message) || (0,_chatCompletionUtils_mjs__WEBPACK_IMPORTED_MODULE_1__.isToolMessage)(message)) && message.content) {\n                // Note, this assumes that {role: 'tool', content: …} is always the result of a call of tool of type=function.\n                this._emit('functionCallResult', message.content);\n            }\n            else if ((0,_chatCompletionUtils_mjs__WEBPACK_IMPORTED_MODULE_1__.isAssistantMessage)(message) && message.function_call) {\n                this._emit('functionCall', message.function_call);\n            }\n            else if ((0,_chatCompletionUtils_mjs__WEBPACK_IMPORTED_MODULE_1__.isAssistantMessage)(message) && message.tool_calls) {\n                for (const tool_call of message.tool_calls) {\n                    if (tool_call.type === 'function') {\n                        this._emit('functionCall', tool_call.function);\n                    }\n                }\n            }\n        }\n    }\n    _connected() {\n        if (this.ended)\n            return;\n        __classPrivateFieldGet(this, _AbstractChatCompletionRunner_resolveConnectedPromise, \"f\").call(this);\n        this._emit('connect');\n    }\n    get ended() {\n        return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_ended, \"f\");\n    }\n    get errored() {\n        return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_errored, \"f\");\n    }\n    get aborted() {\n        return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_aborted, \"f\");\n    }\n    abort() {\n        this.controller.abort();\n    }\n    /**\n     * Adds the listener function to the end of the listeners array for the event.\n     * No checks are made to see if the listener has already been added. Multiple calls passing\n     * the same combination of event and listener will result in the listener being added, and\n     * called, multiple times.\n     * @returns this ChatCompletionStream, so that calls can be chained\n     */\n    on(event, listener) {\n        const listeners = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_listeners, \"f\")[event] || (__classPrivateFieldGet(this, _AbstractChatCompletionRunner_listeners, \"f\")[event] = []);\n        listeners.push({ listener });\n        return this;\n    }\n    /**\n     * Removes the specified listener from the listener array for the event.\n     * off() will remove, at most, one instance of a listener from the listener array. If any single\n     * listener has been added multiple times to the listener array for the specified event, then\n     * off() must be called multiple times to remove each instance.\n     * @returns this ChatCompletionStream, so that calls can be chained\n     */\n    off(event, listener) {\n        const listeners = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_listeners, \"f\")[event];\n        if (!listeners)\n            return this;\n        const index = listeners.findIndex((l) => l.listener === listener);\n        if (index >= 0)\n            listeners.splice(index, 1);\n        return this;\n    }\n    /**\n     * Adds a one-time listener function for the event. The next time the event is triggered,\n     * this listener is removed and then invoked.\n     * @returns this ChatCompletionStream, so that calls can be chained\n     */\n    once(event, listener) {\n        const listeners = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_listeners, \"f\")[event] || (__classPrivateFieldGet(this, _AbstractChatCompletionRunner_listeners, \"f\")[event] = []);\n        listeners.push({ listener, once: true });\n        return this;\n    }\n    /**\n     * This is similar to `.once()`, but returns a Promise that resolves the next time\n     * the event is triggered, instead of calling a listener callback.\n     * @returns a Promise that resolves the next time given event is triggered,\n     * or rejects if an error is emitted.  (If you request the 'error' event,\n     * returns a promise that resolves with the error).\n     *\n     * Example:\n     *\n     *   const message = await stream.emitted('message') // rejects if the stream errors\n     */\n    emitted(event) {\n        return new Promise((resolve, reject) => {\n            __classPrivateFieldSet(this, _AbstractChatCompletionRunner_catchingPromiseCreated, true, \"f\");\n            if (event !== 'error')\n                this.once('error', reject);\n            this.once(event, resolve);\n        });\n    }\n    async done() {\n        __classPrivateFieldSet(this, _AbstractChatCompletionRunner_catchingPromiseCreated, true, \"f\");\n        await __classPrivateFieldGet(this, _AbstractChatCompletionRunner_endPromise, \"f\");\n    }\n    /**\n     * @returns a promise that resolves with the final ChatCompletion, or rejects\n     * if an error occurred or the stream ended prematurely without producing a ChatCompletion.\n     */\n    async finalChatCompletion() {\n        await this.done();\n        const completion = this._chatCompletions[this._chatCompletions.length - 1];\n        if (!completion)\n            throw new openai_error__WEBPACK_IMPORTED_MODULE_0__.OpenAIError('stream ended without producing a ChatCompletion');\n        return completion;\n    }\n    /**\n     * @returns a promise that resolves with the content of the final ChatCompletionMessage, or rejects\n     * if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.\n     */\n    async finalContent() {\n        await this.done();\n        return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, \"m\", _AbstractChatCompletionRunner_getFinalContent).call(this);\n    }\n    /**\n     * @returns a promise that resolves with the the final assistant ChatCompletionMessage response,\n     * or rejects if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.\n     */\n    async finalMessage() {\n        await this.done();\n        return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, \"m\", _AbstractChatCompletionRunner_getFinalMessage).call(this);\n    }\n    /**\n     * @returns a promise that resolves with the content of the final FunctionCall, or rejects\n     * if an error occurred or the stream ended prematurely without producing a ChatCompletionMessage.\n     */\n    async finalFunctionCall() {\n        await this.done();\n        return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, \"m\", _AbstractChatCompletionRunner_getFinalFunctionCall).call(this);\n    }\n    async finalFunctionCallResult() {\n        await this.done();\n        return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, \"m\", _AbstractChatCompletionRunner_getFinalFunctionCallResult).call(this);\n    }\n    async totalUsage() {\n        await this.done();\n        return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, \"m\", _AbstractChatCompletionRunner_calculateTotalUsage).call(this);\n    }\n    allChatCompletions() {\n        return [...this._chatCompletions];\n    }\n    _emit(event, ...args) {\n        // make sure we don't emit any events after end\n        if (__classPrivateFieldGet(this, _AbstractChatCompletionRunner_ended, \"f\")) {\n            return;\n        }\n        if (event === 'end') {\n            __classPrivateFieldSet(this, _AbstractChatCompletionRunner_ended, true, \"f\");\n            __classPrivateFieldGet(this, _AbstractChatCompletionRunner_resolveEndPromise, \"f\").call(this);\n        }\n        const listeners = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_listeners, \"f\")[event];\n        if (listeners) {\n            __classPrivateFieldGet(this, _AbstractChatCompletionRunner_listeners, \"f\")[event] = listeners.filter((l) => !l.once);\n            listeners.forEach(({ listener }) => listener(...args));\n        }\n        if (event === 'abort') {\n            const error = args[0];\n            if (!__classPrivateFieldGet(this, _AbstractChatCompletionRunner_catchingPromiseCreated, \"f\") && !listeners?.length) {\n                Promise.reject(error);\n            }\n            __classPrivateFieldGet(this, _AbstractChatCompletionRunner_rejectConnectedPromise, \"f\").call(this, error);\n            __classPrivateFieldGet(this, _AbstractChatCompletionRunner_rejectEndPromise, \"f\").call(this, error);\n            this._emit('end');\n            return;\n        }\n        if (event === 'error') {\n            // NOTE: _emit('error', error) should only be called from #handleError().\n            const error = args[0];\n            if (!__classPrivateFieldGet(this, _AbstractChatCompletionRunner_catchingPromiseCreated, \"f\") && !listeners?.length) {\n                // Trigger an unhandled rejection if the user hasn't registered any error handlers.\n                // If you are seeing stack traces here, make sure to handle errors via either:\n                // - runner.on('error', () => ...)\n                // - await runner.done()\n                // - await runner.finalChatCompletion()\n                // - etc.\n                Promise.reject(error);\n            }\n            __classPrivateFieldGet(this, _AbstractChatCompletionRunner_rejectConnectedPromise, \"f\").call(this, error);\n            __classPrivateFieldGet(this, _AbstractChatCompletionRunner_rejectEndPromise, \"f\").call(this, error);\n            this._emit('end');\n        }\n    }\n    _emitFinal() {\n        const completion = this._chatCompletions[this._chatCompletions.length - 1];\n        if (completion)\n            this._emit('finalChatCompletion', completion);\n        const finalMessage = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, \"m\", _AbstractChatCompletionRunner_getFinalMessage).call(this);\n        if (finalMessage)\n            this._emit('finalMessage', finalMessage);\n        const finalContent = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, \"m\", _AbstractChatCompletionRunner_getFinalContent).call(this);\n        if (finalContent)\n            this._emit('finalContent', finalContent);\n        const finalFunctionCall = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, \"m\", _AbstractChatCompletionRunner_getFinalFunctionCall).call(this);\n        if (finalFunctionCall)\n            this._emit('finalFunctionCall', finalFunctionCall);\n        const finalFunctionCallResult = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, \"m\", _AbstractChatCompletionRunner_getFinalFunctionCallResult).call(this);\n        if (finalFunctionCallResult != null)\n            this._emit('finalFunctionCallResult', finalFunctionCallResult);\n        if (this._chatCompletions.some((c) => c.usage)) {\n            this._emit('totalUsage', __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, \"m\", _AbstractChatCompletionRunner_calculateTotalUsage).call(this));\n        }\n    }\n    async _createChatCompletion(completions, params, options) {\n        const signal = options?.signal;\n        if (signal) {\n            if (signal.aborted)\n                this.controller.abort();\n            signal.addEventListener('abort', () => this.controller.abort());\n        }\n        __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, \"m\", _AbstractChatCompletionRunner_validateParams).call(this, params);\n        const chatCompletion = await completions.create({ ...params, stream: false }, { ...options, signal: this.controller.signal });\n        this._connected();\n        return this._addChatCompletion(chatCompletion);\n    }\n    async _runChatCompletion(completions, params, options) {\n        for (const message of params.messages) {\n            this._addMessage(message, false);\n        }\n        return await this._createChatCompletion(completions, params, options);\n    }\n    async _runFunctions(completions, params, options) {\n        const role = 'function';\n        const { function_call = 'auto', stream, ...restParams } = params;\n        const singleFunctionToCall = typeof function_call !== 'string' && function_call?.name;\n        const { maxChatCompletions = DEFAULT_MAX_CHAT_COMPLETIONS } = options || {};\n        const functionsByName = {};\n        for (const f of params.functions) {\n            functionsByName[f.name || f.function.name] = f;\n        }\n        const functions = params.functions.map((f) => ({\n            name: f.name || f.function.name,\n            parameters: f.parameters,\n            description: f.description,\n        }));\n        for (const message of params.messages) {\n            this._addMessage(message, false);\n        }\n        for (let i = 0; i < maxChatCompletions; ++i) {\n            const chatCompletion = await this._createChatCompletion(completions, {\n                ...restParams,\n                function_call,\n                functions,\n                messages: [...this.messages],\n            }, options);\n            const message = chatCompletion.choices[0]?.message;\n            if (!message) {\n                throw new openai_error__WEBPACK_IMPORTED_MODULE_0__.OpenAIError(`missing message in ChatCompletion response`);\n            }\n            if (!message.function_call)\n                return;\n            const { name, arguments: args } = message.function_call;\n            const fn = functionsByName[name];\n            if (!fn) {\n                const content = `Invalid function_call: ${JSON.stringify(name)}. Available options are: ${functions\n                    .map((f) => JSON.stringify(f.name))\n                    .join(', ')}. Please try again`;\n                this._addMessage({ role, name, content });\n                continue;\n            }\n            else if (singleFunctionToCall && singleFunctionToCall !== name) {\n                const content = `Invalid function_call: ${JSON.stringify(name)}. ${JSON.stringify(singleFunctionToCall)} requested. Please try again`;\n                this._addMessage({ role, name, content });\n                continue;\n            }\n            let parsed;\n            try {\n                parsed = (0,_RunnableFunction_mjs__WEBPACK_IMPORTED_MODULE_2__.isRunnableFunctionWithParse)(fn) ? await fn.parse(args) : args;\n            }\n            catch (error) {\n                this._addMessage({\n                    role,\n                    name,\n                    content: error instanceof Error ? error.message : String(error),\n                });\n                continue;\n            }\n            // @ts-expect-error it can't rule out `never` type.\n            const rawContent = await fn.function(parsed, this);\n            const content = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, \"m\", _AbstractChatCompletionRunner_stringifyFunctionCallResult).call(this, rawContent);\n            this._addMessage({ role, name, content });\n            if (singleFunctionToCall)\n                return;\n        }\n    }\n    async _runTools(completions, params, options) {\n        const role = 'tool';\n        const { tool_choice = 'auto', stream, ...restParams } = params;\n        const singleFunctionToCall = typeof tool_choice !== 'string' && tool_choice?.function?.name;\n        const { maxChatCompletions = DEFAULT_MAX_CHAT_COMPLETIONS } = options || {};\n        const functionsByName = {};\n        for (const f of params.tools) {\n            if (f.type === 'function') {\n                functionsByName[f.function.name || f.function.function.name] = f.function;\n            }\n        }\n        const tools = 'tools' in params ?\n            params.tools.map((t) => t.type === 'function' ?\n                {\n                    type: 'function',\n                    function: {\n                        name: t.function.name || t.function.function.name,\n                        parameters: t.function.parameters,\n                        description: t.function.description,\n                    },\n                }\n                : t)\n            : undefined;\n        for (const message of params.messages) {\n            this._addMessage(message, false);\n        }\n        for (let i = 0; i < maxChatCompletions; ++i) {\n            const chatCompletion = await this._createChatCompletion(completions, {\n                ...restParams,\n                tool_choice,\n                tools,\n                messages: [...this.messages],\n            }, options);\n            const message = chatCompletion.choices[0]?.message;\n            if (!message) {\n                throw new openai_error__WEBPACK_IMPORTED_MODULE_0__.OpenAIError(`missing message in ChatCompletion response`);\n            }\n            if (!message.tool_calls) {\n                return;\n            }\n            for (const tool_call of message.tool_calls) {\n                if (tool_call.type !== 'function')\n                    continue;\n                const tool_call_id = tool_call.id;\n                const { name, arguments: args } = tool_call.function;\n                const fn = functionsByName[name];\n                if (!fn) {\n                    const content = `Invalid tool_call: ${JSON.stringify(name)}. Available options are: ${tools\n                        .map((f) => JSON.stringify(f.function.name))\n                        .join(', ')}. Please try again`;\n                    this._addMessage({ role, tool_call_id, content });\n                    continue;\n                }\n                else if (singleFunctionToCall && singleFunctionToCall !== name) {\n                    const content = `Invalid tool_call: ${JSON.stringify(name)}. ${JSON.stringify(singleFunctionToCall)} requested. Please try again`;\n                    this._addMessage({ role, tool_call_id, content });\n                    continue;\n                }\n                let parsed;\n                try {\n                    parsed = (0,_RunnableFunction_mjs__WEBPACK_IMPORTED_MODULE_2__.isRunnableFunctionWithParse)(fn) ? await fn.parse(args) : args;\n                }\n                catch (error) {\n                    const content = error instanceof Error ? error.message : String(error);\n                    this._addMessage({ role, tool_call_id, content });\n                    continue;\n                }\n                // @ts-expect-error it can't rule out `never` type.\n                const rawContent = await fn.function(parsed, this);\n                const content = __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, \"m\", _AbstractChatCompletionRunner_stringifyFunctionCallResult).call(this, rawContent);\n                this._addMessage({ role, tool_call_id, content });\n                if (singleFunctionToCall) {\n                    return;\n                }\n            }\n        }\n        return;\n    }\n}\n_AbstractChatCompletionRunner_connectedPromise = new WeakMap(), _AbstractChatCompletionRunner_resolveConnectedPromise = new WeakMap(), _AbstractChatCompletionRunner_rejectConnectedPromise = new WeakMap(), _AbstractChatCompletionRunner_endPromise = new WeakMap(), _AbstractChatCompletionRunner_resolveEndPromise = new WeakMap(), _AbstractChatCompletionRunner_rejectEndPromise = new WeakMap(), _AbstractChatCompletionRunner_listeners = new WeakMap(), _AbstractChatCompletionRunner_ended = new WeakMap(), _AbstractChatCompletionRunner_errored = new WeakMap(), _AbstractChatCompletionRunner_aborted = new WeakMap(), _AbstractChatCompletionRunner_catchingPromiseCreated = new WeakMap(), _AbstractChatCompletionRunner_handleError = new WeakMap(), _AbstractChatCompletionRunner_instances = new WeakSet(), _AbstractChatCompletionRunner_getFinalContent = function _AbstractChatCompletionRunner_getFinalContent() {\n    return __classPrivateFieldGet(this, _AbstractChatCompletionRunner_instances, \"m\", _AbstractChatCompletionRunner_getFinalMessage).call(this).content ?? null;\n}, _AbstractChatCompletionRunner_getFinalMessage = function _AbstractChatCompletionRunner_getFinalMessage() {\n    let i = this.messages.length;\n    while (i-- > 0) {\n        const message = this.messages[i];\n        if ((0,_chatCompletionUtils_mjs__WEBPACK_IMPORTED_MODULE_1__.isAssistantMessage)(message)) {\n            const { function_call, ...rest } = message;\n            const ret = { ...rest, content: message.content ?? null };\n            if (function_call) {\n                ret.function_call = function_call;\n            }\n            return ret;\n        }\n    }\n    throw new openai_error__WEBPACK_IMPORTED_MODULE_0__.OpenAIError('stream ended without producing a ChatCompletionMessage with role=assistant');\n}, _AbstractChatCompletionRunner_getFinalFunctionCall = function _AbstractChatCompletionRunner_getFinalFunctionCall() {\n    for (let i = this.messages.length - 1; i >= 0; i--) {\n        const message = this.messages[i];\n        if ((0,_chatCompletionUtils_mjs__WEBPACK_IMPORTED_MODULE_1__.isAssistantMessage)(message) && message?.function_call) {\n            return message.function_call;\n        }\n        if ((0,_chatCompletionUtils_mjs__WEBPACK_IMPORTED_MODULE_1__.isAssistantMessage)(message) && message?.tool_calls?.length) {\n            return message.tool_calls.at(-1)?.function;\n        }\n    }\n    return;\n}, _AbstractChatCompletionRunner_getFinalFunctionCallResult = function _AbstractChatCompletionRunner_getFinalFunctionCallResult() {\n    for (let i = this.messages.length - 1; i >= 0; i--) {\n        const message = this.messages[i];\n        if ((0,_chatCompletionUtils_mjs__WEBPACK_IMPORTED_MODULE_1__.isFunctionMessage)(message) && message.content != null) {\n            return message.content;\n        }\n        if ((0,_chatCompletionUtils_mjs__WEBPACK_IMPORTED_MODULE_1__.isToolMessage)(message) &&\n            message.content != null &&\n            this.messages.some((x) => x.role === 'assistant' &&\n                x.tool_calls?.some((y) => y.type === 'function' && y.id === message.tool_call_id))) {\n            return message.content;\n        }\n    }\n    return;\n}, _AbstractChatCompletionRunner_calculateTotalUsage = function _AbstractChatCompletionRunner_calculateTotalUsage() {\n    const total = {\n        completion_tokens: 0,\n        prompt_tokens: 0,\n        total_tokens: 0,\n    };\n    for (const { usage } of this._chatCompletions) {\n        if (usage) {\n            total.completion_tokens += usage.completion_tokens;\n            total.prompt_tokens += usage.prompt_tokens;\n            total.total_tokens += usage.total_tokens;\n        }\n    }\n    return total;\n}, _AbstractChatCompletionRunner_validateParams = function _AbstractChatCompletionRunner_validateParams(params) {\n    if (params.n != null && params.n > 1) {\n        throw new openai_error__WEBPACK_IMPORTED_MODULE_0__.OpenAIError('ChatCompletion convenience helpers only support n=1 at this time. To use n>1, please use chat.completions.create() directly.');\n    }\n}, _AbstractChatCompletionRunner_stringifyFunctionCallResult = function _AbstractChatCompletionRunner_stringifyFunctionCallResult(rawContent) {\n    return (typeof rawContent === 'string' ? rawContent\n        : rawContent === undefined ? 'undefined'\n            : JSON.stringify(rawContent));\n};\n//# sourceMappingURL=AbstractChatCompletionRunner.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/lib/AbstractChatCompletionRunner.mjs?");

/***/ }),

/***/ "./node_modules/openai/lib/AssistantStream.mjs":
/*!*****************************************************!*\
  !*** ./node_modules/openai/lib/AssistantStream.mjs ***!
  \*****************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   AssistantStream: () => (/* binding */ AssistantStream)\n/* harmony export */ });\n/* harmony import */ var openai_core__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! openai/core */ \"./node_modules/openai/core.mjs\");\n/* harmony import */ var _AbstractAssistantStreamRunner_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./AbstractAssistantStreamRunner.mjs */ \"./node_modules/openai/lib/AbstractAssistantStreamRunner.mjs\");\n/* harmony import */ var openai_streaming__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! openai/streaming */ \"./node_modules/openai/streaming.mjs\");\n/* harmony import */ var openai_error__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! openai/error */ \"./node_modules/openai/error.mjs\");\nvar __classPrivateFieldGet = (undefined && undefined.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar __classPrivateFieldSet = (undefined && undefined.__classPrivateFieldSet) || function (receiver, state, value, kind, f) {\n    if (kind === \"m\") throw new TypeError(\"Private method is not writable\");\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a setter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\n    return (kind === \"a\" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value)), value;\n};\nvar _AssistantStream_instances, _AssistantStream_events, _AssistantStream_runStepSnapshots, _AssistantStream_messageSnapshots, _AssistantStream_messageSnapshot, _AssistantStream_finalRun, _AssistantStream_currentContentIndex, _AssistantStream_currentContent, _AssistantStream_currentToolCallIndex, _AssistantStream_currentToolCall, _AssistantStream_currentEvent, _AssistantStream_currentRunSnapshot, _AssistantStream_currentRunStepSnapshot, _AssistantStream_addEvent, _AssistantStream_endRequest, _AssistantStream_handleMessage, _AssistantStream_handleRunStep, _AssistantStream_handleEvent, _AssistantStream_accumulateRunStep, _AssistantStream_accumulateMessage, _AssistantStream_accumulateContent, _AssistantStream_handleRun;\n\n\n\n\nclass AssistantStream extends _AbstractAssistantStreamRunner_mjs__WEBPACK_IMPORTED_MODULE_0__.AbstractAssistantStreamRunner {\n    constructor() {\n        super(...arguments);\n        _AssistantStream_instances.add(this);\n        //Track all events in a single list for reference\n        _AssistantStream_events.set(this, []);\n        //Used to accumulate deltas\n        //We are accumulating many types so the value here is not strict\n        _AssistantStream_runStepSnapshots.set(this, {});\n        _AssistantStream_messageSnapshots.set(this, {});\n        _AssistantStream_messageSnapshot.set(this, void 0);\n        _AssistantStream_finalRun.set(this, void 0);\n        _AssistantStream_currentContentIndex.set(this, void 0);\n        _AssistantStream_currentContent.set(this, void 0);\n        _AssistantStream_currentToolCallIndex.set(this, void 0);\n        _AssistantStream_currentToolCall.set(this, void 0);\n        //For current snapshot methods\n        _AssistantStream_currentEvent.set(this, void 0);\n        _AssistantStream_currentRunSnapshot.set(this, void 0);\n        _AssistantStream_currentRunStepSnapshot.set(this, void 0);\n    }\n    [(_AssistantStream_events = new WeakMap(), _AssistantStream_runStepSnapshots = new WeakMap(), _AssistantStream_messageSnapshots = new WeakMap(), _AssistantStream_messageSnapshot = new WeakMap(), _AssistantStream_finalRun = new WeakMap(), _AssistantStream_currentContentIndex = new WeakMap(), _AssistantStream_currentContent = new WeakMap(), _AssistantStream_currentToolCallIndex = new WeakMap(), _AssistantStream_currentToolCall = new WeakMap(), _AssistantStream_currentEvent = new WeakMap(), _AssistantStream_currentRunSnapshot = new WeakMap(), _AssistantStream_currentRunStepSnapshot = new WeakMap(), _AssistantStream_instances = new WeakSet(), Symbol.asyncIterator)]() {\n        const pushQueue = [];\n        const readQueue = [];\n        let done = false;\n        //Catch all for passing along all events\n        this.on('event', (event) => {\n            const reader = readQueue.shift();\n            if (reader) {\n                reader.resolve(event);\n            }\n            else {\n                pushQueue.push(event);\n            }\n        });\n        this.on('end', () => {\n            done = true;\n            for (const reader of readQueue) {\n                reader.resolve(undefined);\n            }\n            readQueue.length = 0;\n        });\n        this.on('abort', (err) => {\n            done = true;\n            for (const reader of readQueue) {\n                reader.reject(err);\n            }\n            readQueue.length = 0;\n        });\n        this.on('error', (err) => {\n            done = true;\n            for (const reader of readQueue) {\n                reader.reject(err);\n            }\n            readQueue.length = 0;\n        });\n        return {\n            next: async () => {\n                if (!pushQueue.length) {\n                    if (done) {\n                        return { value: undefined, done: true };\n                    }\n                    return new Promise((resolve, reject) => readQueue.push({ resolve, reject })).then((chunk) => (chunk ? { value: chunk, done: false } : { value: undefined, done: true }));\n                }\n                const chunk = pushQueue.shift();\n                return { value: chunk, done: false };\n            },\n            return: async () => {\n                this.abort();\n                return { value: undefined, done: true };\n            },\n        };\n    }\n    static fromReadableStream(stream) {\n        const runner = new AssistantStream();\n        runner._run(() => runner._fromReadableStream(stream));\n        return runner;\n    }\n    async _fromReadableStream(readableStream, options) {\n        const signal = options?.signal;\n        if (signal) {\n            if (signal.aborted)\n                this.controller.abort();\n            signal.addEventListener('abort', () => this.controller.abort());\n        }\n        this._connected();\n        const stream = openai_streaming__WEBPACK_IMPORTED_MODULE_1__.Stream.fromReadableStream(readableStream, this.controller);\n        for await (const event of stream) {\n            __classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_addEvent).call(this, event);\n        }\n        if (stream.controller.signal?.aborted) {\n            throw new openai_error__WEBPACK_IMPORTED_MODULE_2__.APIUserAbortError();\n        }\n        return this._addRun(__classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_endRequest).call(this));\n    }\n    toReadableStream() {\n        const stream = new openai_streaming__WEBPACK_IMPORTED_MODULE_1__.Stream(this[Symbol.asyncIterator].bind(this), this.controller);\n        return stream.toReadableStream();\n    }\n    static createToolAssistantStream(threadId, runId, runs, body, options) {\n        const runner = new AssistantStream();\n        runner._run(() => runner._runToolAssistantStream(threadId, runId, runs, body, {\n            ...options,\n            headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'stream' },\n        }));\n        return runner;\n    }\n    async _createToolAssistantStream(run, threadId, runId, params, options) {\n        const signal = options?.signal;\n        if (signal) {\n            if (signal.aborted)\n                this.controller.abort();\n            signal.addEventListener('abort', () => this.controller.abort());\n        }\n        const body = { ...params, stream: true };\n        const stream = await run.submitToolOutputs(threadId, runId, body, {\n            ...options,\n            signal: this.controller.signal,\n        });\n        this._connected();\n        for await (const event of stream) {\n            __classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_addEvent).call(this, event);\n        }\n        if (stream.controller.signal?.aborted) {\n            throw new openai_error__WEBPACK_IMPORTED_MODULE_2__.APIUserAbortError();\n        }\n        return this._addRun(__classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_endRequest).call(this));\n    }\n    static createThreadAssistantStream(body, thread, options) {\n        const runner = new AssistantStream();\n        runner._run(() => runner._threadAssistantStream(body, thread, {\n            ...options,\n            headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'stream' },\n        }));\n        return runner;\n    }\n    static createAssistantStream(threadId, runs, params, options) {\n        const runner = new AssistantStream();\n        runner._run(() => runner._runAssistantStream(threadId, runs, params, {\n            ...options,\n            headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'stream' },\n        }));\n        return runner;\n    }\n    currentEvent() {\n        return __classPrivateFieldGet(this, _AssistantStream_currentEvent, \"f\");\n    }\n    currentRun() {\n        return __classPrivateFieldGet(this, _AssistantStream_currentRunSnapshot, \"f\");\n    }\n    currentMessageSnapshot() {\n        return __classPrivateFieldGet(this, _AssistantStream_messageSnapshot, \"f\");\n    }\n    currentRunStepSnapshot() {\n        return __classPrivateFieldGet(this, _AssistantStream_currentRunStepSnapshot, \"f\");\n    }\n    async finalRunSteps() {\n        await this.done();\n        return Object.values(__classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, \"f\"));\n    }\n    async finalMessages() {\n        await this.done();\n        return Object.values(__classPrivateFieldGet(this, _AssistantStream_messageSnapshots, \"f\"));\n    }\n    async finalRun() {\n        await this.done();\n        if (!__classPrivateFieldGet(this, _AssistantStream_finalRun, \"f\"))\n            throw Error('Final run was not received.');\n        return __classPrivateFieldGet(this, _AssistantStream_finalRun, \"f\");\n    }\n    async _createThreadAssistantStream(thread, params, options) {\n        const signal = options?.signal;\n        if (signal) {\n            if (signal.aborted)\n                this.controller.abort();\n            signal.addEventListener('abort', () => this.controller.abort());\n        }\n        const body = { ...params, stream: true };\n        const stream = await thread.createAndRun(body, { ...options, signal: this.controller.signal });\n        this._connected();\n        for await (const event of stream) {\n            __classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_addEvent).call(this, event);\n        }\n        if (stream.controller.signal?.aborted) {\n            throw new openai_error__WEBPACK_IMPORTED_MODULE_2__.APIUserAbortError();\n        }\n        return this._addRun(__classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_endRequest).call(this));\n    }\n    async _createAssistantStream(run, threadId, params, options) {\n        const signal = options?.signal;\n        if (signal) {\n            if (signal.aborted)\n                this.controller.abort();\n            signal.addEventListener('abort', () => this.controller.abort());\n        }\n        const body = { ...params, stream: true };\n        const stream = await run.create(threadId, body, { ...options, signal: this.controller.signal });\n        this._connected();\n        for await (const event of stream) {\n            __classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_addEvent).call(this, event);\n        }\n        if (stream.controller.signal?.aborted) {\n            throw new openai_error__WEBPACK_IMPORTED_MODULE_2__.APIUserAbortError();\n        }\n        return this._addRun(__classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_endRequest).call(this));\n    }\n    static accumulateDelta(acc, delta) {\n        for (const [key, deltaValue] of Object.entries(delta)) {\n            if (!acc.hasOwnProperty(key)) {\n                acc[key] = deltaValue;\n                continue;\n            }\n            let accValue = acc[key];\n            if (accValue === null || accValue === undefined) {\n                acc[key] = deltaValue;\n                continue;\n            }\n            // We don't accumulate these special properties\n            if (key === 'index' || key === 'type') {\n                acc[key] = deltaValue;\n                continue;\n            }\n            // Type-specific accumulation logic\n            if (typeof accValue === 'string' && typeof deltaValue === 'string') {\n                accValue += deltaValue;\n            }\n            else if (typeof accValue === 'number' && typeof deltaValue === 'number') {\n                accValue += deltaValue;\n            }\n            else if (openai_core__WEBPACK_IMPORTED_MODULE_3__.isObj(accValue) && openai_core__WEBPACK_IMPORTED_MODULE_3__.isObj(deltaValue)) {\n                accValue = this.accumulateDelta(accValue, deltaValue);\n            }\n            else if (Array.isArray(accValue) && Array.isArray(deltaValue)) {\n                if (accValue.every((x) => typeof x === 'string' || typeof x === 'number')) {\n                    accValue.push(...deltaValue); // Use spread syntax for efficient addition\n                    continue;\n                }\n            }\n            else {\n                throw Error(`Unhandled record type: ${key}, deltaValue: ${deltaValue}, accValue: ${accValue}`);\n            }\n            acc[key] = accValue;\n        }\n        return acc;\n    }\n}\n_AssistantStream_addEvent = function _AssistantStream_addEvent(event) {\n    if (this.ended)\n        return;\n    __classPrivateFieldSet(this, _AssistantStream_currentEvent, event, \"f\");\n    __classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_handleEvent).call(this, event);\n    switch (event.event) {\n        case 'thread.created':\n            //No action on this event.\n            break;\n        case 'thread.run.created':\n        case 'thread.run.queued':\n        case 'thread.run.in_progress':\n        case 'thread.run.requires_action':\n        case 'thread.run.completed':\n        case 'thread.run.failed':\n        case 'thread.run.cancelling':\n        case 'thread.run.cancelled':\n        case 'thread.run.expired':\n            __classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_handleRun).call(this, event);\n            break;\n        case 'thread.run.step.created':\n        case 'thread.run.step.in_progress':\n        case 'thread.run.step.delta':\n        case 'thread.run.step.completed':\n        case 'thread.run.step.failed':\n        case 'thread.run.step.cancelled':\n        case 'thread.run.step.expired':\n            __classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_handleRunStep).call(this, event);\n            break;\n        case 'thread.message.created':\n        case 'thread.message.in_progress':\n        case 'thread.message.delta':\n        case 'thread.message.completed':\n        case 'thread.message.incomplete':\n            __classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_handleMessage).call(this, event);\n            break;\n        case 'error':\n            //This is included for completeness, but errors are processed in the SSE event processing so this should not occur\n            throw new Error('Encountered an error event in event processing - errors should be processed earlier');\n    }\n}, _AssistantStream_endRequest = function _AssistantStream_endRequest() {\n    if (this.ended) {\n        throw new openai_error__WEBPACK_IMPORTED_MODULE_2__.OpenAIError(`stream has ended, this shouldn't happen`);\n    }\n    if (!__classPrivateFieldGet(this, _AssistantStream_finalRun, \"f\"))\n        throw Error('Final run has not been received');\n    return __classPrivateFieldGet(this, _AssistantStream_finalRun, \"f\");\n}, _AssistantStream_handleMessage = function _AssistantStream_handleMessage(event) {\n    const [accumulatedMessage, newContent] = __classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_accumulateMessage).call(this, event, __classPrivateFieldGet(this, _AssistantStream_messageSnapshot, \"f\"));\n    __classPrivateFieldSet(this, _AssistantStream_messageSnapshot, accumulatedMessage, \"f\");\n    __classPrivateFieldGet(this, _AssistantStream_messageSnapshots, \"f\")[accumulatedMessage.id] = accumulatedMessage;\n    for (const content of newContent) {\n        const snapshotContent = accumulatedMessage.content[content.index];\n        if (snapshotContent?.type == 'text') {\n            this._emit('textCreated', snapshotContent.text);\n        }\n    }\n    switch (event.event) {\n        case 'thread.message.created':\n            this._emit('messageCreated', event.data);\n            break;\n        case 'thread.message.in_progress':\n            break;\n        case 'thread.message.delta':\n            this._emit('messageDelta', event.data.delta, accumulatedMessage);\n            if (event.data.delta.content) {\n                for (const content of event.data.delta.content) {\n                    //If it is text delta, emit a text delta event\n                    if (content.type == 'text' && content.text) {\n                        let textDelta = content.text;\n                        let snapshot = accumulatedMessage.content[content.index];\n                        if (snapshot && snapshot.type == 'text') {\n                            this._emit('textDelta', textDelta, snapshot.text);\n                        }\n                        else {\n                            throw Error('The snapshot associated with this text delta is not text or missing');\n                        }\n                    }\n                    if (content.index != __classPrivateFieldGet(this, _AssistantStream_currentContentIndex, \"f\")) {\n                        //See if we have in progress content\n                        if (__classPrivateFieldGet(this, _AssistantStream_currentContent, \"f\")) {\n                            switch (__classPrivateFieldGet(this, _AssistantStream_currentContent, \"f\").type) {\n                                case 'text':\n                                    this._emit('textDone', __classPrivateFieldGet(this, _AssistantStream_currentContent, \"f\").text, __classPrivateFieldGet(this, _AssistantStream_messageSnapshot, \"f\"));\n                                    break;\n                                case 'image_file':\n                                    this._emit('imageFileDone', __classPrivateFieldGet(this, _AssistantStream_currentContent, \"f\").image_file, __classPrivateFieldGet(this, _AssistantStream_messageSnapshot, \"f\"));\n                                    break;\n                            }\n                        }\n                        __classPrivateFieldSet(this, _AssistantStream_currentContentIndex, content.index, \"f\");\n                    }\n                    __classPrivateFieldSet(this, _AssistantStream_currentContent, accumulatedMessage.content[content.index], \"f\");\n                }\n            }\n            break;\n        case 'thread.message.completed':\n        case 'thread.message.incomplete':\n            //We emit the latest content we were working on on completion (including incomplete)\n            if (__classPrivateFieldGet(this, _AssistantStream_currentContentIndex, \"f\") !== undefined) {\n                const currentContent = event.data.content[__classPrivateFieldGet(this, _AssistantStream_currentContentIndex, \"f\")];\n                if (currentContent) {\n                    switch (currentContent.type) {\n                        case 'image_file':\n                            this._emit('imageFileDone', currentContent.image_file, __classPrivateFieldGet(this, _AssistantStream_messageSnapshot, \"f\"));\n                            break;\n                        case 'text':\n                            this._emit('textDone', currentContent.text, __classPrivateFieldGet(this, _AssistantStream_messageSnapshot, \"f\"));\n                            break;\n                    }\n                }\n            }\n            if (__classPrivateFieldGet(this, _AssistantStream_messageSnapshot, \"f\")) {\n                this._emit('messageDone', event.data);\n            }\n            __classPrivateFieldSet(this, _AssistantStream_messageSnapshot, undefined, \"f\");\n    }\n}, _AssistantStream_handleRunStep = function _AssistantStream_handleRunStep(event) {\n    const accumulatedRunStep = __classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_accumulateRunStep).call(this, event);\n    __classPrivateFieldSet(this, _AssistantStream_currentRunStepSnapshot, accumulatedRunStep, \"f\");\n    switch (event.event) {\n        case 'thread.run.step.created':\n            this._emit('runStepCreated', event.data);\n            break;\n        case 'thread.run.step.delta':\n            const delta = event.data.delta;\n            if (delta.step_details &&\n                delta.step_details.type == 'tool_calls' &&\n                delta.step_details.tool_calls &&\n                accumulatedRunStep.step_details.type == 'tool_calls') {\n                for (const toolCall of delta.step_details.tool_calls) {\n                    if (toolCall.index == __classPrivateFieldGet(this, _AssistantStream_currentToolCallIndex, \"f\")) {\n                        this._emit('toolCallDelta', toolCall, accumulatedRunStep.step_details.tool_calls[toolCall.index]);\n                    }\n                    else {\n                        if (__classPrivateFieldGet(this, _AssistantStream_currentToolCall, \"f\")) {\n                            this._emit('toolCallDone', __classPrivateFieldGet(this, _AssistantStream_currentToolCall, \"f\"));\n                        }\n                        __classPrivateFieldSet(this, _AssistantStream_currentToolCallIndex, toolCall.index, \"f\");\n                        __classPrivateFieldSet(this, _AssistantStream_currentToolCall, accumulatedRunStep.step_details.tool_calls[toolCall.index], \"f\");\n                        if (__classPrivateFieldGet(this, _AssistantStream_currentToolCall, \"f\"))\n                            this._emit('toolCallCreated', __classPrivateFieldGet(this, _AssistantStream_currentToolCall, \"f\"));\n                    }\n                }\n            }\n            this._emit('runStepDelta', event.data.delta, accumulatedRunStep);\n            break;\n        case 'thread.run.step.completed':\n        case 'thread.run.step.failed':\n        case 'thread.run.step.cancelled':\n        case 'thread.run.step.expired':\n            __classPrivateFieldSet(this, _AssistantStream_currentRunStepSnapshot, undefined, \"f\");\n            const details = event.data.step_details;\n            if (details.type == 'tool_calls') {\n                if (__classPrivateFieldGet(this, _AssistantStream_currentToolCall, \"f\")) {\n                    this._emit('toolCallDone', __classPrivateFieldGet(this, _AssistantStream_currentToolCall, \"f\"));\n                    __classPrivateFieldSet(this, _AssistantStream_currentToolCall, undefined, \"f\");\n                }\n            }\n            this._emit('runStepDone', event.data, accumulatedRunStep);\n            break;\n        case 'thread.run.step.in_progress':\n            break;\n    }\n}, _AssistantStream_handleEvent = function _AssistantStream_handleEvent(event) {\n    __classPrivateFieldGet(this, _AssistantStream_events, \"f\").push(event);\n    this._emit('event', event);\n}, _AssistantStream_accumulateRunStep = function _AssistantStream_accumulateRunStep(event) {\n    switch (event.event) {\n        case 'thread.run.step.created':\n            __classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, \"f\")[event.data.id] = event.data;\n            return event.data;\n        case 'thread.run.step.delta':\n            let snapshot = __classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, \"f\")[event.data.id];\n            if (!snapshot) {\n                throw Error('Received a RunStepDelta before creation of a snapshot');\n            }\n            let data = event.data;\n            if (data.delta) {\n                const accumulated = AssistantStream.accumulateDelta(snapshot, data.delta);\n                __classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, \"f\")[event.data.id] = accumulated;\n            }\n            return __classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, \"f\")[event.data.id];\n        case 'thread.run.step.completed':\n        case 'thread.run.step.failed':\n        case 'thread.run.step.cancelled':\n        case 'thread.run.step.expired':\n        case 'thread.run.step.in_progress':\n            __classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, \"f\")[event.data.id] = event.data;\n            break;\n    }\n    if (__classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, \"f\")[event.data.id])\n        return __classPrivateFieldGet(this, _AssistantStream_runStepSnapshots, \"f\")[event.data.id];\n    throw new Error('No snapshot available');\n}, _AssistantStream_accumulateMessage = function _AssistantStream_accumulateMessage(event, snapshot) {\n    let newContent = [];\n    switch (event.event) {\n        case 'thread.message.created':\n            //On creation the snapshot is just the initial message\n            return [event.data, newContent];\n        case 'thread.message.delta':\n            if (!snapshot) {\n                throw Error('Received a delta with no existing snapshot (there should be one from message creation)');\n            }\n            let data = event.data;\n            //If this delta does not have content, nothing to process\n            if (data.delta.content) {\n                for (const contentElement of data.delta.content) {\n                    if (contentElement.index in snapshot.content) {\n                        let currentContent = snapshot.content[contentElement.index];\n                        snapshot.content[contentElement.index] = __classPrivateFieldGet(this, _AssistantStream_instances, \"m\", _AssistantStream_accumulateContent).call(this, contentElement, currentContent);\n                    }\n                    else {\n                        snapshot.content[contentElement.index] = contentElement;\n                        // This is a new element\n                        newContent.push(contentElement);\n                    }\n                }\n            }\n            return [snapshot, newContent];\n        case 'thread.message.in_progress':\n        case 'thread.message.completed':\n        case 'thread.message.incomplete':\n            //No changes on other thread events\n            if (snapshot) {\n                return [snapshot, newContent];\n            }\n            else {\n                throw Error('Received thread message event with no existing snapshot');\n            }\n    }\n    throw Error('Tried to accumulate a non-message event');\n}, _AssistantStream_accumulateContent = function _AssistantStream_accumulateContent(contentElement, currentContent) {\n    return AssistantStream.accumulateDelta(currentContent, contentElement);\n}, _AssistantStream_handleRun = function _AssistantStream_handleRun(event) {\n    __classPrivateFieldSet(this, _AssistantStream_currentRunSnapshot, event.data, \"f\");\n    switch (event.event) {\n        case 'thread.run.created':\n            break;\n        case 'thread.run.queued':\n            break;\n        case 'thread.run.in_progress':\n            break;\n        case 'thread.run.requires_action':\n        case 'thread.run.cancelled':\n        case 'thread.run.failed':\n        case 'thread.run.completed':\n        case 'thread.run.expired':\n            __classPrivateFieldSet(this, _AssistantStream_finalRun, event.data, \"f\");\n            if (__classPrivateFieldGet(this, _AssistantStream_currentToolCall, \"f\")) {\n                this._emit('toolCallDone', __classPrivateFieldGet(this, _AssistantStream_currentToolCall, \"f\"));\n                __classPrivateFieldSet(this, _AssistantStream_currentToolCall, undefined, \"f\");\n            }\n            break;\n        case 'thread.run.cancelling':\n            break;\n    }\n};\n//# sourceMappingURL=AssistantStream.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/lib/AssistantStream.mjs?");

/***/ }),

/***/ "./node_modules/openai/lib/ChatCompletionRunner.mjs":
/*!**********************************************************!*\
  !*** ./node_modules/openai/lib/ChatCompletionRunner.mjs ***!
  \**********************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   ChatCompletionRunner: () => (/* binding */ ChatCompletionRunner)\n/* harmony export */ });\n/* harmony import */ var _AbstractChatCompletionRunner_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./AbstractChatCompletionRunner.mjs */ \"./node_modules/openai/lib/AbstractChatCompletionRunner.mjs\");\n/* harmony import */ var _chatCompletionUtils_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./chatCompletionUtils.mjs */ \"./node_modules/openai/lib/chatCompletionUtils.mjs\");\n\n\nclass ChatCompletionRunner extends _AbstractChatCompletionRunner_mjs__WEBPACK_IMPORTED_MODULE_0__.AbstractChatCompletionRunner {\n    /** @deprecated - please use `runTools` instead. */\n    static runFunctions(completions, params, options) {\n        const runner = new ChatCompletionRunner();\n        const opts = {\n            ...options,\n            headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'runFunctions' },\n        };\n        runner._run(() => runner._runFunctions(completions, params, opts));\n        return runner;\n    }\n    static runTools(completions, params, options) {\n        const runner = new ChatCompletionRunner();\n        const opts = {\n            ...options,\n            headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'runTools' },\n        };\n        runner._run(() => runner._runTools(completions, params, opts));\n        return runner;\n    }\n    _addMessage(message) {\n        super._addMessage(message);\n        if ((0,_chatCompletionUtils_mjs__WEBPACK_IMPORTED_MODULE_1__.isAssistantMessage)(message) && message.content) {\n            this._emit('content', message.content);\n        }\n    }\n}\n//# sourceMappingURL=ChatCompletionRunner.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/lib/ChatCompletionRunner.mjs?");

/***/ }),

/***/ "./node_modules/openai/lib/ChatCompletionStream.mjs":
/*!**********************************************************!*\
  !*** ./node_modules/openai/lib/ChatCompletionStream.mjs ***!
  \**********************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   ChatCompletionStream: () => (/* binding */ ChatCompletionStream)\n/* harmony export */ });\n/* harmony import */ var openai_error__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! openai/error */ \"./node_modules/openai/error.mjs\");\n/* harmony import */ var _AbstractChatCompletionRunner_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./AbstractChatCompletionRunner.mjs */ \"./node_modules/openai/lib/AbstractChatCompletionRunner.mjs\");\n/* harmony import */ var openai_streaming__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! openai/streaming */ \"./node_modules/openai/streaming.mjs\");\nvar __classPrivateFieldGet = (undefined && undefined.__classPrivateFieldGet) || function (receiver, state, kind, f) {\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a getter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot read private member from an object whose class did not declare it\");\n    return kind === \"m\" ? f : kind === \"a\" ? f.call(receiver) : f ? f.value : state.get(receiver);\n};\nvar __classPrivateFieldSet = (undefined && undefined.__classPrivateFieldSet) || function (receiver, state, value, kind, f) {\n    if (kind === \"m\") throw new TypeError(\"Private method is not writable\");\n    if (kind === \"a\" && !f) throw new TypeError(\"Private accessor was defined without a setter\");\n    if (typeof state === \"function\" ? receiver !== state || !f : !state.has(receiver)) throw new TypeError(\"Cannot write private member to an object whose class did not declare it\");\n    return (kind === \"a\" ? f.call(receiver, value) : f ? f.value = value : state.set(receiver, value)), value;\n};\nvar _ChatCompletionStream_instances, _ChatCompletionStream_currentChatCompletionSnapshot, _ChatCompletionStream_beginRequest, _ChatCompletionStream_addChunk, _ChatCompletionStream_endRequest, _ChatCompletionStream_accumulateChatCompletion;\n\n\n\nclass ChatCompletionStream extends _AbstractChatCompletionRunner_mjs__WEBPACK_IMPORTED_MODULE_0__.AbstractChatCompletionRunner {\n    constructor() {\n        super(...arguments);\n        _ChatCompletionStream_instances.add(this);\n        _ChatCompletionStream_currentChatCompletionSnapshot.set(this, void 0);\n    }\n    get currentChatCompletionSnapshot() {\n        return __classPrivateFieldGet(this, _ChatCompletionStream_currentChatCompletionSnapshot, \"f\");\n    }\n    /**\n     * Intended for use on the frontend, consuming a stream produced with\n     * `.toReadableStream()` on the backend.\n     *\n     * Note that messages sent to the model do not appear in `.on('message')`\n     * in this context.\n     */\n    static fromReadableStream(stream) {\n        const runner = new ChatCompletionStream();\n        runner._run(() => runner._fromReadableStream(stream));\n        return runner;\n    }\n    static createChatCompletion(completions, params, options) {\n        const runner = new ChatCompletionStream();\n        runner._run(() => runner._runChatCompletion(completions, { ...params, stream: true }, { ...options, headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'stream' } }));\n        return runner;\n    }\n    async _createChatCompletion(completions, params, options) {\n        const signal = options?.signal;\n        if (signal) {\n            if (signal.aborted)\n                this.controller.abort();\n            signal.addEventListener('abort', () => this.controller.abort());\n        }\n        __classPrivateFieldGet(this, _ChatCompletionStream_instances, \"m\", _ChatCompletionStream_beginRequest).call(this);\n        const stream = await completions.create({ ...params, stream: true }, { ...options, signal: this.controller.signal });\n        this._connected();\n        for await (const chunk of stream) {\n            __classPrivateFieldGet(this, _ChatCompletionStream_instances, \"m\", _ChatCompletionStream_addChunk).call(this, chunk);\n        }\n        if (stream.controller.signal?.aborted) {\n            throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.APIUserAbortError();\n        }\n        return this._addChatCompletion(__classPrivateFieldGet(this, _ChatCompletionStream_instances, \"m\", _ChatCompletionStream_endRequest).call(this));\n    }\n    async _fromReadableStream(readableStream, options) {\n        const signal = options?.signal;\n        if (signal) {\n            if (signal.aborted)\n                this.controller.abort();\n            signal.addEventListener('abort', () => this.controller.abort());\n        }\n        __classPrivateFieldGet(this, _ChatCompletionStream_instances, \"m\", _ChatCompletionStream_beginRequest).call(this);\n        this._connected();\n        const stream = openai_streaming__WEBPACK_IMPORTED_MODULE_2__.Stream.fromReadableStream(readableStream, this.controller);\n        let chatId;\n        for await (const chunk of stream) {\n            if (chatId && chatId !== chunk.id) {\n                // A new request has been made.\n                this._addChatCompletion(__classPrivateFieldGet(this, _ChatCompletionStream_instances, \"m\", _ChatCompletionStream_endRequest).call(this));\n            }\n            __classPrivateFieldGet(this, _ChatCompletionStream_instances, \"m\", _ChatCompletionStream_addChunk).call(this, chunk);\n            chatId = chunk.id;\n        }\n        if (stream.controller.signal?.aborted) {\n            throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.APIUserAbortError();\n        }\n        return this._addChatCompletion(__classPrivateFieldGet(this, _ChatCompletionStream_instances, \"m\", _ChatCompletionStream_endRequest).call(this));\n    }\n    [(_ChatCompletionStream_currentChatCompletionSnapshot = new WeakMap(), _ChatCompletionStream_instances = new WeakSet(), _ChatCompletionStream_beginRequest = function _ChatCompletionStream_beginRequest() {\n        if (this.ended)\n            return;\n        __classPrivateFieldSet(this, _ChatCompletionStream_currentChatCompletionSnapshot, undefined, \"f\");\n    }, _ChatCompletionStream_addChunk = function _ChatCompletionStream_addChunk(chunk) {\n        if (this.ended)\n            return;\n        const completion = __classPrivateFieldGet(this, _ChatCompletionStream_instances, \"m\", _ChatCompletionStream_accumulateChatCompletion).call(this, chunk);\n        this._emit('chunk', chunk, completion);\n        const delta = chunk.choices[0]?.delta?.content;\n        const snapshot = completion.choices[0]?.message;\n        if (delta != null && snapshot?.role === 'assistant' && snapshot?.content) {\n            this._emit('content', delta, snapshot.content);\n        }\n    }, _ChatCompletionStream_endRequest = function _ChatCompletionStream_endRequest() {\n        if (this.ended) {\n            throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`stream has ended, this shouldn't happen`);\n        }\n        const snapshot = __classPrivateFieldGet(this, _ChatCompletionStream_currentChatCompletionSnapshot, \"f\");\n        if (!snapshot) {\n            throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`request ended without sending any chunks`);\n        }\n        __classPrivateFieldSet(this, _ChatCompletionStream_currentChatCompletionSnapshot, undefined, \"f\");\n        return finalizeChatCompletion(snapshot);\n    }, _ChatCompletionStream_accumulateChatCompletion = function _ChatCompletionStream_accumulateChatCompletion(chunk) {\n        var _a, _b, _c;\n        let snapshot = __classPrivateFieldGet(this, _ChatCompletionStream_currentChatCompletionSnapshot, \"f\");\n        const { choices, ...rest } = chunk;\n        if (!snapshot) {\n            snapshot = __classPrivateFieldSet(this, _ChatCompletionStream_currentChatCompletionSnapshot, {\n                ...rest,\n                choices: [],\n            }, \"f\");\n        }\n        else {\n            Object.assign(snapshot, rest);\n        }\n        for (const { delta, finish_reason, index, logprobs = null, ...other } of chunk.choices) {\n            let choice = snapshot.choices[index];\n            if (!choice) {\n                choice = snapshot.choices[index] = { finish_reason, index, message: {}, logprobs, ...other };\n            }\n            if (logprobs) {\n                if (!choice.logprobs) {\n                    choice.logprobs = Object.assign({}, logprobs);\n                }\n                else {\n                    const { content, ...rest } = logprobs;\n                    Object.assign(choice.logprobs, rest);\n                    if (content) {\n                        (_a = choice.logprobs).content ?? (_a.content = []);\n                        choice.logprobs.content.push(...content);\n                    }\n                }\n            }\n            if (finish_reason)\n                choice.finish_reason = finish_reason;\n            Object.assign(choice, other);\n            if (!delta)\n                continue; // Shouldn't happen; just in case.\n            const { content, function_call, role, tool_calls, ...rest } = delta;\n            Object.assign(choice.message, rest);\n            if (content)\n                choice.message.content = (choice.message.content || '') + content;\n            if (role)\n                choice.message.role = role;\n            if (function_call) {\n                if (!choice.message.function_call) {\n                    choice.message.function_call = function_call;\n                }\n                else {\n                    if (function_call.name)\n                        choice.message.function_call.name = function_call.name;\n                    if (function_call.arguments) {\n                        (_b = choice.message.function_call).arguments ?? (_b.arguments = '');\n                        choice.message.function_call.arguments += function_call.arguments;\n                    }\n                }\n            }\n            if (tool_calls) {\n                if (!choice.message.tool_calls)\n                    choice.message.tool_calls = [];\n                for (const { index, id, type, function: fn, ...rest } of tool_calls) {\n                    const tool_call = ((_c = choice.message.tool_calls)[index] ?? (_c[index] = {}));\n                    Object.assign(tool_call, rest);\n                    if (id)\n                        tool_call.id = id;\n                    if (type)\n                        tool_call.type = type;\n                    if (fn)\n                        tool_call.function ?? (tool_call.function = { arguments: '' });\n                    if (fn?.name)\n                        tool_call.function.name = fn.name;\n                    if (fn?.arguments)\n                        tool_call.function.arguments += fn.arguments;\n                }\n            }\n        }\n        return snapshot;\n    }, Symbol.asyncIterator)]() {\n        const pushQueue = [];\n        const readQueue = [];\n        let done = false;\n        this.on('chunk', (chunk) => {\n            const reader = readQueue.shift();\n            if (reader) {\n                reader.resolve(chunk);\n            }\n            else {\n                pushQueue.push(chunk);\n            }\n        });\n        this.on('end', () => {\n            done = true;\n            for (const reader of readQueue) {\n                reader.resolve(undefined);\n            }\n            readQueue.length = 0;\n        });\n        this.on('abort', (err) => {\n            done = true;\n            for (const reader of readQueue) {\n                reader.reject(err);\n            }\n            readQueue.length = 0;\n        });\n        this.on('error', (err) => {\n            done = true;\n            for (const reader of readQueue) {\n                reader.reject(err);\n            }\n            readQueue.length = 0;\n        });\n        return {\n            next: async () => {\n                if (!pushQueue.length) {\n                    if (done) {\n                        return { value: undefined, done: true };\n                    }\n                    return new Promise((resolve, reject) => readQueue.push({ resolve, reject })).then((chunk) => (chunk ? { value: chunk, done: false } : { value: undefined, done: true }));\n                }\n                const chunk = pushQueue.shift();\n                return { value: chunk, done: false };\n            },\n            return: async () => {\n                this.abort();\n                return { value: undefined, done: true };\n            },\n        };\n    }\n    toReadableStream() {\n        const stream = new openai_streaming__WEBPACK_IMPORTED_MODULE_2__.Stream(this[Symbol.asyncIterator].bind(this), this.controller);\n        return stream.toReadableStream();\n    }\n}\nfunction finalizeChatCompletion(snapshot) {\n    const { id, choices, created, model, system_fingerprint, ...rest } = snapshot;\n    return {\n        ...rest,\n        id,\n        choices: choices.map(({ message, finish_reason, index, logprobs, ...choiceRest }) => {\n            if (!finish_reason)\n                throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`missing finish_reason for choice ${index}`);\n            const { content = null, function_call, tool_calls, ...messageRest } = message;\n            const role = message.role; // this is what we expect; in theory it could be different which would make our types a slight lie but would be fine.\n            if (!role)\n                throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`missing role for choice ${index}`);\n            if (function_call) {\n                const { arguments: args, name } = function_call;\n                if (args == null)\n                    throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`missing function_call.arguments for choice ${index}`);\n                if (!name)\n                    throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`missing function_call.name for choice ${index}`);\n                return {\n                    ...choiceRest,\n                    message: { content, function_call: { arguments: args, name }, role },\n                    finish_reason,\n                    index,\n                    logprobs,\n                };\n            }\n            if (tool_calls) {\n                return {\n                    ...choiceRest,\n                    index,\n                    finish_reason,\n                    logprobs,\n                    message: {\n                        ...messageRest,\n                        role,\n                        content,\n                        tool_calls: tool_calls.map((tool_call, i) => {\n                            const { function: fn, type, id, ...toolRest } = tool_call;\n                            const { arguments: args, name, ...fnRest } = fn || {};\n                            if (id == null)\n                                throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`missing choices[${index}].tool_calls[${i}].id\\n${str(snapshot)}`);\n                            if (type == null)\n                                throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`missing choices[${index}].tool_calls[${i}].type\\n${str(snapshot)}`);\n                            if (name == null)\n                                throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`missing choices[${index}].tool_calls[${i}].function.name\\n${str(snapshot)}`);\n                            if (args == null)\n                                throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`missing choices[${index}].tool_calls[${i}].function.arguments\\n${str(snapshot)}`);\n                            return { ...toolRest, id, type, function: { ...fnRest, name, arguments: args } };\n                        }),\n                    },\n                };\n            }\n            return {\n                ...choiceRest,\n                message: { ...messageRest, content, role },\n                finish_reason,\n                index,\n                logprobs,\n            };\n        }),\n        created,\n        model,\n        object: 'chat.completion',\n        ...(system_fingerprint ? { system_fingerprint } : {}),\n    };\n}\nfunction str(x) {\n    return JSON.stringify(x);\n}\n//# sourceMappingURL=ChatCompletionStream.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/lib/ChatCompletionStream.mjs?");

/***/ }),

/***/ "./node_modules/openai/lib/ChatCompletionStreamingRunner.mjs":
/*!*******************************************************************!*\
  !*** ./node_modules/openai/lib/ChatCompletionStreamingRunner.mjs ***!
  \*******************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   ChatCompletionStreamingRunner: () => (/* binding */ ChatCompletionStreamingRunner)\n/* harmony export */ });\n/* harmony import */ var _ChatCompletionStream_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./ChatCompletionStream.mjs */ \"./node_modules/openai/lib/ChatCompletionStream.mjs\");\n\nclass ChatCompletionStreamingRunner extends _ChatCompletionStream_mjs__WEBPACK_IMPORTED_MODULE_0__.ChatCompletionStream {\n    static fromReadableStream(stream) {\n        const runner = new ChatCompletionStreamingRunner();\n        runner._run(() => runner._fromReadableStream(stream));\n        return runner;\n    }\n    /** @deprecated - please use `runTools` instead. */\n    static runFunctions(completions, params, options) {\n        const runner = new ChatCompletionStreamingRunner();\n        const opts = {\n            ...options,\n            headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'runFunctions' },\n        };\n        runner._run(() => runner._runFunctions(completions, params, opts));\n        return runner;\n    }\n    static runTools(completions, params, options) {\n        const runner = new ChatCompletionStreamingRunner();\n        const opts = {\n            ...options,\n            headers: { ...options?.headers, 'X-Stainless-Helper-Method': 'runTools' },\n        };\n        runner._run(() => runner._runTools(completions, params, opts));\n        return runner;\n    }\n}\n//# sourceMappingURL=ChatCompletionStreamingRunner.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/lib/ChatCompletionStreamingRunner.mjs?");

/***/ }),

/***/ "./node_modules/openai/lib/RunnableFunction.mjs":
/*!******************************************************!*\
  !*** ./node_modules/openai/lib/RunnableFunction.mjs ***!
  \******************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   ParsingFunction: () => (/* binding */ ParsingFunction),\n/* harmony export */   ParsingToolFunction: () => (/* binding */ ParsingToolFunction),\n/* harmony export */   isRunnableFunctionWithParse: () => (/* binding */ isRunnableFunctionWithParse)\n/* harmony export */ });\nfunction isRunnableFunctionWithParse(fn) {\n    return typeof fn.parse === 'function';\n}\n/**\n * This is helper class for passing a `function` and `parse` where the `function`\n * argument type matches the `parse` return type.\n *\n * @deprecated - please use ParsingToolFunction instead.\n */\nclass ParsingFunction {\n    constructor(input) {\n        this.function = input.function;\n        this.parse = input.parse;\n        this.parameters = input.parameters;\n        this.description = input.description;\n        this.name = input.name;\n    }\n}\n/**\n * This is helper class for passing a `function` and `parse` where the `function`\n * argument type matches the `parse` return type.\n */\nclass ParsingToolFunction {\n    constructor(input) {\n        this.type = 'function';\n        this.function = input;\n    }\n}\n//# sourceMappingURL=RunnableFunction.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/lib/RunnableFunction.mjs?");

/***/ }),

/***/ "./node_modules/openai/lib/Util.mjs":
/*!******************************************!*\
  !*** ./node_modules/openai/lib/Util.mjs ***!
  \******************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   allSettledWithThrow: () => (/* binding */ allSettledWithThrow)\n/* harmony export */ });\n/**\n * Like `Promise.allSettled()` but throws an error if any promises are rejected.\n */\nconst allSettledWithThrow = async (promises) => {\n    const results = await Promise.allSettled(promises);\n    const rejected = results.filter((result) => result.status === 'rejected');\n    if (rejected.length) {\n        for (const result of rejected) {\n            console.error(result.reason);\n        }\n        throw new Error(`${rejected.length} promise(s) failed - see the above errors`);\n    }\n    // Note: TS was complaining about using `.filter().map()` here for some reason\n    const values = [];\n    for (const result of results) {\n        if (result.status === 'fulfilled') {\n            values.push(result.value);\n        }\n    }\n    return values;\n};\n//# sourceMappingURL=Util.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/lib/Util.mjs?");

/***/ }),

/***/ "./node_modules/openai/lib/chatCompletionUtils.mjs":
/*!*********************************************************!*\
  !*** ./node_modules/openai/lib/chatCompletionUtils.mjs ***!
  \*********************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   isAssistantMessage: () => (/* binding */ isAssistantMessage),\n/* harmony export */   isFunctionMessage: () => (/* binding */ isFunctionMessage),\n/* harmony export */   isPresent: () => (/* binding */ isPresent),\n/* harmony export */   isToolMessage: () => (/* binding */ isToolMessage)\n/* harmony export */ });\nconst isAssistantMessage = (message) => {\n    return message?.role === 'assistant';\n};\nconst isFunctionMessage = (message) => {\n    return message?.role === 'function';\n};\nconst isToolMessage = (message) => {\n    return message?.role === 'tool';\n};\nfunction isPresent(obj) {\n    return obj != null;\n}\n//# sourceMappingURL=chatCompletionUtils.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/lib/chatCompletionUtils.mjs?");

/***/ }),

/***/ "./node_modules/openai/pagination.mjs":
/*!********************************************!*\
  !*** ./node_modules/openai/pagination.mjs ***!
  \********************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   CursorPage: () => (/* binding */ CursorPage),\n/* harmony export */   Page: () => (/* binding */ Page)\n/* harmony export */ });\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./core.mjs */ \"./node_modules/openai/core.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n/**\n * Note: no pagination actually occurs yet, this is for forwards-compatibility.\n */\nclass Page extends _core_mjs__WEBPACK_IMPORTED_MODULE_0__.AbstractPage {\n    constructor(client, response, body, options) {\n        super(client, response, body, options);\n        this.data = body.data || [];\n        this.object = body.object;\n    }\n    getPaginatedItems() {\n        return this.data ?? [];\n    }\n    // @deprecated Please use `nextPageInfo()` instead\n    /**\n     * This page represents a response that isn't actually paginated at the API level\n     * so there will never be any next page params.\n     */\n    nextPageParams() {\n        return null;\n    }\n    nextPageInfo() {\n        return null;\n    }\n}\nclass CursorPage extends _core_mjs__WEBPACK_IMPORTED_MODULE_0__.AbstractPage {\n    constructor(client, response, body, options) {\n        super(client, response, body, options);\n        this.data = body.data || [];\n    }\n    getPaginatedItems() {\n        return this.data ?? [];\n    }\n    // @deprecated Please use `nextPageInfo()` instead\n    nextPageParams() {\n        const info = this.nextPageInfo();\n        if (!info)\n            return null;\n        if ('params' in info)\n            return info.params;\n        const params = Object.fromEntries(info.url.searchParams);\n        if (!Object.keys(params).length)\n            return null;\n        return params;\n    }\n    nextPageInfo() {\n        const data = this.getPaginatedItems();\n        if (!data.length) {\n            return null;\n        }\n        const id = data[data.length - 1]?.id;\n        if (!id) {\n            return null;\n        }\n        return { params: { after: id } };\n    }\n}\n//# sourceMappingURL=pagination.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/pagination.mjs?");

/***/ }),

/***/ "./node_modules/openai/resource.mjs":
/*!******************************************!*\
  !*** ./node_modules/openai/resource.mjs ***!
  \******************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   APIResource: () => (/* binding */ APIResource)\n/* harmony export */ });\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\nclass APIResource {\n    constructor(client) {\n        this._client = client;\n    }\n}\n//# sourceMappingURL=resource.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resource.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/audio/audio.mjs":
/*!*******************************************************!*\
  !*** ./node_modules/openai/resources/audio/audio.mjs ***!
  \*******************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Audio: () => (/* binding */ Audio)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _speech_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ./speech.mjs */ \"./node_modules/openai/resources/audio/speech.mjs\");\n/* harmony import */ var _transcriptions_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./transcriptions.mjs */ \"./node_modules/openai/resources/audio/transcriptions.mjs\");\n/* harmony import */ var _translations_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ./translations.mjs */ \"./node_modules/openai/resources/audio/translations.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\nclass Audio extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    constructor() {\n        super(...arguments);\n        this.transcriptions = new _transcriptions_mjs__WEBPACK_IMPORTED_MODULE_1__.Transcriptions(this._client);\n        this.translations = new _translations_mjs__WEBPACK_IMPORTED_MODULE_2__.Translations(this._client);\n        this.speech = new _speech_mjs__WEBPACK_IMPORTED_MODULE_3__.Speech(this._client);\n    }\n}\n(function (Audio) {\n    Audio.Transcriptions = _transcriptions_mjs__WEBPACK_IMPORTED_MODULE_1__.Transcriptions;\n    Audio.Translations = _translations_mjs__WEBPACK_IMPORTED_MODULE_2__.Translations;\n    Audio.Speech = _speech_mjs__WEBPACK_IMPORTED_MODULE_3__.Speech;\n})(Audio || (Audio = {}));\n//# sourceMappingURL=audio.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/audio/audio.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/audio/speech.mjs":
/*!********************************************************!*\
  !*** ./node_modules/openai/resources/audio/speech.mjs ***!
  \********************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Speech: () => (/* binding */ Speech)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nclass Speech extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    /**\n     * Generates audio from the input text.\n     */\n    create(body, options) {\n        return this._client.post('/audio/speech', { body, ...options, __binaryResponse: true });\n    }\n}\n(function (Speech) {\n})(Speech || (Speech = {}));\n//# sourceMappingURL=speech.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/audio/speech.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/audio/transcriptions.mjs":
/*!****************************************************************!*\
  !*** ./node_modules/openai/resources/audio/transcriptions.mjs ***!
  \****************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Transcriptions: () => (/* binding */ Transcriptions)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../../core.mjs */ \"./node_modules/openai/uploads.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\nclass Transcriptions extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    /**\n     * Transcribes audio into the input language.\n     */\n    create(body, options) {\n        return this._client.post('/audio/transcriptions', (0,_core_mjs__WEBPACK_IMPORTED_MODULE_1__.multipartFormRequestOptions)({ body, ...options }));\n    }\n}\n(function (Transcriptions) {\n})(Transcriptions || (Transcriptions = {}));\n//# sourceMappingURL=transcriptions.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/audio/transcriptions.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/audio/translations.mjs":
/*!**************************************************************!*\
  !*** ./node_modules/openai/resources/audio/translations.mjs ***!
  \**************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Translations: () => (/* binding */ Translations)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../../core.mjs */ \"./node_modules/openai/uploads.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\nclass Translations extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    /**\n     * Translates audio into English.\n     */\n    create(body, options) {\n        return this._client.post('/audio/translations', (0,_core_mjs__WEBPACK_IMPORTED_MODULE_1__.multipartFormRequestOptions)({ body, ...options }));\n    }\n}\n(function (Translations) {\n})(Translations || (Translations = {}));\n//# sourceMappingURL=translations.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/audio/translations.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/batches.mjs":
/*!***************************************************!*\
  !*** ./node_modules/openai/resources/batches.mjs ***!
  \***************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Batches: () => (/* binding */ Batches),\n/* harmony export */   BatchesPage: () => (/* binding */ BatchesPage)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../core.mjs */ \"./node_modules/openai/core.mjs\");\n/* harmony import */ var _batches_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ./batches.mjs */ \"./node_modules/openai/resources/batches.mjs\");\n/* harmony import */ var _pagination_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../pagination.mjs */ \"./node_modules/openai/pagination.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\nclass Batches extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    /**\n     * Creates and executes a batch from an uploaded file of requests\n     */\n    create(body, options) {\n        return this._client.post('/batches', { body, ...options });\n    }\n    /**\n     * Retrieves a batch.\n     */\n    retrieve(batchId, options) {\n        return this._client.get(`/batches/${batchId}`, options);\n    }\n    list(query = {}, options) {\n        if ((0,_core_mjs__WEBPACK_IMPORTED_MODULE_1__.isRequestOptions)(query)) {\n            return this.list({}, query);\n        }\n        return this._client.getAPIList('/batches', BatchesPage, { query, ...options });\n    }\n    /**\n     * Cancels an in-progress batch. The batch will be in status `cancelling` for up to\n     * 10 minutes, before changing to `cancelled`, where it will have partial results\n     * (if any) available in the output file.\n     */\n    cancel(batchId, options) {\n        return this._client.post(`/batches/${batchId}/cancel`, options);\n    }\n}\nclass BatchesPage extends _pagination_mjs__WEBPACK_IMPORTED_MODULE_2__.CursorPage {\n}\n(function (Batches) {\n    Batches.BatchesPage = _batches_mjs__WEBPACK_IMPORTED_MODULE_3__.BatchesPage;\n})(Batches || (Batches = {}));\n//# sourceMappingURL=batches.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/batches.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/beta/assistants.mjs":
/*!***********************************************************!*\
  !*** ./node_modules/openai/resources/beta/assistants.mjs ***!
  \***********************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Assistants: () => (/* binding */ Assistants),\n/* harmony export */   AssistantsPage: () => (/* binding */ AssistantsPage)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../../core.mjs */ \"./node_modules/openai/core.mjs\");\n/* harmony import */ var _assistants_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ./assistants.mjs */ \"./node_modules/openai/resources/beta/assistants.mjs\");\n/* harmony import */ var _pagination_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../../pagination.mjs */ \"./node_modules/openai/pagination.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\nclass Assistants extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    /**\n     * Create an assistant with a model and instructions.\n     */\n    create(body, options) {\n        return this._client.post('/assistants', {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Retrieves an assistant.\n     */\n    retrieve(assistantId, options) {\n        return this._client.get(`/assistants/${assistantId}`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Modifies an assistant.\n     */\n    update(assistantId, body, options) {\n        return this._client.post(`/assistants/${assistantId}`, {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    list(query = {}, options) {\n        if ((0,_core_mjs__WEBPACK_IMPORTED_MODULE_1__.isRequestOptions)(query)) {\n            return this.list({}, query);\n        }\n        return this._client.getAPIList('/assistants', AssistantsPage, {\n            query,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Delete an assistant.\n     */\n    del(assistantId, options) {\n        return this._client.delete(`/assistants/${assistantId}`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n}\nclass AssistantsPage extends _pagination_mjs__WEBPACK_IMPORTED_MODULE_2__.CursorPage {\n}\n(function (Assistants) {\n    Assistants.AssistantsPage = _assistants_mjs__WEBPACK_IMPORTED_MODULE_3__.AssistantsPage;\n})(Assistants || (Assistants = {}));\n//# sourceMappingURL=assistants.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/beta/assistants.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/beta/beta.mjs":
/*!*****************************************************!*\
  !*** ./node_modules/openai/resources/beta/beta.mjs ***!
  \*****************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Beta: () => (/* binding */ Beta)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _assistants_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ./assistants.mjs */ \"./node_modules/openai/resources/beta/assistants.mjs\");\n/* harmony import */ var _chat_chat_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ./chat/chat.mjs */ \"./node_modules/openai/resources/beta/chat/chat.mjs\");\n/* harmony import */ var _threads_threads_mjs__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! ./threads/threads.mjs */ \"./node_modules/openai/resources/beta/threads/threads.mjs\");\n/* harmony import */ var _vector_stores_vector_stores_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./vector-stores/vector-stores.mjs */ \"./node_modules/openai/resources/beta/vector-stores/vector-stores.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\n\nclass Beta extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    constructor() {\n        super(...arguments);\n        this.vectorStores = new _vector_stores_vector_stores_mjs__WEBPACK_IMPORTED_MODULE_1__.VectorStores(this._client);\n        this.chat = new _chat_chat_mjs__WEBPACK_IMPORTED_MODULE_2__.Chat(this._client);\n        this.assistants = new _assistants_mjs__WEBPACK_IMPORTED_MODULE_3__.Assistants(this._client);\n        this.threads = new _threads_threads_mjs__WEBPACK_IMPORTED_MODULE_4__.Threads(this._client);\n    }\n}\n(function (Beta) {\n    Beta.VectorStores = _vector_stores_vector_stores_mjs__WEBPACK_IMPORTED_MODULE_1__.VectorStores;\n    Beta.VectorStoresPage = _vector_stores_vector_stores_mjs__WEBPACK_IMPORTED_MODULE_1__.VectorStoresPage;\n    Beta.Chat = _chat_chat_mjs__WEBPACK_IMPORTED_MODULE_2__.Chat;\n    Beta.Assistants = _assistants_mjs__WEBPACK_IMPORTED_MODULE_3__.Assistants;\n    Beta.AssistantsPage = _assistants_mjs__WEBPACK_IMPORTED_MODULE_3__.AssistantsPage;\n    Beta.Threads = _threads_threads_mjs__WEBPACK_IMPORTED_MODULE_4__.Threads;\n})(Beta || (Beta = {}));\n//# sourceMappingURL=beta.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/beta/beta.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/beta/chat/chat.mjs":
/*!**********************************************************!*\
  !*** ./node_modules/openai/resources/beta/chat/chat.mjs ***!
  \**********************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Chat: () => (/* binding */ Chat)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _completions_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./completions.mjs */ \"./node_modules/openai/resources/beta/chat/completions.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\nclass Chat extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    constructor() {\n        super(...arguments);\n        this.completions = new _completions_mjs__WEBPACK_IMPORTED_MODULE_1__.Completions(this._client);\n    }\n}\n(function (Chat) {\n    Chat.Completions = _completions_mjs__WEBPACK_IMPORTED_MODULE_1__.Completions;\n})(Chat || (Chat = {}));\n//# sourceMappingURL=chat.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/beta/chat/chat.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/beta/chat/completions.mjs":
/*!*****************************************************************!*\
  !*** ./node_modules/openai/resources/beta/chat/completions.mjs ***!
  \*****************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   ChatCompletionRunner: () => (/* reexport safe */ _lib_ChatCompletionRunner_mjs__WEBPACK_IMPORTED_MODULE_0__.ChatCompletionRunner),\n/* harmony export */   ChatCompletionStream: () => (/* reexport safe */ _lib_ChatCompletionStream_mjs__WEBPACK_IMPORTED_MODULE_3__.ChatCompletionStream),\n/* harmony export */   ChatCompletionStreamingRunner: () => (/* reexport safe */ _lib_ChatCompletionStreamingRunner_mjs__WEBPACK_IMPORTED_MODULE_1__.ChatCompletionStreamingRunner),\n/* harmony export */   Completions: () => (/* binding */ Completions),\n/* harmony export */   ParsingFunction: () => (/* reexport safe */ _lib_RunnableFunction_mjs__WEBPACK_IMPORTED_MODULE_2__.ParsingFunction),\n/* harmony export */   ParsingToolFunction: () => (/* reexport safe */ _lib_RunnableFunction_mjs__WEBPACK_IMPORTED_MODULE_2__.ParsingToolFunction)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! ../../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _lib_ChatCompletionRunner_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../../lib/ChatCompletionRunner.mjs */ \"./node_modules/openai/lib/ChatCompletionRunner.mjs\");\n/* harmony import */ var _lib_ChatCompletionStreamingRunner_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../../../lib/ChatCompletionStreamingRunner.mjs */ \"./node_modules/openai/lib/ChatCompletionStreamingRunner.mjs\");\n/* harmony import */ var _lib_RunnableFunction_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../../../lib/RunnableFunction.mjs */ \"./node_modules/openai/lib/RunnableFunction.mjs\");\n/* harmony import */ var _lib_ChatCompletionStream_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ../../../lib/ChatCompletionStream.mjs */ \"./node_modules/openai/lib/ChatCompletionStream.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\n\n\n\n\nclass Completions extends _resource_mjs__WEBPACK_IMPORTED_MODULE_4__.APIResource {\n    runFunctions(body, options) {\n        if (body.stream) {\n            return _lib_ChatCompletionStreamingRunner_mjs__WEBPACK_IMPORTED_MODULE_1__.ChatCompletionStreamingRunner.runFunctions(this._client.chat.completions, body, options);\n        }\n        return _lib_ChatCompletionRunner_mjs__WEBPACK_IMPORTED_MODULE_0__.ChatCompletionRunner.runFunctions(this._client.chat.completions, body, options);\n    }\n    runTools(body, options) {\n        if (body.stream) {\n            return _lib_ChatCompletionStreamingRunner_mjs__WEBPACK_IMPORTED_MODULE_1__.ChatCompletionStreamingRunner.runTools(this._client.chat.completions, body, options);\n        }\n        return _lib_ChatCompletionRunner_mjs__WEBPACK_IMPORTED_MODULE_0__.ChatCompletionRunner.runTools(this._client.chat.completions, body, options);\n    }\n    /**\n     * Creates a chat completion stream\n     */\n    stream(body, options) {\n        return _lib_ChatCompletionStream_mjs__WEBPACK_IMPORTED_MODULE_3__.ChatCompletionStream.createChatCompletion(this._client.chat.completions, body, options);\n    }\n}\n//# sourceMappingURL=completions.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/beta/chat/completions.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/beta/threads/messages.mjs":
/*!*****************************************************************!*\
  !*** ./node_modules/openai/resources/beta/threads/messages.mjs ***!
  \*****************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Messages: () => (/* binding */ Messages),\n/* harmony export */   MessagesPage: () => (/* binding */ MessagesPage)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../../../core.mjs */ \"./node_modules/openai/core.mjs\");\n/* harmony import */ var _messages_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ./messages.mjs */ \"./node_modules/openai/resources/beta/threads/messages.mjs\");\n/* harmony import */ var _pagination_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../../../pagination.mjs */ \"./node_modules/openai/pagination.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\nclass Messages extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    /**\n     * Create a message.\n     */\n    create(threadId, body, options) {\n        return this._client.post(`/threads/${threadId}/messages`, {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Retrieve a message.\n     */\n    retrieve(threadId, messageId, options) {\n        return this._client.get(`/threads/${threadId}/messages/${messageId}`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Modifies a message.\n     */\n    update(threadId, messageId, body, options) {\n        return this._client.post(`/threads/${threadId}/messages/${messageId}`, {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    list(threadId, query = {}, options) {\n        if ((0,_core_mjs__WEBPACK_IMPORTED_MODULE_1__.isRequestOptions)(query)) {\n            return this.list(threadId, {}, query);\n        }\n        return this._client.getAPIList(`/threads/${threadId}/messages`, MessagesPage, {\n            query,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Deletes a message.\n     */\n    del(threadId, messageId, options) {\n        return this._client.delete(`/threads/${threadId}/messages/${messageId}`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n}\nclass MessagesPage extends _pagination_mjs__WEBPACK_IMPORTED_MODULE_2__.CursorPage {\n}\n(function (Messages) {\n    Messages.MessagesPage = _messages_mjs__WEBPACK_IMPORTED_MODULE_3__.MessagesPage;\n})(Messages || (Messages = {}));\n//# sourceMappingURL=messages.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/beta/threads/messages.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/beta/threads/runs/runs.mjs":
/*!******************************************************************!*\
  !*** ./node_modules/openai/resources/beta/threads/runs/runs.mjs ***!
  \******************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Runs: () => (/* binding */ Runs),\n/* harmony export */   RunsPage: () => (/* binding */ RunsPage)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../../../../core.mjs */ \"./node_modules/openai/core.mjs\");\n/* harmony import */ var _lib_AssistantStream_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ../../../../lib/AssistantStream.mjs */ \"./node_modules/openai/lib/AssistantStream.mjs\");\n/* harmony import */ var _runs_mjs__WEBPACK_IMPORTED_MODULE_5__ = __webpack_require__(/*! ./runs.mjs */ \"./node_modules/openai/resources/beta/threads/runs/runs.mjs\");\n/* harmony import */ var _steps_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./steps.mjs */ \"./node_modules/openai/resources/beta/threads/runs/steps.mjs\");\n/* harmony import */ var _pagination_mjs__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! ../../../../pagination.mjs */ \"./node_modules/openai/pagination.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\n\n\n\nclass Runs extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    constructor() {\n        super(...arguments);\n        this.steps = new _steps_mjs__WEBPACK_IMPORTED_MODULE_1__.Steps(this._client);\n    }\n    create(threadId, body, options) {\n        return this._client.post(`/threads/${threadId}/runs`, {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n            stream: body.stream ?? false,\n        });\n    }\n    /**\n     * Retrieves a run.\n     */\n    retrieve(threadId, runId, options) {\n        return this._client.get(`/threads/${threadId}/runs/${runId}`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Modifies a run.\n     */\n    update(threadId, runId, body, options) {\n        return this._client.post(`/threads/${threadId}/runs/${runId}`, {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    list(threadId, query = {}, options) {\n        if ((0,_core_mjs__WEBPACK_IMPORTED_MODULE_2__.isRequestOptions)(query)) {\n            return this.list(threadId, {}, query);\n        }\n        return this._client.getAPIList(`/threads/${threadId}/runs`, RunsPage, {\n            query,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Cancels a run that is `in_progress`.\n     */\n    cancel(threadId, runId, options) {\n        return this._client.post(`/threads/${threadId}/runs/${runId}/cancel`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * A helper to create a run an poll for a terminal state. More information on Run\n     * lifecycles can be found here:\n     * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps\n     */\n    async createAndPoll(threadId, body, options) {\n        const run = await this.create(threadId, body, options);\n        return await this.poll(threadId, run.id, options);\n    }\n    /**\n     * Create a Run stream\n     *\n     * @deprecated use `stream` instead\n     */\n    createAndStream(threadId, body, options) {\n        return _lib_AssistantStream_mjs__WEBPACK_IMPORTED_MODULE_3__.AssistantStream.createAssistantStream(threadId, this._client.beta.threads.runs, body, options);\n    }\n    /**\n     * A helper to poll a run status until it reaches a terminal state. More\n     * information on Run lifecycles can be found here:\n     * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps\n     */\n    async poll(threadId, runId, options) {\n        const headers = { ...options?.headers, 'X-Stainless-Poll-Helper': 'true' };\n        if (options?.pollIntervalMs) {\n            headers['X-Stainless-Custom-Poll-Interval'] = options.pollIntervalMs.toString();\n        }\n        while (true) {\n            const { data: run, response } = await this.retrieve(threadId, runId, {\n                ...options,\n                headers: { ...options?.headers, ...headers },\n            }).withResponse();\n            switch (run.status) {\n                //If we are in any sort of intermediate state we poll\n                case 'queued':\n                case 'in_progress':\n                case 'cancelling':\n                    let sleepInterval = 5000;\n                    if (options?.pollIntervalMs) {\n                        sleepInterval = options.pollIntervalMs;\n                    }\n                    else {\n                        const headerInterval = response.headers.get('openai-poll-after-ms');\n                        if (headerInterval) {\n                            const headerIntervalMs = parseInt(headerInterval);\n                            if (!isNaN(headerIntervalMs)) {\n                                sleepInterval = headerIntervalMs;\n                            }\n                        }\n                    }\n                    await (0,_core_mjs__WEBPACK_IMPORTED_MODULE_2__.sleep)(sleepInterval);\n                    break;\n                //We return the run in any terminal state.\n                case 'requires_action':\n                case 'incomplete':\n                case 'cancelled':\n                case 'completed':\n                case 'failed':\n                case 'expired':\n                    return run;\n            }\n        }\n    }\n    /**\n     * Create a Run stream\n     */\n    stream(threadId, body, options) {\n        return _lib_AssistantStream_mjs__WEBPACK_IMPORTED_MODULE_3__.AssistantStream.createAssistantStream(threadId, this._client.beta.threads.runs, body, options);\n    }\n    submitToolOutputs(threadId, runId, body, options) {\n        return this._client.post(`/threads/${threadId}/runs/${runId}/submit_tool_outputs`, {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n            stream: body.stream ?? false,\n        });\n    }\n    /**\n     * A helper to submit a tool output to a run and poll for a terminal run state.\n     * More information on Run lifecycles can be found here:\n     * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps\n     */\n    async submitToolOutputsAndPoll(threadId, runId, body, options) {\n        const run = await this.submitToolOutputs(threadId, runId, body, options);\n        return await this.poll(threadId, run.id, options);\n    }\n    /**\n     * Submit the tool outputs from a previous run and stream the run to a terminal\n     * state. More information on Run lifecycles can be found here:\n     * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps\n     */\n    submitToolOutputsStream(threadId, runId, body, options) {\n        return _lib_AssistantStream_mjs__WEBPACK_IMPORTED_MODULE_3__.AssistantStream.createToolAssistantStream(threadId, runId, this._client.beta.threads.runs, body, options);\n    }\n}\nclass RunsPage extends _pagination_mjs__WEBPACK_IMPORTED_MODULE_4__.CursorPage {\n}\n(function (Runs) {\n    Runs.RunsPage = _runs_mjs__WEBPACK_IMPORTED_MODULE_5__.RunsPage;\n    Runs.Steps = _steps_mjs__WEBPACK_IMPORTED_MODULE_1__.Steps;\n    Runs.RunStepsPage = _steps_mjs__WEBPACK_IMPORTED_MODULE_1__.RunStepsPage;\n})(Runs || (Runs = {}));\n//# sourceMappingURL=runs.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/beta/threads/runs/runs.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/beta/threads/runs/steps.mjs":
/*!*******************************************************************!*\
  !*** ./node_modules/openai/resources/beta/threads/runs/steps.mjs ***!
  \*******************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   RunStepsPage: () => (/* binding */ RunStepsPage),\n/* harmony export */   Steps: () => (/* binding */ Steps)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../../../../core.mjs */ \"./node_modules/openai/core.mjs\");\n/* harmony import */ var _steps_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ./steps.mjs */ \"./node_modules/openai/resources/beta/threads/runs/steps.mjs\");\n/* harmony import */ var _pagination_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../../../../pagination.mjs */ \"./node_modules/openai/pagination.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\nclass Steps extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    /**\n     * Retrieves a run step.\n     */\n    retrieve(threadId, runId, stepId, options) {\n        return this._client.get(`/threads/${threadId}/runs/${runId}/steps/${stepId}`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    list(threadId, runId, query = {}, options) {\n        if ((0,_core_mjs__WEBPACK_IMPORTED_MODULE_1__.isRequestOptions)(query)) {\n            return this.list(threadId, runId, {}, query);\n        }\n        return this._client.getAPIList(`/threads/${threadId}/runs/${runId}/steps`, RunStepsPage, {\n            query,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n}\nclass RunStepsPage extends _pagination_mjs__WEBPACK_IMPORTED_MODULE_2__.CursorPage {\n}\n(function (Steps) {\n    Steps.RunStepsPage = _steps_mjs__WEBPACK_IMPORTED_MODULE_3__.RunStepsPage;\n})(Steps || (Steps = {}));\n//# sourceMappingURL=steps.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/beta/threads/runs/steps.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/beta/threads/threads.mjs":
/*!****************************************************************!*\
  !*** ./node_modules/openai/resources/beta/threads/threads.mjs ***!
  \****************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Threads: () => (/* binding */ Threads)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ../../../core.mjs */ \"./node_modules/openai/core.mjs\");\n/* harmony import */ var _lib_AssistantStream_mjs__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! ../../../lib/AssistantStream.mjs */ \"./node_modules/openai/lib/AssistantStream.mjs\");\n/* harmony import */ var _messages_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ./messages.mjs */ \"./node_modules/openai/resources/beta/threads/messages.mjs\");\n/* harmony import */ var _runs_runs_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./runs/runs.mjs */ \"./node_modules/openai/resources/beta/threads/runs/runs.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\n\nclass Threads extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    constructor() {\n        super(...arguments);\n        this.runs = new _runs_runs_mjs__WEBPACK_IMPORTED_MODULE_1__.Runs(this._client);\n        this.messages = new _messages_mjs__WEBPACK_IMPORTED_MODULE_2__.Messages(this._client);\n    }\n    create(body = {}, options) {\n        if ((0,_core_mjs__WEBPACK_IMPORTED_MODULE_3__.isRequestOptions)(body)) {\n            return this.create({}, body);\n        }\n        return this._client.post('/threads', {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Retrieves a thread.\n     */\n    retrieve(threadId, options) {\n        return this._client.get(`/threads/${threadId}`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Modifies a thread.\n     */\n    update(threadId, body, options) {\n        return this._client.post(`/threads/${threadId}`, {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Delete a thread.\n     */\n    del(threadId, options) {\n        return this._client.delete(`/threads/${threadId}`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    createAndRun(body, options) {\n        return this._client.post('/threads/runs', {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n            stream: body.stream ?? false,\n        });\n    }\n    /**\n     * A helper to create a thread, start a run and then poll for a terminal state.\n     * More information on Run lifecycles can be found here:\n     * https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps\n     */\n    async createAndRunPoll(body, options) {\n        const run = await this.createAndRun(body, options);\n        return await this.runs.poll(run.thread_id, run.id, options);\n    }\n    /**\n     * Create a thread and stream the run back\n     */\n    createAndRunStream(body, options) {\n        return _lib_AssistantStream_mjs__WEBPACK_IMPORTED_MODULE_4__.AssistantStream.createThreadAssistantStream(body, this._client.beta.threads, options);\n    }\n}\n(function (Threads) {\n    Threads.Runs = _runs_runs_mjs__WEBPACK_IMPORTED_MODULE_1__.Runs;\n    Threads.RunsPage = _runs_runs_mjs__WEBPACK_IMPORTED_MODULE_1__.RunsPage;\n    Threads.Messages = _messages_mjs__WEBPACK_IMPORTED_MODULE_2__.Messages;\n    Threads.MessagesPage = _messages_mjs__WEBPACK_IMPORTED_MODULE_2__.MessagesPage;\n})(Threads || (Threads = {}));\n//# sourceMappingURL=threads.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/beta/threads/threads.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/beta/vector-stores/file-batches.mjs":
/*!***************************************************************************!*\
  !*** ./node_modules/openai/resources/beta/vector-stores/file-batches.mjs ***!
  \***************************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   FileBatches: () => (/* binding */ FileBatches),\n/* harmony export */   VectorStoreFilesPage: () => (/* reexport safe */ _files_mjs__WEBPACK_IMPORTED_MODULE_2__.VectorStoreFilesPage)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../../../core.mjs */ \"./node_modules/openai/core.mjs\");\n/* harmony import */ var _lib_Util_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ../../../lib/Util.mjs */ \"./node_modules/openai/lib/Util.mjs\");\n/* harmony import */ var _files_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ./files.mjs */ \"./node_modules/openai/resources/beta/vector-stores/files.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\n\nclass FileBatches extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    /**\n     * Create a vector store file batch.\n     */\n    create(vectorStoreId, body, options) {\n        return this._client.post(`/vector_stores/${vectorStoreId}/file_batches`, {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Retrieves a vector store file batch.\n     */\n    retrieve(vectorStoreId, batchId, options) {\n        return this._client.get(`/vector_stores/${vectorStoreId}/file_batches/${batchId}`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Cancel a vector store file batch. This attempts to cancel the processing of\n     * files in this batch as soon as possible.\n     */\n    cancel(vectorStoreId, batchId, options) {\n        return this._client.post(`/vector_stores/${vectorStoreId}/file_batches/${batchId}/cancel`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Create a vector store batch and poll until all files have been processed.\n     */\n    async createAndPoll(vectorStoreId, body, options) {\n        const batch = await this.create(vectorStoreId, body);\n        return await this.poll(vectorStoreId, batch.id, options);\n    }\n    listFiles(vectorStoreId, batchId, query = {}, options) {\n        if ((0,_core_mjs__WEBPACK_IMPORTED_MODULE_1__.isRequestOptions)(query)) {\n            return this.listFiles(vectorStoreId, batchId, {}, query);\n        }\n        return this._client.getAPIList(`/vector_stores/${vectorStoreId}/file_batches/${batchId}/files`, _files_mjs__WEBPACK_IMPORTED_MODULE_2__.VectorStoreFilesPage, { query, ...options, headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers } });\n    }\n    /**\n     * Wait for the given file batch to be processed.\n     *\n     * Note: this will return even if one of the files failed to process, you need to\n     * check batch.file_counts.failed_count to handle this case.\n     */\n    async poll(vectorStoreId, batchId, options) {\n        const headers = { ...options?.headers, 'X-Stainless-Poll-Helper': 'true' };\n        if (options?.pollIntervalMs) {\n            headers['X-Stainless-Custom-Poll-Interval'] = options.pollIntervalMs.toString();\n        }\n        while (true) {\n            const { data: batch, response } = await this.retrieve(vectorStoreId, batchId, {\n                ...options,\n                headers,\n            }).withResponse();\n            switch (batch.status) {\n                case 'in_progress':\n                    let sleepInterval = 5000;\n                    if (options?.pollIntervalMs) {\n                        sleepInterval = options.pollIntervalMs;\n                    }\n                    else {\n                        const headerInterval = response.headers.get('openai-poll-after-ms');\n                        if (headerInterval) {\n                            const headerIntervalMs = parseInt(headerInterval);\n                            if (!isNaN(headerIntervalMs)) {\n                                sleepInterval = headerIntervalMs;\n                            }\n                        }\n                    }\n                    await (0,_core_mjs__WEBPACK_IMPORTED_MODULE_1__.sleep)(sleepInterval);\n                    break;\n                case 'failed':\n                case 'cancelled':\n                case 'completed':\n                    return batch;\n            }\n        }\n    }\n    /**\n     * Uploads the given files concurrently and then creates a vector store file batch.\n     *\n     * The concurrency limit is configurable using the `maxConcurrency` parameter.\n     */\n    async uploadAndPoll(vectorStoreId, { files, fileIds = [] }, options) {\n        if (files === null || files.length == 0) {\n            throw new Error('No files provided to process.');\n        }\n        const configuredConcurrency = options?.maxConcurrency ?? 5;\n        //We cap the number of workers at the number of files (so we don't start any unnecessary workers)\n        const concurrencyLimit = Math.min(configuredConcurrency, files.length);\n        const client = this._client;\n        const fileIterator = files.values();\n        const allFileIds = [...fileIds];\n        //This code is based on this design. The libraries don't accommodate our environment limits.\n        // https://stackoverflow.com/questions/40639432/what-is-the-best-way-to-limit-concurrency-when-using-es6s-promise-all\n        async function processFiles(iterator) {\n            for (let item of iterator) {\n                const fileObj = await client.files.create({ file: item, purpose: 'assistants' }, options);\n                allFileIds.push(fileObj.id);\n            }\n        }\n        //Start workers to process results\n        const workers = Array(concurrencyLimit).fill(fileIterator).map(processFiles);\n        //Wait for all processing to complete.\n        await (0,_lib_Util_mjs__WEBPACK_IMPORTED_MODULE_3__.allSettledWithThrow)(workers);\n        return await this.createAndPoll(vectorStoreId, {\n            file_ids: allFileIds,\n        });\n    }\n}\n(function (FileBatches) {\n})(FileBatches || (FileBatches = {}));\n\n//# sourceMappingURL=file-batches.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/beta/vector-stores/file-batches.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/beta/vector-stores/files.mjs":
/*!********************************************************************!*\
  !*** ./node_modules/openai/resources/beta/vector-stores/files.mjs ***!
  \********************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Files: () => (/* binding */ Files),\n/* harmony export */   VectorStoreFilesPage: () => (/* binding */ VectorStoreFilesPage)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../../../core.mjs */ \"./node_modules/openai/core.mjs\");\n/* harmony import */ var _files_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ./files.mjs */ \"./node_modules/openai/resources/beta/vector-stores/files.mjs\");\n/* harmony import */ var _pagination_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../../../pagination.mjs */ \"./node_modules/openai/pagination.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\n\nclass Files extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    /**\n     * Create a vector store file by attaching a\n     * [File](https://platform.openai.com/docs/api-reference/files) to a\n     * [vector store](https://platform.openai.com/docs/api-reference/vector-stores/object).\n     */\n    create(vectorStoreId, body, options) {\n        return this._client.post(`/vector_stores/${vectorStoreId}/files`, {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Retrieves a vector store file.\n     */\n    retrieve(vectorStoreId, fileId, options) {\n        return this._client.get(`/vector_stores/${vectorStoreId}/files/${fileId}`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    list(vectorStoreId, query = {}, options) {\n        if ((0,_core_mjs__WEBPACK_IMPORTED_MODULE_1__.isRequestOptions)(query)) {\n            return this.list(vectorStoreId, {}, query);\n        }\n        return this._client.getAPIList(`/vector_stores/${vectorStoreId}/files`, VectorStoreFilesPage, {\n            query,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Delete a vector store file. This will remove the file from the vector store but\n     * the file itself will not be deleted. To delete the file, use the\n     * [delete file](https://platform.openai.com/docs/api-reference/files/delete)\n     * endpoint.\n     */\n    del(vectorStoreId, fileId, options) {\n        return this._client.delete(`/vector_stores/${vectorStoreId}/files/${fileId}`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Attach a file to the given vector store and wait for it to be processed.\n     */\n    async createAndPoll(vectorStoreId, body, options) {\n        const file = await this.create(vectorStoreId, body, options);\n        return await this.poll(vectorStoreId, file.id, options);\n    }\n    /**\n     * Wait for the vector store file to finish processing.\n     *\n     * Note: this will return even if the file failed to process, you need to check\n     * file.last_error and file.status to handle these cases\n     */\n    async poll(vectorStoreId, fileId, options) {\n        const headers = { ...options?.headers, 'X-Stainless-Poll-Helper': 'true' };\n        if (options?.pollIntervalMs) {\n            headers['X-Stainless-Custom-Poll-Interval'] = options.pollIntervalMs.toString();\n        }\n        while (true) {\n            const fileResponse = await this.retrieve(vectorStoreId, fileId, {\n                ...options,\n                headers,\n            }).withResponse();\n            const file = fileResponse.data;\n            switch (file.status) {\n                case 'in_progress':\n                    let sleepInterval = 5000;\n                    if (options?.pollIntervalMs) {\n                        sleepInterval = options.pollIntervalMs;\n                    }\n                    else {\n                        const headerInterval = fileResponse.response.headers.get('openai-poll-after-ms');\n                        if (headerInterval) {\n                            const headerIntervalMs = parseInt(headerInterval);\n                            if (!isNaN(headerIntervalMs)) {\n                                sleepInterval = headerIntervalMs;\n                            }\n                        }\n                    }\n                    await (0,_core_mjs__WEBPACK_IMPORTED_MODULE_1__.sleep)(sleepInterval);\n                    break;\n                case 'failed':\n                case 'completed':\n                    return file;\n            }\n        }\n    }\n    /**\n     * Upload a file to the `files` API and then attach it to the given vector store.\n     *\n     * Note the file will be asynchronously processed (you can use the alternative\n     * polling helper method to wait for processing to complete).\n     */\n    async upload(vectorStoreId, file, options) {\n        const fileInfo = await this._client.files.create({ file: file, purpose: 'assistants' }, options);\n        return this.create(vectorStoreId, { file_id: fileInfo.id }, options);\n    }\n    /**\n     * Add a file to a vector store and poll until processing is complete.\n     */\n    async uploadAndPoll(vectorStoreId, file, options) {\n        const fileInfo = await this.upload(vectorStoreId, file, options);\n        return await this.poll(vectorStoreId, fileInfo.id, options);\n    }\n}\nclass VectorStoreFilesPage extends _pagination_mjs__WEBPACK_IMPORTED_MODULE_2__.CursorPage {\n}\n(function (Files) {\n    Files.VectorStoreFilesPage = _files_mjs__WEBPACK_IMPORTED_MODULE_3__.VectorStoreFilesPage;\n})(Files || (Files = {}));\n//# sourceMappingURL=files.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/beta/vector-stores/files.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/beta/vector-stores/vector-stores.mjs":
/*!****************************************************************************!*\
  !*** ./node_modules/openai/resources/beta/vector-stores/vector-stores.mjs ***!
  \****************************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   VectorStores: () => (/* binding */ VectorStores),\n/* harmony export */   VectorStoresPage: () => (/* binding */ VectorStoresPage)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ../../../core.mjs */ \"./node_modules/openai/core.mjs\");\n/* harmony import */ var _vector_stores_mjs__WEBPACK_IMPORTED_MODULE_5__ = __webpack_require__(/*! ./vector-stores.mjs */ \"./node_modules/openai/resources/beta/vector-stores/vector-stores.mjs\");\n/* harmony import */ var _file_batches_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ./file-batches.mjs */ \"./node_modules/openai/resources/beta/vector-stores/file-batches.mjs\");\n/* harmony import */ var _files_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./files.mjs */ \"./node_modules/openai/resources/beta/vector-stores/files.mjs\");\n/* harmony import */ var _pagination_mjs__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! ../../../pagination.mjs */ \"./node_modules/openai/pagination.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\n\n\nclass VectorStores extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    constructor() {\n        super(...arguments);\n        this.files = new _files_mjs__WEBPACK_IMPORTED_MODULE_1__.Files(this._client);\n        this.fileBatches = new _file_batches_mjs__WEBPACK_IMPORTED_MODULE_2__.FileBatches(this._client);\n    }\n    /**\n     * Create a vector store.\n     */\n    create(body, options) {\n        return this._client.post('/vector_stores', {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Retrieves a vector store.\n     */\n    retrieve(vectorStoreId, options) {\n        return this._client.get(`/vector_stores/${vectorStoreId}`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Modifies a vector store.\n     */\n    update(vectorStoreId, body, options) {\n        return this._client.post(`/vector_stores/${vectorStoreId}`, {\n            body,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    list(query = {}, options) {\n        if ((0,_core_mjs__WEBPACK_IMPORTED_MODULE_3__.isRequestOptions)(query)) {\n            return this.list({}, query);\n        }\n        return this._client.getAPIList('/vector_stores', VectorStoresPage, {\n            query,\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n    /**\n     * Delete a vector store.\n     */\n    del(vectorStoreId, options) {\n        return this._client.delete(`/vector_stores/${vectorStoreId}`, {\n            ...options,\n            headers: { 'OpenAI-Beta': 'assistants=v2', ...options?.headers },\n        });\n    }\n}\nclass VectorStoresPage extends _pagination_mjs__WEBPACK_IMPORTED_MODULE_4__.CursorPage {\n}\n(function (VectorStores) {\n    VectorStores.VectorStoresPage = _vector_stores_mjs__WEBPACK_IMPORTED_MODULE_5__.VectorStoresPage;\n    VectorStores.Files = _files_mjs__WEBPACK_IMPORTED_MODULE_1__.Files;\n    VectorStores.VectorStoreFilesPage = _files_mjs__WEBPACK_IMPORTED_MODULE_1__.VectorStoreFilesPage;\n    VectorStores.FileBatches = _file_batches_mjs__WEBPACK_IMPORTED_MODULE_2__.FileBatches;\n})(VectorStores || (VectorStores = {}));\n//# sourceMappingURL=vector-stores.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/beta/vector-stores/vector-stores.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/chat/chat.mjs":
/*!*****************************************************!*\
  !*** ./node_modules/openai/resources/chat/chat.mjs ***!
  \*****************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Chat: () => (/* binding */ Chat)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _completions_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./completions.mjs */ \"./node_modules/openai/resources/chat/completions.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\nclass Chat extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    constructor() {\n        super(...arguments);\n        this.completions = new _completions_mjs__WEBPACK_IMPORTED_MODULE_1__.Completions(this._client);\n    }\n}\n(function (Chat) {\n    Chat.Completions = _completions_mjs__WEBPACK_IMPORTED_MODULE_1__.Completions;\n})(Chat || (Chat = {}));\n//# sourceMappingURL=chat.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/chat/chat.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/chat/completions.mjs":
/*!************************************************************!*\
  !*** ./node_modules/openai/resources/chat/completions.mjs ***!
  \************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Completions: () => (/* binding */ Completions)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nclass Completions extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    create(body, options) {\n        return this._client.post('/chat/completions', { body, ...options, stream: body.stream ?? false });\n    }\n}\n(function (Completions) {\n})(Completions || (Completions = {}));\n//# sourceMappingURL=completions.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/chat/completions.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/completions.mjs":
/*!*******************************************************!*\
  !*** ./node_modules/openai/resources/completions.mjs ***!
  \*******************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Completions: () => (/* binding */ Completions)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nclass Completions extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    create(body, options) {\n        return this._client.post('/completions', { body, ...options, stream: body.stream ?? false });\n    }\n}\n(function (Completions) {\n})(Completions || (Completions = {}));\n//# sourceMappingURL=completions.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/completions.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/embeddings.mjs":
/*!******************************************************!*\
  !*** ./node_modules/openai/resources/embeddings.mjs ***!
  \******************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Embeddings: () => (/* binding */ Embeddings)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nclass Embeddings extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    /**\n     * Creates an embedding vector representing the input text.\n     */\n    create(body, options) {\n        return this._client.post('/embeddings', { body, ...options });\n    }\n}\n(function (Embeddings) {\n})(Embeddings || (Embeddings = {}));\n//# sourceMappingURL=embeddings.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/embeddings.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/files.mjs":
/*!*************************************************!*\
  !*** ./node_modules/openai/resources/files.mjs ***!
  \*************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   FileObjectsPage: () => (/* binding */ FileObjectsPage),\n/* harmony export */   Files: () => (/* binding */ Files)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../core.mjs */ \"./node_modules/openai/core.mjs\");\n/* harmony import */ var _error_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ../error.mjs */ \"./node_modules/openai/error.mjs\");\n/* harmony import */ var _files_mjs__WEBPACK_IMPORTED_MODULE_5__ = __webpack_require__(/*! ./files.mjs */ \"./node_modules/openai/resources/files.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../core.mjs */ \"./node_modules/openai/uploads.mjs\");\n/* harmony import */ var _pagination_mjs__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! ../pagination.mjs */ \"./node_modules/openai/pagination.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\n\n\n\nclass Files extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    /**\n     * Upload a file that can be used across various endpoints. Individual files can be\n     * up to 512 MB, and the size of all files uploaded by one organization can be up\n     * to 100 GB.\n     *\n     * The Assistants API supports files up to 2 million tokens and of specific file\n     * types. See the\n     * [Assistants Tools guide](https://platform.openai.com/docs/assistants/tools) for\n     * details.\n     *\n     * The Fine-tuning API only supports `.jsonl` files. The input also has certain\n     * required formats for fine-tuning\n     * [chat](https://platform.openai.com/docs/api-reference/fine-tuning/chat-input) or\n     * [completions](https://platform.openai.com/docs/api-reference/fine-tuning/completions-input)\n     * models.\n     *\n     * The Batch API only supports `.jsonl` files up to 100 MB in size. The input also\n     * has a specific required\n     * [format](https://platform.openai.com/docs/api-reference/batch/request-input).\n     *\n     * Please [contact us](https://help.openai.com/) if you need to increase these\n     * storage limits.\n     */\n    create(body, options) {\n        return this._client.post('/files', (0,_core_mjs__WEBPACK_IMPORTED_MODULE_1__.multipartFormRequestOptions)({ body, ...options }));\n    }\n    /**\n     * Returns information about a specific file.\n     */\n    retrieve(fileId, options) {\n        return this._client.get(`/files/${fileId}`, options);\n    }\n    list(query = {}, options) {\n        if ((0,_core_mjs__WEBPACK_IMPORTED_MODULE_2__.isRequestOptions)(query)) {\n            return this.list({}, query);\n        }\n        return this._client.getAPIList('/files', FileObjectsPage, { query, ...options });\n    }\n    /**\n     * Delete a file.\n     */\n    del(fileId, options) {\n        return this._client.delete(`/files/${fileId}`, options);\n    }\n    /**\n     * Returns the contents of the specified file.\n     */\n    content(fileId, options) {\n        return this._client.get(`/files/${fileId}/content`, { ...options, __binaryResponse: true });\n    }\n    /**\n     * Returns the contents of the specified file.\n     *\n     * @deprecated The `.content()` method should be used instead\n     */\n    retrieveContent(fileId, options) {\n        return this._client.get(`/files/${fileId}/content`, {\n            ...options,\n            headers: { Accept: 'application/json', ...options?.headers },\n        });\n    }\n    /**\n     * Waits for the given file to be processed, default timeout is 30 mins.\n     */\n    async waitForProcessing(id, { pollInterval = 5000, maxWait = 30 * 60 * 1000 } = {}) {\n        const TERMINAL_STATES = new Set(['processed', 'error', 'deleted']);\n        const start = Date.now();\n        let file = await this.retrieve(id);\n        while (!file.status || !TERMINAL_STATES.has(file.status)) {\n            await (0,_core_mjs__WEBPACK_IMPORTED_MODULE_2__.sleep)(pollInterval);\n            file = await this.retrieve(id);\n            if (Date.now() - start > maxWait) {\n                throw new _error_mjs__WEBPACK_IMPORTED_MODULE_3__.APIConnectionTimeoutError({\n                    message: `Giving up on waiting for file ${id} to finish processing after ${maxWait} milliseconds.`,\n                });\n            }\n        }\n        return file;\n    }\n}\n/**\n * Note: no pagination actually occurs yet, this is for forwards-compatibility.\n */\nclass FileObjectsPage extends _pagination_mjs__WEBPACK_IMPORTED_MODULE_4__.Page {\n}\n(function (Files) {\n    Files.FileObjectsPage = _files_mjs__WEBPACK_IMPORTED_MODULE_5__.FileObjectsPage;\n})(Files || (Files = {}));\n//# sourceMappingURL=files.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/files.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/fine-tuning/fine-tuning.mjs":
/*!*******************************************************************!*\
  !*** ./node_modules/openai/resources/fine-tuning/fine-tuning.mjs ***!
  \*******************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   FineTuning: () => (/* binding */ FineTuning)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _jobs_jobs_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./jobs/jobs.mjs */ \"./node_modules/openai/resources/fine-tuning/jobs/jobs.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\nclass FineTuning extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    constructor() {\n        super(...arguments);\n        this.jobs = new _jobs_jobs_mjs__WEBPACK_IMPORTED_MODULE_1__.Jobs(this._client);\n    }\n}\n(function (FineTuning) {\n    FineTuning.Jobs = _jobs_jobs_mjs__WEBPACK_IMPORTED_MODULE_1__.Jobs;\n    FineTuning.FineTuningJobsPage = _jobs_jobs_mjs__WEBPACK_IMPORTED_MODULE_1__.FineTuningJobsPage;\n    FineTuning.FineTuningJobEventsPage = _jobs_jobs_mjs__WEBPACK_IMPORTED_MODULE_1__.FineTuningJobEventsPage;\n})(FineTuning || (FineTuning = {}));\n//# sourceMappingURL=fine-tuning.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/fine-tuning/fine-tuning.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/fine-tuning/jobs/checkpoints.mjs":
/*!************************************************************************!*\
  !*** ./node_modules/openai/resources/fine-tuning/jobs/checkpoints.mjs ***!
  \************************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Checkpoints: () => (/* binding */ Checkpoints),\n/* harmony export */   FineTuningJobCheckpointsPage: () => (/* binding */ FineTuningJobCheckpointsPage)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../../../core.mjs */ \"./node_modules/openai/core.mjs\");\n/* harmony import */ var _checkpoints_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ./checkpoints.mjs */ \"./node_modules/openai/resources/fine-tuning/jobs/checkpoints.mjs\");\n/* harmony import */ var _pagination_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../../../pagination.mjs */ \"./node_modules/openai/pagination.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\nclass Checkpoints extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    list(fineTuningJobId, query = {}, options) {\n        if ((0,_core_mjs__WEBPACK_IMPORTED_MODULE_1__.isRequestOptions)(query)) {\n            return this.list(fineTuningJobId, {}, query);\n        }\n        return this._client.getAPIList(`/fine_tuning/jobs/${fineTuningJobId}/checkpoints`, FineTuningJobCheckpointsPage, { query, ...options });\n    }\n}\nclass FineTuningJobCheckpointsPage extends _pagination_mjs__WEBPACK_IMPORTED_MODULE_2__.CursorPage {\n}\n(function (Checkpoints) {\n    Checkpoints.FineTuningJobCheckpointsPage = _checkpoints_mjs__WEBPACK_IMPORTED_MODULE_3__.FineTuningJobCheckpointsPage;\n})(Checkpoints || (Checkpoints = {}));\n//# sourceMappingURL=checkpoints.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/fine-tuning/jobs/checkpoints.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/fine-tuning/jobs/jobs.mjs":
/*!*****************************************************************!*\
  !*** ./node_modules/openai/resources/fine-tuning/jobs/jobs.mjs ***!
  \*****************************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   FineTuningJobEventsPage: () => (/* binding */ FineTuningJobEventsPage),\n/* harmony export */   FineTuningJobsPage: () => (/* binding */ FineTuningJobsPage),\n/* harmony export */   Jobs: () => (/* binding */ Jobs)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../../../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ../../../core.mjs */ \"./node_modules/openai/core.mjs\");\n/* harmony import */ var _jobs_mjs__WEBPACK_IMPORTED_MODULE_4__ = __webpack_require__(/*! ./jobs.mjs */ \"./node_modules/openai/resources/fine-tuning/jobs/jobs.mjs\");\n/* harmony import */ var _checkpoints_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ./checkpoints.mjs */ \"./node_modules/openai/resources/fine-tuning/jobs/checkpoints.mjs\");\n/* harmony import */ var _pagination_mjs__WEBPACK_IMPORTED_MODULE_3__ = __webpack_require__(/*! ../../../pagination.mjs */ \"./node_modules/openai/pagination.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\n\n\nclass Jobs extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    constructor() {\n        super(...arguments);\n        this.checkpoints = new _checkpoints_mjs__WEBPACK_IMPORTED_MODULE_1__.Checkpoints(this._client);\n    }\n    /**\n     * Creates a fine-tuning job which begins the process of creating a new model from\n     * a given dataset.\n     *\n     * Response includes details of the enqueued job including job status and the name\n     * of the fine-tuned models once complete.\n     *\n     * [Learn more about fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)\n     */\n    create(body, options) {\n        return this._client.post('/fine_tuning/jobs', { body, ...options });\n    }\n    /**\n     * Get info about a fine-tuning job.\n     *\n     * [Learn more about fine-tuning](https://platform.openai.com/docs/guides/fine-tuning)\n     */\n    retrieve(fineTuningJobId, options) {\n        return this._client.get(`/fine_tuning/jobs/${fineTuningJobId}`, options);\n    }\n    list(query = {}, options) {\n        if ((0,_core_mjs__WEBPACK_IMPORTED_MODULE_2__.isRequestOptions)(query)) {\n            return this.list({}, query);\n        }\n        return this._client.getAPIList('/fine_tuning/jobs', FineTuningJobsPage, { query, ...options });\n    }\n    /**\n     * Immediately cancel a fine-tune job.\n     */\n    cancel(fineTuningJobId, options) {\n        return this._client.post(`/fine_tuning/jobs/${fineTuningJobId}/cancel`, options);\n    }\n    listEvents(fineTuningJobId, query = {}, options) {\n        if ((0,_core_mjs__WEBPACK_IMPORTED_MODULE_2__.isRequestOptions)(query)) {\n            return this.listEvents(fineTuningJobId, {}, query);\n        }\n        return this._client.getAPIList(`/fine_tuning/jobs/${fineTuningJobId}/events`, FineTuningJobEventsPage, {\n            query,\n            ...options,\n        });\n    }\n}\nclass FineTuningJobsPage extends _pagination_mjs__WEBPACK_IMPORTED_MODULE_3__.CursorPage {\n}\nclass FineTuningJobEventsPage extends _pagination_mjs__WEBPACK_IMPORTED_MODULE_3__.CursorPage {\n}\n(function (Jobs) {\n    Jobs.FineTuningJobsPage = _jobs_mjs__WEBPACK_IMPORTED_MODULE_4__.FineTuningJobsPage;\n    Jobs.FineTuningJobEventsPage = _jobs_mjs__WEBPACK_IMPORTED_MODULE_4__.FineTuningJobEventsPage;\n    Jobs.Checkpoints = _checkpoints_mjs__WEBPACK_IMPORTED_MODULE_1__.Checkpoints;\n    Jobs.FineTuningJobCheckpointsPage = _checkpoints_mjs__WEBPACK_IMPORTED_MODULE_1__.FineTuningJobCheckpointsPage;\n})(Jobs || (Jobs = {}));\n//# sourceMappingURL=jobs.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/fine-tuning/jobs/jobs.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/images.mjs":
/*!**************************************************!*\
  !*** ./node_modules/openai/resources/images.mjs ***!
  \**************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Images: () => (/* binding */ Images)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _core_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../core.mjs */ \"./node_modules/openai/uploads.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\nclass Images extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    /**\n     * Creates a variation of a given image.\n     */\n    createVariation(body, options) {\n        return this._client.post('/images/variations', (0,_core_mjs__WEBPACK_IMPORTED_MODULE_1__.multipartFormRequestOptions)({ body, ...options }));\n    }\n    /**\n     * Creates an edited or extended image given an original image and a prompt.\n     */\n    edit(body, options) {\n        return this._client.post('/images/edits', (0,_core_mjs__WEBPACK_IMPORTED_MODULE_1__.multipartFormRequestOptions)({ body, ...options }));\n    }\n    /**\n     * Creates an image given a prompt.\n     */\n    generate(body, options) {\n        return this._client.post('/images/generations', { body, ...options });\n    }\n}\n(function (Images) {\n})(Images || (Images = {}));\n//# sourceMappingURL=images.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/images.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/models.mjs":
/*!**************************************************!*\
  !*** ./node_modules/openai/resources/models.mjs ***!
  \**************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Models: () => (/* binding */ Models),\n/* harmony export */   ModelsPage: () => (/* binding */ ModelsPage)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n/* harmony import */ var _models_mjs__WEBPACK_IMPORTED_MODULE_2__ = __webpack_require__(/*! ./models.mjs */ \"./node_modules/openai/resources/models.mjs\");\n/* harmony import */ var _pagination_mjs__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! ../pagination.mjs */ \"./node_modules/openai/pagination.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\n\n\nclass Models extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    /**\n     * Retrieves a model instance, providing basic information about the model such as\n     * the owner and permissioning.\n     */\n    retrieve(model, options) {\n        return this._client.get(`/models/${model}`, options);\n    }\n    /**\n     * Lists the currently available models, and provides basic information about each\n     * one such as the owner and availability.\n     */\n    list(options) {\n        return this._client.getAPIList('/models', ModelsPage, options);\n    }\n    /**\n     * Delete a fine-tuned model. You must have the Owner role in your organization to\n     * delete a model.\n     */\n    del(model, options) {\n        return this._client.delete(`/models/${model}`, options);\n    }\n}\n/**\n * Note: no pagination actually occurs yet, this is for forwards-compatibility.\n */\nclass ModelsPage extends _pagination_mjs__WEBPACK_IMPORTED_MODULE_1__.Page {\n}\n(function (Models) {\n    Models.ModelsPage = _models_mjs__WEBPACK_IMPORTED_MODULE_2__.ModelsPage;\n})(Models || (Models = {}));\n//# sourceMappingURL=models.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/models.mjs?");

/***/ }),

/***/ "./node_modules/openai/resources/moderations.mjs":
/*!*******************************************************!*\
  !*** ./node_modules/openai/resources/moderations.mjs ***!
  \*******************************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Moderations: () => (/* binding */ Moderations)\n/* harmony export */ });\n/* harmony import */ var _resource_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ../resource.mjs */ \"./node_modules/openai/resource.mjs\");\n// File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\n\nclass Moderations extends _resource_mjs__WEBPACK_IMPORTED_MODULE_0__.APIResource {\n    /**\n     * Classifies if text is potentially harmful.\n     */\n    create(body, options) {\n        return this._client.post('/moderations', { body, ...options });\n    }\n}\n(function (Moderations) {\n})(Moderations || (Moderations = {}));\n//# sourceMappingURL=moderations.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/resources/moderations.mjs?");

/***/ }),

/***/ "./node_modules/openai/streaming.mjs":
/*!*******************************************!*\
  !*** ./node_modules/openai/streaming.mjs ***!
  \*******************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   Stream: () => (/* binding */ Stream),\n/* harmony export */   _decodeChunks: () => (/* binding */ _decodeChunks),\n/* harmony export */   _iterSSEMessages: () => (/* binding */ _iterSSEMessages),\n/* harmony export */   readableStreamAsyncIterable: () => (/* binding */ readableStreamAsyncIterable)\n/* harmony export */ });\n/* harmony import */ var _shims_index_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./_shims/index.mjs */ \"./node_modules/openai/_shims/index.mjs\");\n/* harmony import */ var openai_error__WEBPACK_IMPORTED_MODULE_1__ = __webpack_require__(/*! openai/error */ \"./node_modules/openai/error.mjs\");\n\n\n\nclass Stream {\n    constructor(iterator, controller) {\n        this.iterator = iterator;\n        this.controller = controller;\n    }\n    static fromSSEResponse(response, controller) {\n        let consumed = false;\n        async function* iterator() {\n            if (consumed) {\n                throw new Error('Cannot iterate over a consumed stream, use `.tee()` to split the stream.');\n            }\n            consumed = true;\n            let done = false;\n            try {\n                for await (const sse of _iterSSEMessages(response, controller)) {\n                    if (done)\n                        continue;\n                    if (sse.data.startsWith('[DONE]')) {\n                        done = true;\n                        continue;\n                    }\n                    if (sse.event === null) {\n                        let data;\n                        try {\n                            data = JSON.parse(sse.data);\n                        }\n                        catch (e) {\n                            console.error(`Could not parse message into JSON:`, sse.data);\n                            console.error(`From chunk:`, sse.raw);\n                            throw e;\n                        }\n                        if (data && data.error) {\n                            throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.APIError(undefined, data.error, undefined, undefined);\n                        }\n                        yield data;\n                    }\n                    else {\n                        let data;\n                        try {\n                            data = JSON.parse(sse.data);\n                        }\n                        catch (e) {\n                            console.error(`Could not parse message into JSON:`, sse.data);\n                            console.error(`From chunk:`, sse.raw);\n                            throw e;\n                        }\n                        // TODO: Is this where the error should be thrown?\n                        if (sse.event == 'error') {\n                            throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.APIError(undefined, data.error, data.message, undefined);\n                        }\n                        yield { event: sse.event, data: data };\n                    }\n                }\n                done = true;\n            }\n            catch (e) {\n                // If the user calls `stream.controller.abort()`, we should exit without throwing.\n                if (e instanceof Error && e.name === 'AbortError')\n                    return;\n                throw e;\n            }\n            finally {\n                // If the user `break`s, abort the ongoing request.\n                if (!done)\n                    controller.abort();\n            }\n        }\n        return new Stream(iterator, controller);\n    }\n    /**\n     * Generates a Stream from a newline-separated ReadableStream\n     * where each item is a JSON value.\n     */\n    static fromReadableStream(readableStream, controller) {\n        let consumed = false;\n        async function* iterLines() {\n            const lineDecoder = new LineDecoder();\n            const iter = readableStreamAsyncIterable(readableStream);\n            for await (const chunk of iter) {\n                for (const line of lineDecoder.decode(chunk)) {\n                    yield line;\n                }\n            }\n            for (const line of lineDecoder.flush()) {\n                yield line;\n            }\n        }\n        async function* iterator() {\n            if (consumed) {\n                throw new Error('Cannot iterate over a consumed stream, use `.tee()` to split the stream.');\n            }\n            consumed = true;\n            let done = false;\n            try {\n                for await (const line of iterLines()) {\n                    if (done)\n                        continue;\n                    if (line)\n                        yield JSON.parse(line);\n                }\n                done = true;\n            }\n            catch (e) {\n                // If the user calls `stream.controller.abort()`, we should exit without throwing.\n                if (e instanceof Error && e.name === 'AbortError')\n                    return;\n                throw e;\n            }\n            finally {\n                // If the user `break`s, abort the ongoing request.\n                if (!done)\n                    controller.abort();\n            }\n        }\n        return new Stream(iterator, controller);\n    }\n    [Symbol.asyncIterator]() {\n        return this.iterator();\n    }\n    /**\n     * Splits the stream into two streams which can be\n     * independently read from at different speeds.\n     */\n    tee() {\n        const left = [];\n        const right = [];\n        const iterator = this.iterator();\n        const teeIterator = (queue) => {\n            return {\n                next: () => {\n                    if (queue.length === 0) {\n                        const result = iterator.next();\n                        left.push(result);\n                        right.push(result);\n                    }\n                    return queue.shift();\n                },\n            };\n        };\n        return [\n            new Stream(() => teeIterator(left), this.controller),\n            new Stream(() => teeIterator(right), this.controller),\n        ];\n    }\n    /**\n     * Converts this stream to a newline-separated ReadableStream of\n     * JSON stringified values in the stream\n     * which can be turned back into a Stream with `Stream.fromReadableStream()`.\n     */\n    toReadableStream() {\n        const self = this;\n        let iter;\n        const encoder = new TextEncoder();\n        return new _shims_index_mjs__WEBPACK_IMPORTED_MODULE_0__.ReadableStream({\n            async start() {\n                iter = self[Symbol.asyncIterator]();\n            },\n            async pull(ctrl) {\n                try {\n                    const { value, done } = await iter.next();\n                    if (done)\n                        return ctrl.close();\n                    const bytes = encoder.encode(JSON.stringify(value) + '\\n');\n                    ctrl.enqueue(bytes);\n                }\n                catch (err) {\n                    ctrl.error(err);\n                }\n            },\n            async cancel() {\n                await iter.return?.();\n            },\n        });\n    }\n}\nasync function* _iterSSEMessages(response, controller) {\n    if (!response.body) {\n        controller.abort();\n        throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`Attempted to iterate over a response with no body`);\n    }\n    const sseDecoder = new SSEDecoder();\n    const lineDecoder = new LineDecoder();\n    const iter = readableStreamAsyncIterable(response.body);\n    for await (const sseChunk of iterSSEChunks(iter)) {\n        for (const line of lineDecoder.decode(sseChunk)) {\n            const sse = sseDecoder.decode(line);\n            if (sse)\n                yield sse;\n        }\n    }\n    for (const line of lineDecoder.flush()) {\n        const sse = sseDecoder.decode(line);\n        if (sse)\n            yield sse;\n    }\n}\n/**\n * Given an async iterable iterator, iterates over it and yields full\n * SSE chunks, i.e. yields when a double new-line is encountered.\n */\nasync function* iterSSEChunks(iterator) {\n    let data = new Uint8Array();\n    for await (const chunk of iterator) {\n        if (chunk == null) {\n            continue;\n        }\n        const binaryChunk = chunk instanceof ArrayBuffer ? new Uint8Array(chunk)\n            : typeof chunk === 'string' ? new TextEncoder().encode(chunk)\n                : chunk;\n        let newData = new Uint8Array(data.length + binaryChunk.length);\n        newData.set(data);\n        newData.set(binaryChunk, data.length);\n        data = newData;\n        let patternIndex;\n        while ((patternIndex = findDoubleNewlineIndex(data)) !== -1) {\n            yield data.slice(0, patternIndex);\n            data = data.slice(patternIndex);\n        }\n    }\n    if (data.length > 0) {\n        yield data;\n    }\n}\nfunction findDoubleNewlineIndex(buffer) {\n    // This function searches the buffer for the end patterns (\\r\\r, \\n\\n, \\r\\n\\r\\n)\n    // and returns the index right after the first occurrence of any pattern,\n    // or -1 if none of the patterns are found.\n    const newline = 0x0a; // \\n\n    const carriage = 0x0d; // \\r\n    for (let i = 0; i < buffer.length - 2; i++) {\n        if (buffer[i] === newline && buffer[i + 1] === newline) {\n            // \\n\\n\n            return i + 2;\n        }\n        if (buffer[i] === carriage && buffer[i + 1] === carriage) {\n            // \\r\\r\n            return i + 2;\n        }\n        if (buffer[i] === carriage &&\n            buffer[i + 1] === newline &&\n            i + 3 < buffer.length &&\n            buffer[i + 2] === carriage &&\n            buffer[i + 3] === newline) {\n            // \\r\\n\\r\\n\n            return i + 4;\n        }\n    }\n    return -1;\n}\nclass SSEDecoder {\n    constructor() {\n        this.event = null;\n        this.data = [];\n        this.chunks = [];\n    }\n    decode(line) {\n        if (line.endsWith('\\r')) {\n            line = line.substring(0, line.length - 1);\n        }\n        if (!line) {\n            // empty line and we didn't previously encounter any messages\n            if (!this.event && !this.data.length)\n                return null;\n            const sse = {\n                event: this.event,\n                data: this.data.join('\\n'),\n                raw: this.chunks,\n            };\n            this.event = null;\n            this.data = [];\n            this.chunks = [];\n            return sse;\n        }\n        this.chunks.push(line);\n        if (line.startsWith(':')) {\n            return null;\n        }\n        let [fieldname, _, value] = partition(line, ':');\n        if (value.startsWith(' ')) {\n            value = value.substring(1);\n        }\n        if (fieldname === 'event') {\n            this.event = value;\n        }\n        else if (fieldname === 'data') {\n            this.data.push(value);\n        }\n        return null;\n    }\n}\n/**\n * A re-implementation of httpx's `LineDecoder` in Python that handles incrementally\n * reading lines from text.\n *\n * https://github.com/encode/httpx/blob/920333ea98118e9cf617f246905d7b202510941c/httpx/_decoders.py#L258\n */\nclass LineDecoder {\n    constructor() {\n        this.buffer = [];\n        this.trailingCR = false;\n    }\n    decode(chunk) {\n        let text = this.decodeText(chunk);\n        if (this.trailingCR) {\n            text = '\\r' + text;\n            this.trailingCR = false;\n        }\n        if (text.endsWith('\\r')) {\n            this.trailingCR = true;\n            text = text.slice(0, -1);\n        }\n        if (!text) {\n            return [];\n        }\n        const trailingNewline = LineDecoder.NEWLINE_CHARS.has(text[text.length - 1] || '');\n        let lines = text.split(LineDecoder.NEWLINE_REGEXP);\n        // if there is a trailing new line then the last entry will be an empty\n        // string which we don't care about\n        if (trailingNewline) {\n            lines.pop();\n        }\n        if (lines.length === 1 && !trailingNewline) {\n            this.buffer.push(lines[0]);\n            return [];\n        }\n        if (this.buffer.length > 0) {\n            lines = [this.buffer.join('') + lines[0], ...lines.slice(1)];\n            this.buffer = [];\n        }\n        if (!trailingNewline) {\n            this.buffer = [lines.pop() || ''];\n        }\n        return lines;\n    }\n    decodeText(bytes) {\n        if (bytes == null)\n            return '';\n        if (typeof bytes === 'string')\n            return bytes;\n        // Node:\n        if (typeof Buffer !== 'undefined') {\n            if (bytes instanceof Buffer) {\n                return bytes.toString();\n            }\n            if (bytes instanceof Uint8Array) {\n                return Buffer.from(bytes).toString();\n            }\n            throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`Unexpected: received non-Uint8Array (${bytes.constructor.name}) stream chunk in an environment with a global \"Buffer\" defined, which this library assumes to be Node. Please report this error.`);\n        }\n        // Browser\n        if (typeof TextDecoder !== 'undefined') {\n            if (bytes instanceof Uint8Array || bytes instanceof ArrayBuffer) {\n                this.textDecoder ?? (this.textDecoder = new TextDecoder('utf8'));\n                return this.textDecoder.decode(bytes);\n            }\n            throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`Unexpected: received non-Uint8Array/ArrayBuffer (${bytes.constructor.name}) in a web platform. Please report this error.`);\n        }\n        throw new openai_error__WEBPACK_IMPORTED_MODULE_1__.OpenAIError(`Unexpected: neither Buffer nor TextDecoder are available as globals. Please report this error.`);\n    }\n    flush() {\n        if (!this.buffer.length && !this.trailingCR) {\n            return [];\n        }\n        const lines = [this.buffer.join('')];\n        this.buffer = [];\n        this.trailingCR = false;\n        return lines;\n    }\n}\n// prettier-ignore\nLineDecoder.NEWLINE_CHARS = new Set(['\\n', '\\r']);\nLineDecoder.NEWLINE_REGEXP = /\\r\\n|[\\n\\r]/g;\n/** This is an internal helper function that's just used for testing */\nfunction _decodeChunks(chunks) {\n    const decoder = new LineDecoder();\n    const lines = [];\n    for (const chunk of chunks) {\n        lines.push(...decoder.decode(chunk));\n    }\n    return lines;\n}\nfunction partition(str, delimiter) {\n    const index = str.indexOf(delimiter);\n    if (index !== -1) {\n        return [str.substring(0, index), delimiter, str.substring(index + delimiter.length)];\n    }\n    return [str, '', ''];\n}\n/**\n * Most browsers don't yet have async iterable support for ReadableStream,\n * and Node has a very different way of reading bytes from its \"ReadableStream\".\n *\n * This polyfill was pulled from https://github.com/MattiasBuelens/web-streams-polyfill/pull/122#issuecomment-1627354490\n */\nfunction readableStreamAsyncIterable(stream) {\n    if (stream[Symbol.asyncIterator])\n        return stream;\n    const reader = stream.getReader();\n    return {\n        async next() {\n            try {\n                const result = await reader.read();\n                if (result?.done)\n                    reader.releaseLock(); // release lock when stream becomes closed\n                return result;\n            }\n            catch (e) {\n                reader.releaseLock(); // release lock when stream becomes errored\n                throw e;\n            }\n        },\n        async return() {\n            const cancelPromise = reader.cancel();\n            reader.releaseLock();\n            await cancelPromise;\n            return { done: true, value: undefined };\n        },\n        [Symbol.asyncIterator]() {\n            return this;\n        },\n    };\n}\n//# sourceMappingURL=streaming.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/streaming.mjs?");

/***/ }),

/***/ "./node_modules/openai/uploads.mjs":
/*!*****************************************!*\
  !*** ./node_modules/openai/uploads.mjs ***!
  \*****************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   createForm: () => (/* binding */ createForm),\n/* harmony export */   fileFromPath: () => (/* reexport safe */ _shims_index_mjs__WEBPACK_IMPORTED_MODULE_0__.fileFromPath),\n/* harmony export */   isBlobLike: () => (/* binding */ isBlobLike),\n/* harmony export */   isFileLike: () => (/* binding */ isFileLike),\n/* harmony export */   isMultipartBody: () => (/* binding */ isMultipartBody),\n/* harmony export */   isResponseLike: () => (/* binding */ isResponseLike),\n/* harmony export */   isUploadable: () => (/* binding */ isUploadable),\n/* harmony export */   maybeMultipartFormRequestOptions: () => (/* binding */ maybeMultipartFormRequestOptions),\n/* harmony export */   multipartFormRequestOptions: () => (/* binding */ multipartFormRequestOptions),\n/* harmony export */   toFile: () => (/* binding */ toFile)\n/* harmony export */ });\n/* harmony import */ var _shims_index_mjs__WEBPACK_IMPORTED_MODULE_0__ = __webpack_require__(/*! ./_shims/index.mjs */ \"./node_modules/openai/_shims/index.mjs\");\n\n\nconst isResponseLike = (value) => value != null &&\n    typeof value === 'object' &&\n    typeof value.url === 'string' &&\n    typeof value.blob === 'function';\nconst isFileLike = (value) => value != null &&\n    typeof value === 'object' &&\n    typeof value.name === 'string' &&\n    typeof value.lastModified === 'number' &&\n    isBlobLike(value);\n/**\n * The BlobLike type omits arrayBuffer() because @types/node-fetch@^2.6.4 lacks it; but this check\n * adds the arrayBuffer() method type because it is available and used at runtime\n */\nconst isBlobLike = (value) => value != null &&\n    typeof value === 'object' &&\n    typeof value.size === 'number' &&\n    typeof value.type === 'string' &&\n    typeof value.text === 'function' &&\n    typeof value.slice === 'function' &&\n    typeof value.arrayBuffer === 'function';\nconst isUploadable = (value) => {\n    return isFileLike(value) || isResponseLike(value) || (0,_shims_index_mjs__WEBPACK_IMPORTED_MODULE_0__.isFsReadStream)(value);\n};\n/**\n * Helper for creating a {@link File} to pass to an SDK upload method from a variety of different data formats\n * @param value the raw content of the file.  Can be an {@link Uploadable}, {@link BlobLikePart}, or {@link AsyncIterable} of {@link BlobLikePart}s\n * @param {string=} name the name of the file. If omitted, toFile will try to determine a file name from bits if possible\n * @param {Object=} options additional properties\n * @param {string=} options.type the MIME type of the content\n * @param {number=} options.lastModified the last modified timestamp\n * @returns a {@link File} with the given properties\n */\nasync function toFile(value, name, options) {\n    // If it's a promise, resolve it.\n    value = await value;\n    // Use the file's options if there isn't one provided\n    options ?? (options = isFileLike(value) ? { lastModified: value.lastModified, type: value.type } : {});\n    if (isResponseLike(value)) {\n        const blob = await value.blob();\n        name || (name = new URL(value.url).pathname.split(/[\\\\/]/).pop() ?? 'unknown_file');\n        return new _shims_index_mjs__WEBPACK_IMPORTED_MODULE_0__.File([blob], name, options);\n    }\n    const bits = await getBytes(value);\n    name || (name = getName(value) ?? 'unknown_file');\n    if (!options.type) {\n        const type = bits[0]?.type;\n        if (typeof type === 'string') {\n            options = { ...options, type };\n        }\n    }\n    return new _shims_index_mjs__WEBPACK_IMPORTED_MODULE_0__.File(bits, name, options);\n}\nasync function getBytes(value) {\n    let parts = [];\n    if (typeof value === 'string' ||\n        ArrayBuffer.isView(value) || // includes Uint8Array, Buffer, etc.\n        value instanceof ArrayBuffer) {\n        parts.push(value);\n    }\n    else if (isBlobLike(value)) {\n        parts.push(await value.arrayBuffer());\n    }\n    else if (isAsyncIterableIterator(value) // includes Readable, ReadableStream, etc.\n    ) {\n        for await (const chunk of value) {\n            parts.push(chunk); // TODO, consider validating?\n        }\n    }\n    else {\n        throw new Error(`Unexpected data type: ${typeof value}; constructor: ${value?.constructor\n            ?.name}; props: ${propsForError(value)}`);\n    }\n    return parts;\n}\nfunction propsForError(value) {\n    const props = Object.getOwnPropertyNames(value);\n    return `[${props.map((p) => `\"${p}\"`).join(', ')}]`;\n}\nfunction getName(value) {\n    return (getStringFromMaybeBuffer(value.name) ||\n        getStringFromMaybeBuffer(value.filename) ||\n        // For fs.ReadStream\n        getStringFromMaybeBuffer(value.path)?.split(/[\\\\/]/).pop());\n}\nconst getStringFromMaybeBuffer = (x) => {\n    if (typeof x === 'string')\n        return x;\n    if (typeof Buffer !== 'undefined' && x instanceof Buffer)\n        return String(x);\n    return undefined;\n};\nconst isAsyncIterableIterator = (value) => value != null && typeof value === 'object' && typeof value[Symbol.asyncIterator] === 'function';\nconst isMultipartBody = (body) => body && typeof body === 'object' && body.body && body[Symbol.toStringTag] === 'MultipartBody';\n/**\n * Returns a multipart/form-data request if any part of the given request body contains a File / Blob value.\n * Otherwise returns the request as is.\n */\nconst maybeMultipartFormRequestOptions = async (opts) => {\n    if (!hasUploadableValue(opts.body))\n        return opts;\n    const form = await createForm(opts.body);\n    return (0,_shims_index_mjs__WEBPACK_IMPORTED_MODULE_0__.getMultipartRequestOptions)(form, opts);\n};\nconst multipartFormRequestOptions = async (opts) => {\n    const form = await createForm(opts.body);\n    return (0,_shims_index_mjs__WEBPACK_IMPORTED_MODULE_0__.getMultipartRequestOptions)(form, opts);\n};\nconst createForm = async (body) => {\n    const form = new _shims_index_mjs__WEBPACK_IMPORTED_MODULE_0__.FormData();\n    await Promise.all(Object.entries(body || {}).map(([key, value]) => addFormValue(form, key, value)));\n    return form;\n};\nconst hasUploadableValue = (value) => {\n    if (isUploadable(value))\n        return true;\n    if (Array.isArray(value))\n        return value.some(hasUploadableValue);\n    if (value && typeof value === 'object') {\n        for (const k in value) {\n            if (hasUploadableValue(value[k]))\n                return true;\n        }\n    }\n    return false;\n};\nconst addFormValue = async (form, key, value) => {\n    if (value === undefined)\n        return;\n    if (value == null) {\n        throw new TypeError(`Received null for \"${key}\"; to pass null in FormData, you must use the string 'null'`);\n    }\n    // TODO: make nested formats configurable\n    if (typeof value === 'string' || typeof value === 'number' || typeof value === 'boolean') {\n        form.append(key, String(value));\n    }\n    else if (isUploadable(value)) {\n        const file = await toFile(value);\n        form.append(key, file);\n    }\n    else if (Array.isArray(value)) {\n        await Promise.all(value.map((entry) => addFormValue(form, key + '[]', entry)));\n    }\n    else if (typeof value === 'object') {\n        await Promise.all(Object.entries(value).map(([name, prop]) => addFormValue(form, `${key}[${name}]`, prop)));\n    }\n    else {\n        throw new TypeError(`Invalid value given to form, expected a string, number, boolean, object, Array, File or Blob but got ${value} instead`);\n    }\n};\n//# sourceMappingURL=uploads.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/uploads.mjs?");

/***/ }),

/***/ "./node_modules/openai/version.mjs":
/*!*****************************************!*\
  !*** ./node_modules/openai/version.mjs ***!
  \*****************************************/
/***/ ((__unused_webpack___webpack_module__, __webpack_exports__, __webpack_require__) => {

eval("__webpack_require__.r(__webpack_exports__);\n/* harmony export */ __webpack_require__.d(__webpack_exports__, {\n/* harmony export */   VERSION: () => (/* binding */ VERSION)\n/* harmony export */ });\nconst VERSION = '4.52.0'; // x-release-please-version\n//# sourceMappingURL=version.mjs.map\n\n//# sourceURL=webpack://wpml-chat-support/./node_modules/openai/version.mjs?");

/***/ })

/******/ 	});
/************************************************************************/
/******/ 	// The module cache
/******/ 	var __webpack_module_cache__ = {};
/******/ 	
/******/ 	// The require function
/******/ 	function __webpack_require__(moduleId) {
/******/ 		// Check if module is in cache
/******/ 		var cachedModule = __webpack_module_cache__[moduleId];
/******/ 		if (cachedModule !== undefined) {
/******/ 			return cachedModule.exports;
/******/ 		}
/******/ 		// Create a new module (and put it into the cache)
/******/ 		var module = __webpack_module_cache__[moduleId] = {
/******/ 			// no module.id needed
/******/ 			// no module.loaded needed
/******/ 			exports: {}
/******/ 		};
/******/ 	
/******/ 		// Execute the module function
/******/ 		__webpack_modules__[moduleId](module, module.exports, __webpack_require__);
/******/ 	
/******/ 		// Return the exports of the module
/******/ 		return module.exports;
/******/ 	}
/******/ 	
/************************************************************************/
/******/ 	/* webpack/runtime/define property getters */
/******/ 	(() => {
/******/ 		// define getter functions for harmony exports
/******/ 		__webpack_require__.d = (exports, definition) => {
/******/ 			for(var key in definition) {
/******/ 				if(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {
/******/ 					Object.defineProperty(exports, key, { enumerable: true, get: definition[key] });
/******/ 				}
/******/ 			}
/******/ 		};
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/hasOwnProperty shorthand */
/******/ 	(() => {
/******/ 		__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))
/******/ 	})();
/******/ 	
/******/ 	/* webpack/runtime/make namespace object */
/******/ 	(() => {
/******/ 		// define __esModule on exports
/******/ 		__webpack_require__.r = (exports) => {
/******/ 			if(typeof Symbol !== 'undefined' && Symbol.toStringTag) {
/******/ 				Object.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });
/******/ 			}
/******/ 			Object.defineProperty(exports, '__esModule', { value: true });
/******/ 		};
/******/ 	})();
/******/ 	
/************************************************************************/
/******/ 	
/******/ 	// startup
/******/ 	// Load entry module and return exports
/******/ 	// This entry module can't be inlined because the eval devtool is used.
/******/ 	var __webpack_exports__ = __webpack_require__("./src/index.js");
/******/ 	
/******/ })()
;